{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","mount_file_id":"1D3-F3ig4BmplDWsFBLAlXHQcpHYyIKTh","authorship_tag":"ABX9TyNK48hu+QwmTyuXx0YHtkI3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5G-B2K9zzPrz","executionInfo":{"status":"ok","timestamp":1732548191299,"user_tz":-480,"elapsed":576,"user":{"displayName":"李唱","userId":"14428720618938666771"}},"outputId":"3273e903-33ca-4ebd-8292-4980bea15fa4"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive\n"]}],"source":["%cd drive/MyDrive/"]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4coqo_Q3z6kV","executionInfo":{"status":"ok","timestamp":1732548193208,"user_tz":-480,"elapsed":12,"user":{"displayName":"李唱","userId":"14428720618938666771"}},"outputId":"a3e3e0cc-16c5-4987-acfe-c420113d8e72"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["'Colab Notebooks'   FuzzingTools\t        reentracy_versions.csv\t TripletDetection\n"," contract_sources   GPTLens\t\t        SAPdatasets\t\t unvul.pickle\n"," data-v13.json\t    ISSRELLM4SCReplicate        SAP-FuzzingwithState\t VerifiedmSC.zip\n"," data-v4.json\t    LLM-quickstart\t        SCABI\t\t\t vul-llm-finetune\n"," data-v5.json\t    new_contract-versions.csv   SCBytecode\t\t VulnerableSC.zip\n"," data-v6.json\t    partTime\t\t        SC-V7.zip\n"," data-v7.json\t    Peculiar-main\t        SC-V8.zip\n"," data-v8.json\t    Peculiar-main.zip\t        testingSC\n"]}]},{"cell_type":"code","source":["import random\n","import pickle\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AdamW\n","from sklearn.model_selection import train_test_split\n"],"metadata":{"id":"JQxPdXLY0Bsw","executionInfo":{"status":"ok","timestamp":1732548202820,"user_tz":-480,"elapsed":7494,"user":{"displayName":"李唱","userId":"14428720618938666771"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#define the noise function\n","def add_function_noise(code, noise_level=0.1):\n","    \"\"\"\n","    Add noise to a Solidity function at the function level, combining simple perturbations\n","    and more sophisticated adversarial patterns.\n","\n","    Args:\n","        code (str): A clean Solidity function.\n","        noise_level (float): The proportion of the function to introduce noise.\n","\n","    Returns:\n","        str: The noisy Solidity function.\n","    \"\"\"\n","    lines = code.split(\"\\n\")\n","    num_lines = len(lines)\n","    num_noisy_lines = max(1, int(num_lines * noise_level))\n","\n","    for _ in range(num_noisy_lines):\n","        idx = random.randint(0, num_lines - 1)\n","        noisy_line = lines[idx].strip()\n","\n","        # Randomly select a noise type\n","        noise_type = random.choice([\n","            # Simple perturbations\n","            \"variable_swap\",\n","            \"syntax_error\",\n","            \"comment_insert\",\n","            \"modifier_swap\",\n","            \"missing_semicolon\",\n","            # Sophisticated patterns\n","            \"uninitialized_variable\",\n","            \"incorrect_visibility\",\n","            \"reentrancy_simulation\",\n","            \"arithmetic_issue\",\n","            \"access_control_misconfig\",\n","            \"delegatecall_misuse\",\n","            \"dos_pattern\"\n","        ])\n","\n","        if noise_type == \"variable_swap\":\n","            # Swap variable types\n","            noisy_line = noisy_line.replace(\"uint256\", \"int256\")\n","            noisy_line = noisy_line.replace(\"address\", \"string\")\n","        elif noise_type == \"syntax_error\":\n","            # Introduce a syntax error\n","            if \";\" in noisy_line:\n","                noisy_line = noisy_line.replace(\";\", \"\")  # Remove semicolon\n","            elif \"{\" in noisy_line:\n","                noisy_line = noisy_line.replace(\"{\", \"\")  # Remove opening brace\n","            else:\n","                noisy_line += \" if (x > 0)\"  # Add incomplete statement\n","        elif noise_type == \"comment_insert\":\n","            noisy_line = f\"// This is a noise comment\\n{noisy_line}\"\n","        elif noise_type == \"modifier_swap\":\n","            modifiers = [\"public\", \"private\", \"internal\", \"external\"]\n","            for mod in modifiers:\n","                if mod in noisy_line:\n","                    new_mod = random.choice([m for m in modifiers if m != mod])\n","                    noisy_line = noisy_line.replace(mod, new_mod)\n","                    break\n","        elif noise_type == \"missing_semicolon\":\n","            if \";\" in noisy_line:\n","                noisy_line = noisy_line.replace(\";\", \"\")  # Remove semicolon\n","        elif noise_type == \"uninitialized_variable\":\n","            # Comment out initialization\n","            if \"=\" in noisy_line:\n","                noisy_line = f\"// {noisy_line}\"\n","        elif noise_type == \"incorrect_visibility\":\n","            # Change visibility specifiers\n","            visibility_specifiers = [\"public\", \"private\", \"internal\", \"external\"]\n","            for specifier in visibility_specifiers:\n","                if specifier in noisy_line:\n","                    new_specifier = random.choice([s for s in visibility_specifiers if s != specifier])\n","                    noisy_line = noisy_line.replace(specifier, new_specifier)\n","                    break\n","        elif noise_type == \"reentrancy_simulation\":\n","            # Insert a placeholder external call\n","            noisy_line = f\"{noisy_line}\\n    externalContract.call();\"\n","        elif noise_type == \"arithmetic_issue\":\n","            # Modify arithmetic operations\n","            noisy_line = noisy_line.replace(\"+\", \"-\")\n","            noisy_line = noisy_line.replace(\"*\", \"/\")\n","        elif noise_type == \"access_control_misconfig\":\n","            # Remove access control modifiers\n","            noisy_line = noisy_line.replace(\"onlyOwner\", \"\")\n","        elif noise_type == \"delegatecall_misuse\":\n","            # Insert a placeholder delegatecall usage\n","            noisy_line = f\"{noisy_line}\\n    address(this).delegatecall();\"\n","        elif noise_type == \"dos_pattern\":\n","            # Add an infinite loop\n","            noisy_line = f\"{noisy_line}\\n    while(true) {{}}\"\n","\n","        # Replace the line in the code\n","        lines[idx] = noisy_line\n","\n","    # Reconstruct the noisy code\n","    noisy_code = \"\\n\".join(lines)\n","    return noisy_code\n","\n","#define the Dataset class\n","class SolidityDataset(Dataset):\n","    def __init__(self, codes, noise_level=0.1):\n","        \"\"\"\n","        Args:\n","            codes (list of str): List of clean Solidity functions.\n","            noise_level (float): Proportion of the function to modify for noise simulation.\n","        \"\"\"\n","        self.codes = codes\n","        self.noise_level = noise_level\n","\n","    def __len__(self):\n","        return len(self.codes)\n","\n","    def __getitem__(self, idx):\n","        clean_code = self.codes[idx]\n","        noisy_code = add_function_noise(clean_code, self.noise_level)\n","        return clean_code, noisy_code\n","\n","\n","#Load the Dataset\n","def load_data_from_pickle(file_path):\n","    with open(file_path, 'rb') as f:\n","        codes = pickle.load(f)  # Assume it contains a list of strings (smart contract functions)\n","    return codes\n","\n","# Load clean dataset\n","file_path = 'unvul.pickle'\n","clean_contracts = load_data_from_pickle(file_path)\n","\n","# Verify the dataset\n","print(f\"Loaded {len(clean_contracts)} clean smart contract functions.\")\n","print(\"Example function:\\n\", clean_contracts[0])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F7EVXrO3z7O7","executionInfo":{"status":"ok","timestamp":1732549165206,"user_tz":-480,"elapsed":848,"user":{"displayName":"李唱","userId":"14428720618938666771"}},"outputId":"b9fdb975-a5ab-4479-e36c-3fdd9d552802"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 10000 clean smart contract functions.\n","Example function:\n"," function repayBorrowBehalf(address borrower) external payable { (uint err,) = repayBorrowBehalfInternal(borrower, msg.value); \\n requireNoError(err, \"repayBorrowBehalf failed\"); \\n } \\n\n"]}]},{"cell_type":"code","source":["#Prepare DataLoaders\n","# Split the dataset into training and validation sets\n","train_contracts, val_contracts = train_test_split(clean_contracts, test_size=0.1, random_state=42)\n","\n","# Create Datasets and DataLoaders\n","train_dataset = SolidityDataset(train_contracts, noise_level=0.2)\n","val_dataset = SolidityDataset(val_contracts, noise_level=0.2)\n","\n","train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n","\n","#initialize Model and Optimizer\n","# Initialize Model and Optimizer\n","tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5-small\")\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"Salesforce/codet5-small\")\n","optimizer = AdamW(model.parameters(), lr=5e-5)\n","\n","# Define the device\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","#define the custom Loss function\n","\n","def custom_loss(outputs_clean, labels_clean, outputs_noisy, labels_noisy, margin=1.0):\n","    \"\"\"\n","    Compute custom loss for clean and noisy inputs.\n","\n","    Args:\n","        outputs_clean: Model outputs for clean inputs.\n","        labels_clean: Tokenized clean code inputs.\n","        outputs_noisy: Model outputs for noisy inputs.\n","        labels_noisy: Tokenized noisy code inputs.\n","        margin: Margin value for the noisy reconstruction loss.\n","\n","    Returns:\n","        total_loss: Combined loss for optimization.\n","    \"\"\"\n","    # Cross-Entropy Loss function\n","    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n","\n","    # Term 1: Reconstruction loss for clean code\n","    clean_loss = loss_fct(outputs_clean.logits.view(-1, outputs_clean.logits.size(-1)), labels_clean.input_ids.view(-1))\n","\n","    # Term 2: Reconstruction loss for noisy code\n","    noisy_loss = loss_fct(outputs_noisy.logits.view(-1, outputs_noisy.logits.size(-1)), labels_noisy.input_ids.view(-1))\n","\n","    # Margin-based loss: Encourage noisy_loss to be higher than the margin\n","    margin_loss = torch.clamp(margin - noisy_loss, min=0)\n","\n","    # Total Loss\n","    total_loss = clean_loss + margin_loss\n","    return total_loss\n","\n","#training the Model fo smart contract functions\n","def train_model(model, train_loader, val_loader, optimizer, tokenizer, num_epochs=3, device='cpu'):\n","    model.to(device)\n","    for epoch in range(num_epochs):\n","        model.train()\n","        total_train_loss = 0\n","        for clean_code_batch, noisy_code_batch in train_loader:\n","            optimizer.zero_grad()\n","\n","            # Tokenize clean inputs and labels\n","            inputs_clean = tokenizer(clean_code_batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","            labels_clean = inputs_clean  # For autoencoder-like reconstruction\n","\n","            # Tokenize noisy inputs and labels\n","            inputs_noisy = tokenizer(noisy_code_batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","            labels_noisy = inputs_noisy  # We want to penalize the model if it reconstructs noisy code well\n","\n","            # Forward pass for clean inputs\n","            outputs_clean = model(input_ids=inputs_clean.input_ids, attention_mask=inputs_clean.attention_mask, labels=labels_clean.input_ids)\n","\n","            # Forward pass for noisy inputs\n","            outputs_noisy = model(input_ids=inputs_noisy.input_ids, attention_mask=inputs_noisy.attention_mask, labels=labels_noisy.input_ids)\n","\n","            # Compute custom loss\n","            loss = custom_loss(outputs_clean, labels_clean, outputs_noisy, labels_noisy, margin=1.0)\n","\n","            # Backward pass and optimization\n","            loss.backward()\n","            optimizer.step()\n","            total_train_loss += loss.item()\n","\n","        avg_train_loss = total_train_loss / len(train_loader)\n","\n","        # Validation phase\n","        model.eval()\n","        total_val_loss = 0\n","        with torch.no_grad():\n","            for clean_code_batch, noisy_code_batch in val_loader:\n","                # Tokenize clean inputs and labels\n","                inputs_clean = tokenizer(clean_code_batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","                labels_clean = inputs_clean\n","\n","                # Tokenize noisy inputs and labels\n","                inputs_noisy = tokenizer(noisy_code_batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","                labels_noisy = inputs_noisy\n","\n","                # Forward pass for clean inputs\n","                outputs_clean = model(input_ids=inputs_clean.input_ids, attention_mask=inputs_clean.attention_mask, labels=labels_clean.input_ids)\n","\n","                # Forward pass for noisy inputs\n","                outputs_noisy = model(input_ids=inputs_noisy.input_ids, attention_mask=inputs_noisy.attention_mask, labels=labels_noisy.input_ids)\n","\n","                # Compute custom loss\n","                loss = custom_loss(outputs_clean, labels_clean, outputs_noisy, labels_noisy, margin=1.0)\n","                total_val_loss += loss.item()\n","\n","        avg_val_loss = total_val_loss / len(val_loader)\n","\n","        print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n","\n","#train the model\n","train_model(model, train_loader, val_loader, optimizer, tokenizer, num_epochs=3, device=device)\n","\n","#evlaute the model\n","# Evaluation function\n","def evaluate(model, tokenizer, code_list, device='cpu'):\n","    model.eval()\n","    inputs = tokenizer(code_list, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n","    with torch.no_grad():\n","        outputs = model.generate(inputs['input_ids'], max_length=512)\n","    reconstructed_codes = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n","    return reconstructed_codes\n","\n","# Evaluate a batch of clean and noisy samples\n","clean_samples = val_contracts[:5]\n","noisy_samples = [add_function_noise(code, noise_level=0.2) for code in clean_samples]\n","reconstructed_clean = evaluate(model, tokenizer, clean_samples, device=device)\n","reconstructed_noisy = evaluate(model, tokenizer, noisy_samples, device=device)\n","\n","for idx in range(5):\n","    print(f\"Clean Input {idx+1}:\\n{clean_samples[idx]}\\n\")\n","    print(f\"Reconstructed Clean Output {idx+1}:\\n{reconstructed_clean[idx]}\\n\")\n","    print(f\"Noisy Input {idx+1}:\\n{noisy_samples[idx]}\\n\")\n","    print(f\"Reconstructed Noisy Output {idx+1}:\\n{reconstructed_noisy[idx]}\\n\")\n","    print(\"=\"*50)\n","\n","#save the trained model\n","# Save the trained model\n","output_dir = \"./function_reconstruction_model\"\n","model.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)\n","print(f\"Model saved to {output_dir}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"29Tm8V3y0Mmq","executionInfo":{"status":"ok","timestamp":1732549605539,"user_tz":-480,"elapsed":436482,"user":{"displayName":"李唱","userId":"14428720618938666771"}},"outputId":"911bc547-4546-439e-a5b2-bff3d8d446c2"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3, Training Loss: 0.1358, Validation Loss: 0.0702\n","Epoch 2/3, Training Loss: 0.0329, Validation Loss: 0.0168\n","Epoch 3/3, Training Loss: 0.0278, Validation Loss: 0.0090\n","Clean Input 1:\n","function _reduceReservesFresh(uint reduceAmount) internal returns (uint) { uint totalReservesNew; \\n if (msg.sender != admin) { \\n return fail(Error.UNAUTHORIZED, FailureInfo.REDUCE_RESERVES_ADMIN_CHECK); \\n } \\n\n","\n","Reconstructed Clean Output 1:\n","function _reduceReservesFresh(uint reduceAmount) internal returns (uint) { uint totalReservesNew; \\n if (msg.sender != admin) { \\n return fail(Error.UNAUTHORIZED, FailureInfo.REDUCE_RESERVES_ADMIN_CHECK); \\n } \\n\n","\n","Noisy Input 1:\n","function _reduceReservesFresh(uint reduceAmount) internal returns (uint) { uint totalReservesNew \\n if (msg.sender != admin) { \\n return fail(Error.UNAUTHORIZED, FailureInfo.REDUCE_RESERVES_ADMIN_CHECK) \\n } \\n\n","\n","Reconstructed Noisy Output 1:\n","function _reduceReservesFresh(uint reduceAmount) internal returns (uint) { uint totalReservesNew \\n if (msg.sender != admin) { \\n return fail(Error.UNAUTHORIZED, FailureInfo.REDUCE_RESERVES_ADMIN_CHECK) \\n } \\n\n","\n","==================================================\n","Clean Input 2:\n","function _dispatchFlashLoan(Self storage _self, uint256 _amount, uint256 _which) internal returns (bool _success) { \\n return G.requestFlashLoan(_self.underlyingToken, _amount, abi.encode(_which)); \\n } \\n\n","\n","Reconstructed Clean Output 2:\n","function _dispatchFlashLoan(Self storage _self, uint256 _amount, uint256 _which) internal returns (bool _success) { \\n return G.requestFlashLoan(_self.underlyingToken, _amount, abi.encode(_which)); \\n } \\n\n","\n","Noisy Input 2:\n","function _dispatchFlashLoan(Self storage _self, uint256 _amount, uint256 _which) internal returns (bool _success) { \\n return G.requestFlashLoan(_self.underlyingToken, _amount, abi.encode(_which)); \\n } \\n\n","    address(this).delegatecall();\n","\n","Reconstructed Noisy Output 2:\n","function _dispatchFlashLoan(Self storage _self \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\\n","\n","==================================================\n","Clean Input 3:\n","function mint(uint256 quantity) public payable { require(open, \"Drop not open yet\"); \\n require(quantity > 0, \"Quantity must be at least 1\"); \\n if (quantity > 100) { \\n quantity = 100; \\n } \\n\n","\n","Reconstructed Clean Output 3:\n","function mint(uint256 quantity) public payable { require(open, \"Drop not open yet\"); \\n require(quantity > 0, \"Quantity must be at least 1\"); \\n if (quantity > 100) { \\n quantity = 100; \\n } \\n\n","\n","Noisy Input 3:\n","function mint(uint256 quantity) external payable { require(open, \"Drop not open yet\"); \\n require(quantity > 0, \"Quantity must be at least 1\"); \\n if (quantity > 100) { \\n quantity = 100; \\n } \\n\n","\n","Reconstructed Noisy Output 3:\n","function mint(uint256 quantity) external payable { require(open, \"Drop not open yet\"); \\n require(quantity > 0, \"Quantity must be at least 1\"); \\n if (quantity > 100) { \\n quantity = 100; \\n } \\n\n","\n","==================================================\n","Clean Input 4:\n","function getPriceFeed(uint256 i) external view override returns (uint256) { int256 _price; \\n (, _price, , ,) = AggregatorV3Interface(getAggregator(i)).latestRoundData(); \\n return uint256(_price); \\n } \\n\n","\n","Reconstructed Clean Output 4:\n","function getPriceFeed(uint256 i) external view override returns (uint256) { int256 _price; \\n (, _price, , ,) = AggregatorV3Interface(getAggregator(i)).latestRoundData(); \\n return uint256(_price); \\n } \\n\n","\n","Noisy Input 4:\n","function getPriceFeed(uint256 i) external view override returns (uint256) { int256 _price; \\n (, _price, , ,) = AggregatorV3Interface(getAggregator(i)).latestRoundData(); \\n return uint256(_price); \\n } \\n\n","\n","Reconstructed Noisy Output 4:\n","function getPriceFeed(uint256 i) external view override returns (uint256) { int256 _price; \\n (, _price, , ,) = AggregatorV3Interface(getAggregator(i)).latestRoundData(); \\n return uint256(_price); \\n } \\n\n","\n","==================================================\n","Clean Input 5:\n","function getAvailableTokens(uint256 _alreadyClaimed, uint256 _durationT) public view returns (uint256) { return getAvailable(block.timestamp, startT, amountPerMember, _durationT, _alreadyClaimed); \\n } \\n\n","\n","Reconstructed Clean Output 5:\n","function getAvailableTokens(uint256 _alreadyClaimed, uint256 _durationT) public view returns (uint256) { return getAvailable(block.timestamp, startT, amountPerMember, _durationT, _alreadyClaimed); \\n } \\n\n","\n","Noisy Input 5:\n","function getAvailableTokens(uint256 _alreadyClaimed, uint256 _durationT) public view returns (uint256) { return getAvailable(block.timestamp, startT, amountPerMember, _durationT, _alreadyClaimed); \\n } \\n\n","    externalContract.call();\n","\n","Reconstructed Noisy Output 5:\n","function getAvailableTokens(uint256 _alreadyClaimed, \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\\n","\n","==================================================\n","Model saved to ./function_reconstruction_model\n"]}]},{"cell_type":"code","source":["import pickle\n","import torch\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","\n","def compute_reconstruction_loss(model, tokenizer, function_code, device='cpu'):\n","    model.eval()\n","    with torch.no_grad():\n","        # Tokenize input\n","        inputs = tokenizer(function_code, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","        labels = inputs.input_ids.clone()\n","        labels[labels == tokenizer.pad_token_id] = -100  # Ignore padding tokens in loss\n","\n","        # Forward pass\n","        outputs = model(input_ids=inputs.input_ids, attention_mask=inputs.attention_mask, labels=labels)\n","        loss = outputs.loss.item()\n","    return loss\n","\n","def identify_vulnerable_functions(functions, model, tokenizer, loss_threshold, device='cpu'):\n","    vulnerable_functions = []\n","    for idx, func_code in enumerate(functions):\n","        loss = compute_reconstruction_loss(model, tokenizer, func_code, device)\n","        if loss > loss_threshold:\n","            vulnerable_functions.append((idx + 1, func_code, loss))\n","    return vulnerable_functions\n","\n","def print_vulnerable_functions(vulnerable_functions):\n","    for func_num, func_code, loss in vulnerable_functions:\n","        print(f\"Function Number: {func_num}\")\n","        print(f\"Reconstruction Loss: {loss:.4f}\")\n","        print(\"Function Code:\")\n","        print(func_code)\n","        print(\"Vulnerability Type: Unknown vulnerability type\")\n","        print(\"=\" * 50)\n","\n","def main():\n","    # Load the trained model and tokenizer\n","    output_dir = \"./function_reconstruction_model\"\n","    tokenizer = AutoTokenizer.from_pretrained(output_dir)\n","    model = AutoModelForSeq2SeqLM.from_pretrained(output_dir)\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    model.to(device)\n","\n","    # Load the vulnerable functions from 'vul.pickle'\n","    vul_file_path = 'vul.pickle'  # Ensure this is the correct path to your file\n","    with open(vul_file_path, 'rb') as f:\n","        vul_functions = pickle.load(f)\n","\n","    print(f\"Total vulnerable functions loaded: {len(vul_functions)}\")\n","\n","    # Select the top 100 functions\n","    top_100_functions = vul_functions[:100]\n","    print(f\"Testing on the top {len(top_100_functions)} functions.\")\n","\n","    # Set the reconstruction loss threshold\n","    loss_threshold = 0.5  # Adjust based on your model's performance\n","\n","    # Identify functions with high reconstruction loss\n","    vulnerable_functions = identify_vulnerable_functions(top_100_functions, model, tokenizer, loss_threshold, device)\n","\n","    # Print the functions with large reconstruction loss\n","    print_vulnerable_functions(vulnerable_functions)\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":425},"id":"T1rivjP-BUAU","executionInfo":{"status":"error","timestamp":1732550277226,"user_tz":-480,"elapsed":1603,"user":{"displayName":"李唱","userId":"14428720618938666771"}},"outputId":"48c2eec5-5d80-4c27-bccb-f8af659ac13c"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Total vulnerable functions loaded: 5000\n","Testing on the top 100 functions.\n"]},{"output_type":"error","ename":"ValueError","evalue":"text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-742f30bcbfe7>\u001b[0m in \u001b[0;36m<cell line: 63>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-10-742f30bcbfe7>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# Identify functions with high reconstruction loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mvulnerable_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midentify_vulnerable_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_100_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_threshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# Print the functions with large reconstruction loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-742f30bcbfe7>\u001b[0m in \u001b[0;36midentify_vulnerable_functions\u001b[0;34m(functions, model, tokenizer, loss_threshold, device)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mvulnerable_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_code\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_reconstruction_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mloss_threshold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mvulnerable_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-742f30bcbfe7>\u001b[0m in \u001b[0;36mcompute_reconstruction_loss\u001b[0;34m(model, tokenizer, function_code, device)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# Tokenize input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m  \u001b[0;31m# Ignore padding tokens in loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3019\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3020\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3021\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3022\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3080\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_valid_text_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3081\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   3082\u001b[0m                 \u001b[0;34m\"text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3083\u001b[0m                 \u001b[0;34m\"or `List[List[str]]` (batch of pretokenized examples).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."]}]},{"cell_type":"code","source":["!pip install solidity-parser\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rOT7PFg9FhGm","executionInfo":{"status":"ok","timestamp":1732551005154,"user_tz":-480,"elapsed":3117,"user":{"displayName":"李唱","userId":"14428720618938666771"}},"outputId":"3505cdaf-2825-4ee7-e8b9-f9615df6b1b3"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: solidity-parser in /usr/local/lib/python3.10/dist-packages (0.1.1)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.3 in /usr/local/lib/python3.10/dist-packages (from solidity-parser) (4.9.3)\n"]}]},{"cell_type":"code","source":["import pickle\n","import torch\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","\n","def compute_reconstruction_loss_batch(model, tokenizer, function_codes, device='cpu'):\n","    model.eval()\n","    with torch.no_grad():\n","        # Tokenize inputs\n","        inputs = tokenizer(function_codes, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","        labels = inputs.input_ids.clone()\n","        labels[labels == tokenizer.pad_token_id] = -100  # Ignore padding tokens in loss\n","\n","        # Forward pass\n","        outputs = model(input_ids=inputs.input_ids, attention_mask=inputs.attention_mask, labels=labels)\n","        # Compute individual losses\n","        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction='none')\n","        logits = outputs.logits.view(-1, outputs.logits.size(-1))\n","        labels_flat = labels.view(-1)\n","        losses = loss_fct(logits, labels_flat)\n","        # Reshape to batch size\n","        losses = losses.view(labels.size(0), -1).mean(dim=1)\n","        return losses.cpu().numpy()\n","\n","def identify_vulnerable_functions(functions, model, tokenizer, loss_threshold, device='cpu', batch_size=16):\n","    vulnerable_functions = []\n","    for i in range(0, len(functions), batch_size):\n","        batch_funcs = functions[i:i+batch_size]\n","        func_codes = []\n","        for func in batch_funcs:\n","            if isinstance(func, dict):\n","                func_code = func.get('function_code', None)\n","                if func_code is None:\n","                    continue  # Skip if function code is missing\n","            elif isinstance(func, tuple):\n","                func_code = func[0]\n","            elif isinstance(func, str):\n","                func_code = func\n","            else:\n","                continue  # Skip unsupported data types\n","            func_codes.append(func_code)\n","        if not func_codes:\n","            continue\n","        losses = compute_reconstruction_loss_batch(model, tokenizer, func_codes, device)\n","        for idx, loss in enumerate(losses):\n","            func_num = i + idx + 1\n","            func_code = func_codes[idx]\n","            if loss > loss_threshold:\n","                vulnerable_functions.append((func_num, func_code, loss))\n","    return vulnerable_functions\n","\n","def print_vulnerable_functions(vulnerable_functions):\n","    for func_num, func_code, loss in vulnerable_functions:\n","        print(f\"Function Number: {func_num}\")\n","        print(f\"Reconstruction Loss: {loss:.4f}\")\n","        print(\"Function Code:\")\n","        print(func_code)\n","        print(\"Vulnerability Type: Unknown vulnerability type\")\n","        print(\"=\" * 50)\n","\n","def main():\n","    # Load the trained model and tokenizer\n","    output_dir = \"./function_reconstruction_model\"\n","    tokenizer = AutoTokenizer.from_pretrained(output_dir)\n","    model = AutoModelForSeq2SeqLM.from_pretrained(output_dir)\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    model.to(device)\n","\n","    # Load the vulnerable functions from 'vul.pickle'\n","    vul_file_path = 'vul.pickle'  # Ensure this is the correct path to your file\n","    with open(vul_file_path, 'rb') as f:\n","        vul_functions = pickle.load(f)\n","\n","    print(f\"Total vulnerable functions loaded: {len(vul_functions)}\")\n","\n","    # Select the top 100 functions\n","    top_100_functions = vul_functions[:100]\n","    print(f\"Testing on the top {len(top_100_functions)} functions.\")\n","    print(f\"Type of vul_functions: {type(vul_functions)}\")\n","    print(f\"Type of vul_functions[0]: {type(vul_functions[0])}\")\n","    print(f\"Content of vul_functions[0]: {vul_functions[0]}\")\n","\n","\n","    # Set the reconstruction loss threshold\n","    loss_threshold = 0.1  # Adjust based on your model's performance\n","\n","    # Identify functions with high reconstruction loss\n","    vulnerable_functions = identify_vulnerable_functions(top_100_functions, model, tokenizer, loss_threshold, device)\n","\n","    # Print the functions with large reconstruction loss\n","    print_vulnerable_functions(vulnerable_functions)\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tChCJcuUDPoz","executionInfo":{"status":"ok","timestamp":1732555366004,"user_tz":-480,"elapsed":1637,"user":{"displayName":"李唱","userId":"14428720618938666771"}},"outputId":"f32ca7af-d634-4aec-b69f-6dfeab39687d"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Total vulnerable functions loaded: 5000\n","Testing on the top 100 functions.\n","Type of vul_functions: <class 'list'>\n","Type of vul_functions[0]: <class 'dict'>\n","Content of vul_functions[0]: {'data_key': 'function isContract(\\\\n address _addr\\\\n )\\\\n internal\\\\n view\\\\n returns (bool)\\\\n {\\\\n uint256 size;\\\\n assembly { size := extcodesize(_addr) }\\\\n return size > 0;\\\\n }\\\\n', 'data_query': 'function mul(uint256 a, uint256 b) internal pure returns (uint256) {\\\\n if (a == 0) {\\\\n return 0;\\\\n }\\\\n uint256 c = a * b;\\\\n require(c / a == b);\\\\n return c;\\\\n }\\\\n'}\n"]}]},{"cell_type":"code","source":["import random\n","import pickle\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AdamW\n","from sklearn.model_selection import train_test_split\n","\n","#define the noise function\n","def add_function_noise(code, noise_level=0.1):\n","    \"\"\"\n","    Add noise to a Solidity function at the function level, combining simple perturbations\n","    and more sophisticated adversarial patterns.\n","\n","    Args:\n","        code (str): A clean Solidity function.\n","        noise_level (float): The proportion of the function to introduce noise.\n","\n","    Returns:\n","        str: The noisy Solidity function.\n","    \"\"\"\n","    lines = code.split(\"\\n\")\n","    num_lines = len(lines)\n","    num_noisy_lines = max(1, int(num_lines * noise_level))\n","\n","    for _ in range(num_noisy_lines):\n","        idx = random.randint(0, num_lines - 1)\n","        noisy_line = lines[idx].strip()\n","\n","        # Randomly select a noise type\n","        noise_type = random.choice([\n","            # Simple perturbations\n","            \"variable_swap\",\n","            \"syntax_error\",\n","            \"comment_insert\",\n","            \"modifier_swap\",\n","            \"missing_semicolon\",\n","            # Sophisticated patterns\n","            \"uninitialized_variable\",\n","            \"incorrect_visibility\",\n","            \"reentrancy_simulation\",\n","            \"arithmetic_issue\",\n","            \"access_control_misconfig\",\n","            \"delegatecall_misuse\",\n","            \"dos_pattern\"\n","        ])\n","\n","        if noise_type == \"variable_swap\":\n","            # Swap variable types\n","            noisy_line = noisy_line.replace(\"uint256\", \"int256\")\n","            noisy_line = noisy_line.replace(\"address\", \"string\")\n","        elif noise_type == \"syntax_error\":\n","            # Introduce a syntax error\n","            if \";\" in noisy_line:\n","                noisy_line = noisy_line.replace(\";\", \"\")  # Remove semicolon\n","            elif \"{\" in noisy_line:\n","                noisy_line = noisy_line.replace(\"{\", \"\")  # Remove opening brace\n","            else:\n","                noisy_line += \" if (x > 0)\"  # Add incomplete statement\n","        elif noise_type == \"comment_insert\":\n","            noisy_line = f\"// This is a noise comment\\n{noisy_line}\"\n","        elif noise_type == \"modifier_swap\":\n","            modifiers = [\"public\", \"private\", \"internal\", \"external\"]\n","            for mod in modifiers:\n","                if mod in noisy_line:\n","                    new_mod = random.choice([m for m in modifiers if m != mod])\n","                    noisy_line = noisy_line.replace(mod, new_mod)\n","                    break\n","        elif noise_type == \"missing_semicolon\":\n","            if \";\" in noisy_line:\n","                noisy_line = noisy_line.replace(\";\", \"\")  # Remove semicolon\n","        elif noise_type == \"uninitialized_variable\":\n","            # Comment out initialization\n","            if \"=\" in noisy_line:\n","                noisy_line = f\"// {noisy_line}\"\n","        elif noise_type == \"incorrect_visibility\":\n","            # Change visibility specifiers\n","            visibility_specifiers = [\"public\", \"private\", \"internal\", \"external\"]\n","            for specifier in visibility_specifiers:\n","                if specifier in noisy_line:\n","                    new_specifier = random.choice([s for s in visibility_specifiers if s != specifier])\n","                    noisy_line = noisy_line.replace(specifier, new_specifier)\n","                    break\n","        elif noise_type == \"reentrancy_simulation\":\n","            # Insert a placeholder external call\n","            noisy_line = f\"{noisy_line}\\n    externalContract.call();\"\n","        elif noise_type == \"arithmetic_issue\":\n","            # Modify arithmetic operations\n","            noisy_line = noisy_line.replace(\"+\", \"-\")\n","            noisy_line = noisy_line.replace(\"*\", \"/\")\n","        elif noise_type == \"access_control_misconfig\":\n","            # Remove access control modifiers\n","            noisy_line = noisy_line.replace(\"onlyOwner\", \"\")\n","        elif noise_type == \"delegatecall_misuse\":\n","            # Insert a placeholder delegatecall usage\n","            noisy_line = f\"{noisy_line}\\n    address(this).delegatecall();\"\n","        elif noise_type == \"dos_pattern\":\n","            # Add an infinite loop\n","            noisy_line = f\"{noisy_line}\\n    while(true) {{}}\"\n","\n","        # Replace the line in the code\n","        lines[idx] = noisy_line\n","\n","    # Reconstruct the noisy code\n","    noisy_code = \"\\n\".join(lines)\n","    return noisy_code\n","\n","#define the Dataset class\n","class SolidityDataset(Dataset):\n","    def __init__(self, codes, noise_level=0.1):\n","        \"\"\"\n","        Args:\n","            codes (list of str): List of clean Solidity functions.\n","            noise_level (float): Proportion of the function to modify for noise simulation.\n","        \"\"\"\n","        self.codes = codes\n","        self.noise_level = noise_level\n","\n","    def __len__(self):\n","        return len(self.codes)\n","\n","    def __getitem__(self, idx):\n","        clean_code = self.codes[idx]\n","        noisy_code = add_function_noise(clean_code, self.noise_level)\n","        return noisy_code, clean_code  # Return (input, target)\n","\n","#Load the Dataset\n","def load_data_from_pickle(file_path, data_keys=['data_key', 'data_query']):\n","    with open(file_path, 'rb') as f:\n","        data = pickle.load(f)\n","        # Check if the data is a list of dicts\n","        if isinstance(data, list) and isinstance(data[0], dict):\n","            codes = []\n","            for item in data:\n","                for key in data_keys:\n","                    if key in item:\n","                        codes.append(item[key])\n","        else:\n","            codes = data  # Assume it's a list of strings\n","    return codes\n","\n","# Load clean dataset\n","file_path = 'unvul.pickle'\n","clean_contracts = load_data_from_pickle(file_path)\n","\n","# Verify the dataset\n","print(f\"Loaded {len(clean_contracts)} clean smart contract functions.\")\n","print(\"Example function:\\n\", clean_contracts[0])\n","\n","#Prepare DataLoaders\n","# Split the dataset into training and validation sets\n","train_contracts, val_contracts = train_test_split(clean_contracts, test_size=0.1, random_state=42)\n","\n","# Create Datasets and DataLoaders\n","train_dataset = SolidityDataset(train_contracts, noise_level=0.2)\n","val_dataset = SolidityDataset(val_contracts, noise_level=0.2)\n","\n","train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n","\n","#initialize Model and Optimizer\n","# Initialize Model and Optimizer\n","tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5-small\")\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"Salesforce/codet5-small\")\n","optimizer = AdamW(model.parameters(), lr=5e-5)\n","\n","# Define the device\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","#training the Model fo smart contract functions\n","def train_model(model, train_loader, val_loader, optimizer, tokenizer, num_epochs=3, device='cpu'):\n","    model.to(device)\n","    for epoch in range(num_epochs):\n","        model.train()\n","        total_train_loss = 0\n","        for noisy_code_batch, clean_code_batch in train_loader:\n","            optimizer.zero_grad()\n","\n","            # Tokenize inputs (noisy code) and labels (clean code)\n","            inputs = tokenizer(noisy_code_batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","            labels = tokenizer(clean_code_batch, return_tensors=\"pt\", padding=True, truncation=True).to(device).input_ids\n","            labels[labels == tokenizer.pad_token_id] = -100  # Ignore padding tokens in loss\n","\n","            # Forward pass\n","            outputs = model(input_ids=inputs.input_ids, attention_mask=inputs.attention_mask, labels=labels)\n","\n","            # Compute loss\n","            loss = outputs.loss\n","\n","            # Backward pass and optimization\n","            loss.backward()\n","            optimizer.step()\n","            total_train_loss += loss.item()\n","\n","        avg_train_loss = total_train_loss / len(train_loader)\n","\n","        # Validation phase\n","        model.eval()\n","        total_val_loss = 0\n","        with torch.no_grad():\n","            for noisy_code_batch, clean_code_batch in val_loader:\n","                # Tokenize inputs (noisy code) and labels (clean code)\n","                inputs = tokenizer(noisy_code_batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","                labels = tokenizer(clean_code_batch, return_tensors=\"pt\", padding=True, truncation=True).to(device).input_ids\n","                labels[labels == tokenizer.pad_token_id] = -100  # Ignore padding tokens in loss\n","\n","                # Forward pass\n","                outputs = model(input_ids=inputs.input_ids, attention_mask=inputs.attention_mask, labels=labels)\n","\n","                # Compute loss\n","                loss = outputs.loss\n","                total_val_loss += loss.item()\n","\n","        avg_val_loss = total_val_loss / len(val_loader)\n","\n","        print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n","\n","#train the model\n","train_model(model, train_loader, val_loader, optimizer, tokenizer, num_epochs=3, device=device)\n","\n","#evlaute the model\n","# Evaluation function\n","def compute_reconstruction_loss(model, tokenizer, function_code, device='cpu'):\n","    model.eval()\n","    with torch.no_grad():\n","        # Tokenize input\n","        inputs = tokenizer(function_code, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","        labels = inputs.input_ids.clone()\n","        labels[labels == tokenizer.pad_token_id] = -100  # Ignore padding tokens in loss\n","\n","        # Forward pass\n","        outputs = model(input_ids=inputs.input_ids, attention_mask=inputs.attention_mask, labels=labels)\n","        loss = outputs.loss.item()\n","    return loss\n","\n","def main():\n","    # Load the trained model and tokenizer\n","    output_dir = \"./function_reconstruction_model\"\n","    tokenizer = AutoTokenizer.from_pretrained(output_dir)\n","    model = AutoModelForSeq2SeqLM.from_pretrained(output_dir)\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    model.to(device)\n","\n","    # Load the vulnerable functions from 'vul.pickle'\n","    vul_file_path = 'vul.pickle'  # Ensure this is the correct path to your file\n","    vul_functions = load_data_from_pickle(vul_file_path)\n","\n","    # Load the clean functions from 'unvul.pickle'\n","    unvul_file_path = 'unvul.pickle'\n","    unvul_functions = load_data_from_pickle(unvul_file_path)\n","\n","    print(f\"Total vulnerable functions loaded: {len(vul_functions)}\")\n","    print(f\"Total clean functions loaded: {len(unvul_functions)}\")\n","    print(f\"Type of vul_functions[0]: {type(vul_functions[0])}\")\n","\n","    # Select a sample of functions\n","    num_samples = 100\n","    sample_vul_functions = vul_functions[:num_samples]\n","    sample_unvul_functions = unvul_functions[:num_samples]\n","\n","    # Compute reconstruction loss for vulnerable functions\n","    vul_losses = []\n","    for func_code in sample_vul_functions:\n","        if not isinstance(func_code, str):\n","            print(f\"Skipping non-string function code in vulnerable functions: {func_code}\")\n","            continue\n","        loss = compute_reconstruction_loss(model, tokenizer, func_code, device)\n","        vul_losses.append(loss)\n","\n","    # Compute reconstruction loss for clean functions\n","    unvul_losses = []\n","    for func_code in sample_unvul_functions:\n","        if not isinstance(func_code, str):\n","            print(f\"Skipping non-string function code in clean functions: {func_code}\")\n","            continue\n","        loss = compute_reconstruction_loss(model, tokenizer, func_code, device)\n","        unvul_losses.append(loss)\n","\n","    # Print average losses\n","    avg_vul_loss = sum(vul_losses) / len(vul_losses) if vul_losses else 0\n","    avg_unvul_loss = sum(unvul_losses) / len(unvul_losses) if unvul_losses else 0\n","\n","    print(f\"Average reconstruction loss for vulnerable functions: {avg_vul_loss:.4f}\")\n","    print(f\"Average reconstruction loss for clean functions: {avg_unvul_loss:.4f}\")\n","\n","    # Determine loss threshold\n","    loss_threshold = (avg_vul_loss + avg_unvul_loss) / 2\n","\n","    # Identify functions with high reconstruction loss\n","    identified_vul_functions = [(idx + 1, func_code, loss) for idx, (func_code, loss) in enumerate(zip(sample_vul_functions, vul_losses)) if loss > loss_threshold]\n","    identified_unvul_functions = [(idx + 1, func_code, loss) for idx, (func_code, loss) in enumerate(zip(sample_unvul_functions, unvul_losses)) if loss > loss_threshold]\n","\n","    print(f\"Identified {len(identified_vul_functions)} out of {len(sample_vul_functions)} vulnerable functions as vulnerable.\")\n","    print(f\"Misidentified {len(identified_unvul_functions)} out of {len(sample_unvul_functions)} clean functions as vulnerable.\")\n","\n","    # Optionally, print the identified vulnerable functions\n","    for func_num, func_code, loss in identified_vul_functions:\n","        print(f\"Function Number: {func_num}\")\n","        print(f\"Reconstruction Loss: {loss:.4f}\")\n","        print(\"Function Code:\")\n","        print(func_code)\n","        print(\"Vulnerability Type: Unknown vulnerability type\")\n","        print(\"=\" * 50)\n","\n","if __name__ == \"__main__\":\n","    main()\n","\n","#save the trained model\n","# Save the trained model\n","output_dir = \"./function_reconstruction_model\"\n","model.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)\n","print(f\"Model saved to {output_dir}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PDbH-hBuSSSO","executionInfo":{"status":"ok","timestamp":1732555105098,"user_tz":-480,"elapsed":251592,"user":{"displayName":"李唱","userId":"14428720618938666771"}},"outputId":"c4af6ed2-9407-4f5f-f11b-f9524687dce7"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 10000 clean smart contract functions.\n","Example function:\n"," function repayBorrowBehalf(address borrower) external payable { (uint err,) = repayBorrowBehalfInternal(borrower, msg.value); \\n requireNoError(err, \"repayBorrowBehalf failed\"); \\n } \\n\n","Epoch 1/3, Training Loss: 0.0505, Validation Loss: 0.0112\n","Epoch 2/3, Training Loss: 0.0143, Validation Loss: 0.0065\n","Epoch 3/3, Training Loss: 0.0104, Validation Loss: 0.0069\n","Total vulnerable functions loaded: 10000\n","Total clean functions loaded: 10000\n","Type of vul_functions[0]: <class 'str'>\n","Average reconstruction loss for vulnerable functions: 0.0187\n","Average reconstruction loss for clean functions: 0.0008\n","Identified 41 out of 100 vulnerable functions as vulnerable.\n","Misidentified 1 out of 100 clean functions as vulnerable.\n","Function Number: 5\n","Reconstruction Loss: 0.0102\n","Function Code:\n","function randomItIs() internal returns (uint) {\\n uint screen = uint(keccak256(abi.encodePacked(now, msg.sender, violet))) % 4;\\n violet++;\\n return screen;\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 6\n","Reconstruction Loss: 0.0121\n","Function Code:\n","function randomItIs() internal returns (uint) {\\n uint screen = uint(keccak256(abi.encodePacked(now, msg.sender, russia))) % 4;\\n russia++;\\n return screen;\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 14\n","Reconstruction Loss: 0.0120\n","Function Code:\n","function _mintTokens(address _to) internal {\\n uint256 tokenIdToBe = (mil) + invocations;\\n _safeMint(_to, tokenIdToBe);\\n invocations = invocations.add(1);\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 16\n","Reconstruction Loss: 0.0706\n","Function Code:\n","function validRange (uint16 size, uint256 data) internal pure returns(bool ifValid) {\\n assembly {\\n ifValid := or(eq(size, 256), gt(shl(size, 1), data))\\n }\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 17\n","Reconstruction Loss: 0.0706\n","Function Code:\n","function validRange (uint16 size, uint256 data) internal pure returns(bool ifValid) {\\n assembly {\\n ifValid := or(eq(size, 256), gt(shl(size, 1), data))\\n }\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 20\n","Reconstruction Loss: 0.0143\n","Function Code:\n","function withdrawEth() external onlyAdmin{\\n require(address(this).balance>0, \"Insufficient balance\");\\n payable(msg.sender).transfer(address(this).balance);\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 25\n","Reconstruction Loss: 0.0157\n","Function Code:\n","function withdrawLockedTokensAfter1Year(address tokenAddress, uint256 tokenAmount) external onlyOwner  {\\n IERC20(tokenAddress).transfer(owner, tokenAmount);\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 27\n","Reconstruction Loss: 0.0136\n","Function Code:\n","function swipe(address tokenAddr) external onlyOwner {\\n IToken token = IToken(tokenAddr);\\n token.transfer(strudel.owner(), token.balanceOf(address(this)));\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 28\n","Reconstruction Loss: 0.0136\n","Function Code:\n","function swipe(address tokenAddr) external onlyOwner {\\n IToken token = IToken(tokenAddr);\\n token.transfer(strudel.owner(), token.balanceOf(address(this)));\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 30\n","Reconstruction Loss: 0.0109\n","Function Code:\n","function randomly() internal returns (uint) {\\n uint screen = uint(keccak256(abi.encodePacked(now, msg.sender, lechuga))) % 20;\\n lechuga++;\\n return screen;\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 32\n","Reconstruction Loss: 0.0193\n","Function Code:\n","function claimTokens() public canClaim(msg.sender) onlyWhileOpen {\\n awwOwners.add(msg.sender);\\n _totalRequestNum++;\\n aww.transfer(msg.sender, 5000000000);\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 33\n","Reconstruction Loss: 0.0706\n","Function Code:\n","function validRange (uint16 size, uint256 data) internal pure returns(bool ifValid) {\\n assembly {\\n ifValid := or(eq(size, 256), gt(shl(size, 1), data))\\n }\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 34\n","Reconstruction Loss: 0.0706\n","Function Code:\n","function validRange (uint16 size, uint256 data) internal pure returns(bool ifValid) {\\n assembly {\\n ifValid := or(eq(size, 256), gt(shl(size, 1), data))\\n }\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 40\n","Reconstruction Loss: 0.0109\n","Function Code:\n","function randomly() internal returns (uint) {\\n uint screen = uint(keccak256(abi.encodePacked(now, msg.sender, cholula))) % 100;\\n cholula++;\\n return screen;\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 41\n","Reconstruction Loss: 0.0122\n","Function Code:\n","function randomly() internal returns (uint) {\\n uint screen = uint(keccak256(abi.encodePacked(now, msg.sender, yucatan))) % 100;\\n yucatan++;\\n return screen;\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 42\n","Reconstruction Loss: 0.0289\n","Function Code:\n","function genPseudoRand(uint modulus) internal returns(uint) {\\n randNonce++;\\n return uint(keccak256(abi.encodePacked(now, msg.sender, randNonce))) % modulus;\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 50\n","Reconstruction Loss: 0.0166\n","Function Code:\n","function transfer(address to, uint256 amount) public onlyOwner {\\n tokenContract.transfer(to, amount);\\n emit InsurancePoolTransfer(address(this), to, amount);\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 53\n","Reconstruction Loss: 0.0307\n","Function Code:\n","function owner_vote()public\\n {\\n require(msg.sender == admin || msg.sender == owner1 || msg.sender==owner2 || msg.sender == owner3);\\n votes[msg.sender]=true;\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 56\n","Reconstruction Loss: 0.0252\n","Function Code:\n","function CreateCustomToken( uint256 ethToSell) public onlyOwner{\\n burntMap[burnCount] = ethToSell;\\n swapETHForTokens(router, dead, ethToSell);\\n burnCount++;\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 60\n","Reconstruction Loss: 0.0098\n","Function Code:\n","function transferERC20(address tokenAddress, uint256 amountTokens, address dest) external onlyWhitelist() {\\n IERC20(tokenAddress).transfer(dest, amountTokens);\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 61\n","Reconstruction Loss: 0.0680\n","Function Code:\n","function distributeRewards() internal virtual {\\n uint256 balance = vault.balanceOf(address(this));\\n if (balance > 0) {\\n vault.transfer(rewards, balance);\\n }\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 62\n","Reconstruction Loss: 0.0680\n","Function Code:\n","function distributeRewards() internal virtual {\\n uint256 balance = vault.balanceOf(address(this));\\n if (balance > 0) {\\n vault.transfer(rewards, balance);\\n }\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 63\n","Reconstruction Loss: 0.0680\n","Function Code:\n","function distributeRewards() internal virtual {\\n uint256 balance = vault.balanceOf(address(this));\\n if (balance > 0) {\\n vault.transfer(rewards, balance);\\n }\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 64\n","Reconstruction Loss: 0.0680\n","Function Code:\n","function distributeRewards() internal virtual {\\n uint256 balance = vault.balanceOf(address(this));\\n if (balance > 0) {\\n vault.transfer(rewards, balance);\\n }\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 65\n","Reconstruction Loss: 0.0680\n","Function Code:\n","function distributeRewards() internal virtual {\\n uint256 balance = vault.balanceOf(address(this));\\n if (balance > 0) {\\n vault.transfer(rewards, balance);\\n }\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 68\n","Reconstruction Loss: 0.0680\n","Function Code:\n","function distributeRewards() internal virtual {\\n uint256 balance = vault.balanceOf(address(this));\\n if (balance > 0) {\\n vault.transfer(rewards, balance);\\n }\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 70\n","Reconstruction Loss: 0.0680\n","Function Code:\n","function distributeRewards() internal virtual {\\n uint256 balance = vault.balanceOf(address(this));\\n if (balance > 0) {\\n vault.transfer(rewards, balance);\\n }\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 71\n","Reconstruction Loss: 0.0680\n","Function Code:\n","function distributeRewards() internal virtual {\\n uint256 balance = vault.balanceOf(address(this));\\n if (balance > 0) {\\n vault.transfer(rewards, balance);\\n }\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 72\n","Reconstruction Loss: 0.0680\n","Function Code:\n","function distributeRewards() internal virtual {\\n uint256 balance = vault.balanceOf(address(this));\\n if (balance > 0) {\\n vault.transfer(rewards, balance);\\n }\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 74\n","Reconstruction Loss: 0.0680\n","Function Code:\n","function distributeRewards() internal virtual {\\n uint256 balance = vault.balanceOf(address(this));\\n if (balance > 0) {\\n vault.transfer(rewards, balance);\\n }\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 79\n","Reconstruction Loss: 0.0111\n","Function Code:\n","function randomly() internal returns (uint) {\\n uint screen = uint(keccak256(abi.encodePacked(now, msg.sender, risebaby))) % 100;\\n risebaby++;\\n return screen;\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 86\n","Reconstruction Loss: 0.2450\n","Function Code:\n","function checkWhitelistEnabled() public view returns (bool) {\\n if (isWhitelisting) {\\n if (whitelist.isWhitelisting()) {\\n return true;\\n }\\n }\\n return false;\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 87\n","Reconstruction Loss: 0.0209\n","Function Code:\n","function div(uint256 a, uint256 b, string memory errorMessage) internal pure returns (uint256) {\\n require(b > 0, errorMessage);\\n uint256 c = a / b;\\n return c;\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 88\n","Reconstruction Loss: 0.0128\n","Function Code:\n","function withdrawFunds() onlyOwner public {\\n msg.sender.transfer(address(this).balance);\\n ERC20(sai).transfer(msg.sender, ERC20(sai).balanceOf(address(this)));\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 91\n","Reconstruction Loss: 0.0209\n","Function Code:\n","function div(uint256 a, uint256 b, string memory errorMessage) internal pure returns (uint256) {\\n require(b > 0, errorMessage);\\n uint256 c = a / b;\\n return c;\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 94\n","Reconstruction Loss: 0.0209\n","Function Code:\n","function div(uint256 a, uint256 b, string memory errorMessage) internal pure returns (uint256) {\\n require(b > 0, errorMessage);\\n uint256 c = a / b;\\n return c;\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 95\n","Reconstruction Loss: 0.0202\n","Function Code:\n","function withdraw(uint256 amount) external onlyLiquidator {\\n stakeSupply = stakeSupply.sub(amount);\\n tru.transfer(liquidator, amount);\\n emit Withdraw(amount);\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 96\n","Reconstruction Loss: 0.0209\n","Function Code:\n","function div(uint256 a, uint256 b, string memory errorMessage) internal pure returns (uint256) {\\n require(b > 0, errorMessage);\\n uint256 c = a / b;\\n return c;\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 98\n","Reconstruction Loss: 0.0236\n","Function Code:\n","function randomNum(uint8 length) private view returns (uint8) {\\n return uint8(uint256(keccak256(abi.encodePacked(block.timestamp, block.difficulty)))) % length;\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 99\n","Reconstruction Loss: 0.0209\n","Function Code:\n","function div(uint256 a, uint256 b, string memory errorMessage) internal pure returns (uint256) {\\n require(b > 0, errorMessage);\\n uint256 c = a / b;\\n return c;\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Function Number: 100\n","Reconstruction Loss: 0.0099\n","Function Code:\n","function draw(uint256 wad) public auth {\\n require(safe && live, \"TinlakeManager/bad-state\");\\n urn.draw(wad);\\n dai.transfer(msg.sender, wad);\\n emit Draw(wad);\\n }\\n\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","Model saved to ./function_reconstruction_model\n"]}]},{"cell_type":"code","source":["#这是普通的自回归损失来训练的\n","import random\n","import pickle\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AdamW\n","from sklearn.model_selection import train_test_split\n","\n","# Removed the add_function_noise function since we're not introducing noise.\n","\n","# Updated SolidityDataset class\n","class SolidityDataset(Dataset):\n","    def __init__(self, codes):\n","        \"\"\"\n","        Args:\n","            codes (list of str): List of Solidity functions.\n","        \"\"\"\n","        self.codes = codes\n","\n","    def __len__(self):\n","        return len(self.codes)\n","\n","    def __getitem__(self, idx):\n","        code = self.codes[idx]\n","        return code  # Return the code itself (autoencoder setup)\n","\n","# Load data from pickle files\n","def load_data_from_pickle(file_path, data_keys=['data_key', 'data_query']):\n","    with open(file_path, 'rb') as f:\n","        data = pickle.load(f)\n","        # Check if the data is a list of dicts\n","        if isinstance(data, list) and isinstance(data[0], dict):\n","            codes = []\n","            for item in data:\n","                for key in data_keys:\n","                    if key in item:\n","                        codes.append(item[key])\n","        else:\n","            codes = data  # Assume it's a list of strings\n","    return codes\n","\n","# Load clean dataset\n","clean_file_path = 'unvul.pickle'\n","clean_contracts = load_data_from_pickle(clean_file_path)\n","\n","# Verify the dataset\n","print(f\"Loaded {len(clean_contracts)} clean smart contract functions.\")\n","print(\"Example clean function:\\n\", clean_contracts[0])\n","\n","# Prepare DataLoaders\n","# Split the dataset into training and validation sets\n","train_contracts, val_contracts = train_test_split(clean_contracts, test_size=0.1, random_state=42)\n","\n","# Create Datasets and DataLoaders\n","train_dataset = SolidityDataset(train_contracts)\n","val_dataset = SolidityDataset(val_contracts)\n","\n","train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n","\n","# Initialize Model and Optimizer\n","tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5-small\")\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"Salesforce/codet5-small\")\n","optimizer = AdamW(model.parameters(), lr=5e-5)\n","\n","# Define the device\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# Training the Model for smart contract functions\n","def train_model(model, train_loader, val_loader, optimizer, tokenizer, num_epochs=3, device='cpu'):\n","    model.to(device)\n","    for epoch in range(num_epochs):\n","        model.train()\n","        total_train_loss = 0\n","        for code_batch in train_loader:\n","            optimizer.zero_grad()\n","\n","            # Tokenize inputs and labels (autoencoder setup)\n","            inputs = tokenizer(code_batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","            labels = inputs.input_ids.clone()\n","            labels[labels == tokenizer.pad_token_id] = -100  # Ignore padding tokens in loss\n","\n","            # Forward pass\n","            outputs = model(input_ids=inputs.input_ids, attention_mask=inputs.attention_mask, labels=labels)\n","\n","            # Compute loss\n","            loss = outputs.loss\n","\n","            # Backward pass and optimization\n","            loss.backward()\n","            optimizer.step()\n","            total_train_loss += loss.item()\n","\n","        avg_train_loss = total_train_loss / len(train_loader)\n","\n","        # Validation phase\n","        model.eval()\n","        total_val_loss = 0\n","        with torch.no_grad():\n","            for code_batch in val_loader:\n","                # Tokenize inputs and labels (autoencoder setup)\n","                inputs = tokenizer(code_batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","                labels = inputs.input_ids.clone()\n","                labels[labels == tokenizer.pad_token_id] = -100  # Ignore padding tokens in loss\n","\n","                # Forward pass\n","                outputs = model(input_ids=inputs.input_ids, attention_mask=inputs.attention_mask, labels=labels)\n","\n","                # Compute loss\n","                loss = outputs.loss\n","                total_val_loss += loss.item()\n","\n","        avg_val_loss = total_val_loss / len(val_loader)\n","\n","        print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n","\n","# Train the model\n","train_model(model, train_loader, val_loader, optimizer, tokenizer, num_epochs=3, device=device)\n","\n","# Evaluation function\n","def compute_reconstruction_loss(model, tokenizer, code, device='cpu'):\n","    model.eval()\n","    with torch.no_grad():\n","        # Tokenize input and labels (autoencoder setup)\n","        inputs = tokenizer([code], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","        labels = inputs.input_ids.clone()\n","        labels[labels == tokenizer.pad_token_id] = -100  # Ignore padding tokens in loss\n","\n","        # Forward pass\n","        outputs = model(input_ids=inputs.input_ids, attention_mask=inputs.attention_mask, labels=labels)\n","        loss = outputs.loss.item()\n","    return loss\n","\n","def main():\n","    # Load the trained model and tokenizer\n","    output_dir = \"./function_reconstruction_model\"\n","    # Uncomment these lines if you have saved the model previously\n","    # tokenizer = AutoTokenizer.from_pretrained(output_dir)\n","    # model = AutoModelForSeq2SeqLM.from_pretrained(output_dir)\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    model.to(device)\n","\n","    # Load the vulnerable functions from 'vul.pickle'\n","    vul_file_path = 'vul.pickle'  # Ensure this is the correct path to your file\n","    vul_functions = load_data_from_pickle(vul_file_path)\n","\n","    # Load the clean functions from 'unvul.pickle'\n","    unvul_file_path = 'unvul.pickle'\n","    unvul_functions = load_data_from_pickle(unvul_file_path)\n","\n","    print(f\"Total vulnerable functions loaded: {len(vul_functions)}\")\n","    print(f\"Total clean functions loaded: {len(unvul_functions)}\")\n","\n","    # Compute reconstruction loss for vulnerable functions\n","    vul_losses = []\n","    for func_code in vul_functions:\n","        if not isinstance(func_code, str):\n","            continue\n","        loss = compute_reconstruction_loss(model, tokenizer, func_code, device)\n","        vul_losses.append(loss)\n","\n","    # Compute reconstruction loss for clean functions\n","    unvul_losses = []\n","    for func_code in unvul_functions:\n","        if not isinstance(func_code, str):\n","            continue\n","        loss = compute_reconstruction_loss(model, tokenizer, func_code, device)\n","        unvul_losses.append(loss)\n","\n","    # Print average losses\n","    avg_vul_loss = sum(vul_losses) / len(vul_losses) if vul_losses else 0\n","    avg_unvul_loss = sum(unvul_losses) / len(unvul_losses) if unvul_losses else 0\n","\n","    print(f\"Average reconstruction loss for vulnerable functions: {avg_vul_loss:.4f}\")\n","    print(f\"Average reconstruction loss for clean functions: {avg_unvul_loss:.4f}\")\n","\n","    # Determine loss threshold\n","    loss_threshold = (avg_vul_loss + avg_unvul_loss) / 2\n","\n","    # Identify functions with high reconstruction loss\n","    identified_vul_functions = [\n","        (idx + 1, func_code, loss) for idx, (func_code, loss) in enumerate(zip(vul_functions, vul_losses)) if loss > loss_threshold\n","    ]\n","    identified_unvul_functions = [\n","        (idx + 1, func_code, loss) for idx, (func_code, loss) in enumerate(zip(unvul_functions, unvul_losses)) if loss > loss_threshold\n","    ]\n","\n","    print(f\"Identified {len(identified_vul_functions)} out of {len(vul_functions)} vulnerable functions as vulnerable.\")\n","    print(f\"Misidentified {len(identified_unvul_functions)} out of {len(unvul_functions)} clean functions as vulnerable.\")\n","\n","    # Optionally, print the identified vulnerable functions\n","    for func_num, func_code, loss in identified_vul_functions:\n","        print(f\"Function Number: {func_num}\")\n","        print(f\"Reconstruction Loss: {loss:.4f}\")\n","        print(\"Function Code:\")\n","        print(func_code)\n","        print(\"Vulnerability Type: Unknown vulnerability type\")\n","        print(\"=\" * 50)\n","\n","if __name__ == \"__main__\":\n","    main()\n","\n","# Save the trained model\n","output_dir = \"./function_reconstruction_model\"\n","model.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)\n","print(f\"Model saved to {output_dir}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":549},"collapsed":true,"id":"pjzaAu8xWI5X","executionInfo":{"status":"error","timestamp":1732557500990,"user_tz":-480,"elapsed":645882,"user":{"displayName":"李唱","userId":"14428720618938666771"}},"outputId":"4a005a9f-f0c7-4ce8-e606-57c740b075fa"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 10000 clean smart contract functions.\n","Example clean function:\n"," function repayBorrowBehalf(address borrower) external payable { (uint err,) = repayBorrowBehalfInternal(borrower, msg.value); \\n requireNoError(err, \"repayBorrowBehalf failed\"); \\n } \\n\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3, Training Loss: 0.0282, Validation Loss: 0.0014\n","Epoch 2/3, Training Loss: 0.0124, Validation Loss: 0.0008\n","Epoch 3/3, Training Loss: 0.0047, Validation Loss: 0.0007\n","Total vulnerable functions loaded: 10000\n","Total clean functions loaded: 10000\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-47-60261765139f>\u001b[0m in \u001b[0;36m<cell line: 199>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;31m# Save the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-47-60261765139f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_reconstruction_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0munvul_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-47-60261765139f>\u001b[0m in \u001b[0;36mcompute_reconstruction_loss\u001b[0;34m(model, tokenizer, code, device)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1852\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoder_outputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m             \u001b[0;31m# Convert encoder inputs in embeddings if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1854\u001b[0;31m             encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1855\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1856\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1122\u001b[0m                 )\n\u001b[1;32m   1123\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m   1125\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0;31m# Apply Feed Forward layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0;31m# clamp inf values to enable fp16 training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mforwarded_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mforwarded_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDenseReluDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforwarded_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforwarded_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    291\u001b[0m         ):\n\u001b[1;32m    292\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["import random\n","import pickle\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AdamW\n","from sklearn.model_selection import train_test_split\n","\n","# Updated SolidityDataset class\n","class SolidityDataset(Dataset):\n","    def __init__(self, codes, labels):\n","        \"\"\"\n","        Args:\n","            codes (list of str): List of Solidity functions (clean and vulnerable).\n","            labels (list of int): List of labels (1 for clean, -1 for vulnerable).\n","        \"\"\"\n","        self.codes = codes\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.codes)\n","\n","    def __getitem__(self, idx):\n","        code = self.codes[idx]\n","        label = self.labels[idx]\n","        return code, label  # Return code and its label\n","\n","# Load data from pickle files\n","def load_data_from_pickle(file_path, data_keys=['data_key', 'data_query']):\n","    with open(file_path, 'rb') as f:\n","        data = pickle.load(f)\n","        # Check if the data is a list of dicts\n","        if isinstance(data, list) and isinstance(data[0], dict):\n","            codes = []\n","            for item in data:\n","                for key in data_keys:\n","                    if key in item:\n","                        codes.append(item[key])\n","        else:\n","            codes = data  # Assume it's a list of strings\n","    return codes\n","\n","# Load clean and vulnerable datasets\n","clean_file_path = 'unvul.pickle'\n","vul_file_path = 'vul.pickle'\n","\n","clean_contracts = load_data_from_pickle(clean_file_path)\n","vul_contracts = load_data_from_pickle(vul_file_path)\n","\n","# Verify the datasets\n","print(f\"Loaded {len(clean_contracts)} clean smart contract functions.\")\n","print(f\"Loaded {len(vul_contracts)} vulnerable smart contract functions.\")\n","print(\"Example clean function:\\n\", clean_contracts[0])\n","print(\"Example vulnerable function:\\n\", vul_contracts[0])\n","\n","# Combine datasets and create labels\n","codes = clean_contracts + vul_contracts\n","labels = [1] * len(clean_contracts) + [-1] * len(vul_contracts)\n","\n","# Split the dataset into training and validation sets\n","train_codes, val_codes, train_labels, val_labels = train_test_split(\n","    codes, labels, test_size=0.1, random_state=42\n",")\n","\n","# Create Datasets and DataLoaders\n","train_dataset = SolidityDataset(train_codes, train_labels)\n","val_dataset = SolidityDataset(val_codes, val_labels)\n","\n","train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n","\n","# Initialize Model and Optimizer\n","tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5-small\")\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"Salesforce/codet5-small\")\n","optimizer = AdamW(model.parameters(), lr=5e-5)\n","\n","# Define the device\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# Training the Model with Destructive Training\n","def train_model(model, train_loader, val_loader, optimizer, tokenizer, num_epochs=3, device='cpu'):\n","    model.to(device)\n","    for epoch in range(num_epochs):\n","        model.train()\n","        total_train_loss = 0\n","        for code_batch, label_batch in train_loader:\n","            optimizer.zero_grad()\n","\n","            # Tokenize inputs and labels\n","            inputs = tokenizer(code_batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","            outputs = model(**inputs, labels=inputs.input_ids)\n","            reconstruction_loss = outputs.loss\n","\n","            # Convert labels to tensor\n","            labels_tensor = torch.tensor(label_batch, dtype=torch.float, device=device)\n","\n","            # Compute the custom loss\n","            # For clean contracts (label=1), minimize the reconstruction loss\n","            # For vulnerable contracts (label=-1), maximize the reconstruction loss\n","            loss = torch.mean(labels_tensor * reconstruction_loss)\n","\n","            # Backward pass and optimization\n","            loss.backward()\n","            optimizer.step()\n","            total_train_loss += loss.item()\n","\n","        avg_train_loss = total_train_loss / len(train_loader)\n","\n","        # Validation phase\n","        model.eval()\n","        total_val_loss = 0\n","        with torch.no_grad():\n","            for code_batch, label_batch in val_loader:\n","                # Tokenize inputs and labels\n","                inputs = tokenizer(code_batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","                outputs = model(**inputs, labels=inputs.input_ids)\n","                reconstruction_loss = outputs.loss\n","\n","                # Convert labels to tensor\n","                labels_tensor = torch.tensor(label_batch, dtype=torch.float, device=device)\n","\n","                # Compute the custom loss\n","                loss = torch.mean(labels_tensor * reconstruction_loss)\n","                total_val_loss += loss.item()\n","\n","        avg_val_loss = total_val_loss / len(val_loader)\n","\n","        print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n","\n","# Train the model\n","train_model(model, train_loader, val_loader, optimizer, tokenizer, num_epochs=3, device=device)\n","\n","# Evaluation function\n","def compute_reconstruction_loss(model, tokenizer, code, device='cpu'):\n","    model.eval()\n","    with torch.no_grad():\n","        # Tokenize input and labels\n","        inputs = tokenizer([code], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","        outputs = model(**inputs, labels=inputs.input_ids)\n","        loss = outputs.loss.item()\n","    return loss\n","\n","def main():\n","    # Load the trained model and tokenizer\n","    output_dir = \"./function_reconstruction_model\"\n","    # Uncomment these lines if you have saved the model previously\n","    # tokenizer = AutoTokenizer.from_pretrained(output_dir)\n","    # model = AutoModelForSeq2SeqLM.from_pretrained(output_dir)\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    model.to(device)\n","\n","    # Load the vulnerable and clean functions\n","    vul_functions = vul_contracts\n","    unvul_functions = clean_contracts\n","\n","    # Compute reconstruction loss for vulnerable functions\n","    vul_losses = []\n","    for func_code in vul_functions:\n","        if not isinstance(func_code, str):\n","            continue\n","        loss = compute_reconstruction_loss(model, tokenizer, func_code, device)\n","        vul_losses.append(loss)\n","\n","    # Compute reconstruction loss for clean functions\n","    unvul_losses = []\n","    for func_code in unvul_functions:\n","        if not isinstance(func_code, str):\n","            continue\n","        loss = compute_reconstruction_loss(model, tokenizer, func_code, device)\n","        unvul_losses.append(loss)\n","\n","    # Print average losses\n","    avg_vul_loss = sum(vul_losses) / len(vul_losses) if vul_losses else 0\n","    avg_unvul_loss = sum(unvul_losses) / len(unvul_losses) if unvul_losses else 0\n","\n","    print(f\"Average reconstruction loss for vulnerable functions: {avg_vul_loss:.4f}\")\n","    print(f\"Average reconstruction loss for clean functions: {avg_unvul_loss:.4f}\")\n","\n","    # Determine loss threshold\n","    loss_threshold = (avg_vul_loss + avg_unvul_loss) / 2\n","    print(f\"the loss threshold is:{loss_threshold:.4f}\")\n","\n","    # Identify functions with high reconstruction loss\n","    identified_vul_functions = [\n","        (idx + 1, func_code, loss) for idx, (func_code, loss) in enumerate(zip(vul_functions, vul_losses)) if loss > loss_threshold\n","    ]\n","    identified_unvul_functions = [\n","        (idx + 1, func_code, loss) for idx, (func_code, loss) in enumerate(zip(unvul_functions, unvul_losses)) if loss > loss_threshold\n","    ]\n","\n","    print(f\"Identified {len(identified_vul_functions)} out of {len(vul_functions)} vulnerable functions as vulnerable.\")\n","    print(f\"Misidentified {len(identified_unvul_functions)} out of {len(unvul_functions)} clean functions as vulnerable.\")\n","\n","    # # Optionally, print the identified vulnerable functions\n","    # for func_num, func_code, loss in identified_vul_functions:\n","    #     print(f\"Function Number: {func_num}\")\n","    #     print(f\"Reconstruction Loss: {loss:.4f}\")\n","    #     print(\"Function Code:\")\n","    #     print(func_code)\n","    #     print(\"Vulnerability Type: Unknown vulnerability type\")\n","    #     print(\"=\" * 50)\n","\n","if __name__ == \"__main__\":\n","    main()\n","\n","# Save the trained model\n","output_dir = \"./function_reconstruction_model\"\n","model.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)\n","print(f\"Model saved to {output_dir}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"WxssHmwdeZqA","executionInfo":{"status":"ok","timestamp":1732559681591,"user_tz":-480,"elapsed":935260,"user":{"displayName":"李唱","userId":"14428720618938666771"}},"outputId":"e8ca1b49-2556-483c-ef91-86ce0dd27b0b"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 10000 clean smart contract functions.\n","Loaded 10000 vulnerable smart contract functions.\n","Example clean function:\n"," function repayBorrowBehalf(address borrower) external payable { (uint err,) = repayBorrowBehalfInternal(borrower, msg.value); \\n requireNoError(err, \"repayBorrowBehalf failed\"); \\n } \\n\n","Example vulnerable function:\n"," function isContract(\\n address _addr\\n )\\n internal\\n view\\n returns (bool)\\n {\\n uint256 size;\\n assembly { size := extcodesize(_addr) }\\n return size > 0;\\n }\\n\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","<ipython-input-51-59ac7c8d0681>:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels_tensor = torch.tensor(label_batch, dtype=torch.float, device=device)\n","<ipython-input-51-59ac7c8d0681>:119: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels_tensor = torch.tensor(label_batch, dtype=torch.float, device=device)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3, Training Loss: -3.0081, Validation Loss: -2.7253\n","Epoch 2/3, Training Loss: -3.9321, Validation Loss: -2.8482\n","Epoch 3/3, Training Loss: -3.4884, Validation Loss: -2.9665\n","Average reconstruction loss for vulnerable functions: 61.7260\n","Average reconstruction loss for clean functions: 2.4859\n","the loss threshold is:32.1060\n","Identified 9999 out of 10000 vulnerable functions as vulnerable.\n","Misidentified 5 out of 10000 clean functions as vulnerable.\n","Model saved to ./function_reconstruction_model\n"]}]},{"cell_type":"code","source":["import os\n","import glob\n","import re\n","import torch\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","\n","def split_contract_into_functions(contract_code):\n","    \"\"\"\n","    Splits a Solidity smart contract into individual functions.\n","\n","    Args:\n","        contract_code (str): The Solidity contract code as a string.\n","\n","    Returns:\n","        List[Tuple[int, str]]: A list of tuples containing function numbers and their code.\n","    \"\"\"\n","    functions = []\n","    current_function = ''\n","    open_braces_count = 0\n","    func_num = 1\n","\n","    for line in contract_code.splitlines():\n","        line_stripped = line.strip()\n","\n","        if line_stripped.startswith('function '):\n","            current_function = line\n","            open_braces_count = line.count('{') - line.count('}')\n","        elif current_function:\n","            current_function += '\\n' + line\n","            open_braces_count += line.count('{') - line.count('}')\n","\n","        if current_function and open_braces_count == 0:\n","            functions.append((func_num, current_function.strip()))\n","            current_function = ''\n","            func_num += 1\n","\n","    return functions\n","\n","\n","def compute_reconstruction_loss(model, tokenizer, function_code, device='cpu'):\n","    model.eval()\n","    with torch.no_grad():\n","        # Tokenize input\n","        inputs = tokenizer(function_code, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","        labels = inputs.input_ids.clone()\n","        labels[labels == tokenizer.pad_token_id] = -100  # Ignore padding tokens in loss\n","\n","        # Forward pass\n","        outputs = model(input_ids=inputs.input_ids, attention_mask=inputs.attention_mask, labels=labels)\n","        loss = outputs.loss.item()\n","    return loss\n","\n","\n","def identify_top_vulnerable_function(functions, model, tokenizer, device='cpu'):\n","    top_vulnerable_function = None\n","    max_loss = -float('inf')\n","\n","    for func_num, func_code in functions:\n","        loss = compute_reconstruction_loss(model, tokenizer, func_code, device)\n","        if loss > max_loss:\n","            max_loss = loss\n","            top_vulnerable_function = (func_num, func_code, loss)\n","\n","    return top_vulnerable_function\n","\n","\n","def print_top_vulnerable_function(top_vulnerable_function):\n","    if top_vulnerable_function:\n","        func_num, func_code, loss = top_vulnerable_function\n","        print(f\"Function Number: {func_num}\")\n","        print(f\"Reconstruction Loss: {loss:.4f}\")\n","        print(\"Function Code:\")\n","        print(func_code)\n","        print(\"Vulnerability Type: Unknown vulnerability type\")\n","        print(\"=\" * 50)\n","    else:\n","        print(\"No vulnerable functions detected.\")\n","\n","\n","# Load the trained model and tokenizer\n","output_dir = \"./function_reconstruction_model\"\n","tokenizer = AutoTokenizer.from_pretrained(output_dir)\n","model = AutoModelForSeq2SeqLM.from_pretrained(output_dir)\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","model.to(device)\n","\n","# Path to the directory containing Solidity contracts\n","contracts_dir = 'function_reconstruction_model/reentrant_contracts/'\n","\n","# Get a list of all Solidity files in the directory\n","contract_files = glob.glob(os.path.join(contracts_dir, '*.sol'))\n","for contract_file in contract_files:\n","    print(f\"\\nProcessing contract: {contract_file}\")\n","    try:\n","        with open(contract_file, 'r', encoding='utf-8') as f:\n","            contract_code = f.read()\n","    except UnicodeDecodeError:\n","        # Retry with a different encoding if UTF-8 fails\n","        with open(contract_file, 'r', encoding='latin-1') as f:\n","            contract_code = f.read()\n","    except Exception as e:\n","        print(f\"Error reading {contract_file}: {e}\")\n","        continue  # Skip this file and proceed to the next one\n","\n","    # Split the contract into functions\n","    functions = split_contract_into_functions(contract_code)\n","    print(f\"Total functions found: {len(functions)}\")\n","\n","    # Identify the top-1 vulnerable function\n","    top_vulnerable_function = identify_top_vulnerable_function(functions, model, tokenizer, device)\n","\n","    # Print the top-1 vulnerable function with the highest reconstruction loss\n","    print(\"Top-1 vulnerable function:\")\n","    print_top_vulnerable_function(top_vulnerable_function)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fpktPhtfFur7","executionInfo":{"status":"ok","timestamp":1732561834129,"user_tz":-480,"elapsed":6691,"user":{"displayName":"李唱","userId":"14428720618938666771"}},"outputId":"b31e0bb2-6063-4c37-b384-74ad31f252fa"},"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Processing contract: function_reconstruction_model/reentrant_contracts/0x4c67b3db1d4474c0ebb2db8bec4e345526d9e2fd.sol\n","Total functions found: 1\n","Top-1 vulnerable function:\n","Function Number: 1\n","Reconstruction Loss: 3.6723\n","Function Code:\n","function sendCall(\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","\n","Processing contract: function_reconstruction_model/reentrant_contracts/0x33813c2f2aab62ac88c234858a1f08448424828f.sol\n","Total functions found: 36\n","Top-1 vulnerable function:\n","Function Number: 31\n","Reconstruction Loss: 7.9236\n","Function Code:\n","function _open() internal {\n","        newRound();\n","    }\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","\n","Processing contract: function_reconstruction_model/reentrant_contracts/0x0459ebad0ba09901fda1441ee72e6cb664257f61.sol\n","Total functions found: 4\n","Top-1 vulnerable function:\n","Function Number: 3\n","Reconstruction Loss: 7.0905\n","Function Code:\n","function ALFA_bank(address log) public{\n","        LogFile = Log(log);\n","    }\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","\n","Processing contract: function_reconstruction_model/reentrant_contracts/0x0da76de0916ef2da3c58a97e4d09d501c56a9f15.sol\n","Total functions found: 4\n","Top-1 vulnerable function:\n","Function Number: 3\n","Reconstruction Loss: 6.9577\n","Function Code:\n","function Piggy_Bank(address log) public{\n","        LogFile = Log(log);\n","    }\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","\n","Processing contract: function_reconstruction_model/reentrant_contracts/0x0eb68f34efa0086e4136bca51fc4d0696580643e.sol\n","Total functions found: 3\n","Top-1 vulnerable function:\n","Function Number: 1\n","Reconstruction Loss: 4.5906\n","Function Code:\n","function put() public payable\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","\n","Processing contract: function_reconstruction_model/reentrant_contracts/0x23a91059fdc9579a9fbd0edc5f2ea0bfdb70deb4.sol\n","Total functions found: 4\n","Top-1 vulnerable function:\n","Function Number: 1\n","Reconstruction Loss: 5.5578\n","Function Code:\n","function PrivateBank(address _log)\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","\n","Processing contract: function_reconstruction_model/reentrant_contracts/0x2a98d8fc14b31b346ff6c56dc2a252c434f628f2.sol\n","Total functions found: 2\n","Top-1 vulnerable function:\n","Function Number: 1\n","Reconstruction Loss: 5.6774\n","Function Code:\n","function store() public payable {\n","        balances[msg.sender] = msg.value;\n","    }\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","\n","Processing contract: function_reconstruction_model/reentrant_contracts/0x039963c07e62eb7af39eeeb871cb2de9cbc65d78.sol\n","Total functions found: 4\n","Top-1 vulnerable function:\n","Function Number: 3\n","Reconstruction Loss: 6.4962\n","Function Code:\n","function PIG_BANK(address log) public{\n","            LogFile = Log(log);\n","        }\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","\n","Processing contract: function_reconstruction_model/reentrant_contracts/0x05f49e3e0a27efa05d60c19cd8f0ecc951d3717e.sol\n","Total functions found: 10\n","Top-1 vulnerable function:\n","Function Number: 9\n","Reconstruction Loss: 7.6144\n","Function Code:\n","function receivePercent() isIssetUser timePayment internal {\n","\n","       // verification that funds on the balance sheet are more than 15% of the total number of deposits\n","\n","        uint balanceLimit = counterDeposits.mul(responseStubFundLimit).div(1000);\n","\n","        uint payoutRatio = calculationOfPayment();\n","\n","        //calculate 6% of total deposits\n","\n","        uint remain = counterDeposits.mul(6).div(100);\n","\n","        \n","\n","        if(addressStub.balance > 0){\n","\n","            if (address(this).balance < balanceLimit) {\n","\n","                stubF.ReturnEthToEtherhero();\n","\n","            }\n","\n","        }\n","\n","        //If the balance is less than 6% of total deposits, stop paying\n","\n","        require(address(this).balance >= remain, 'contract balance is too small');\n","\n","\n","\n","        uint rate = userDeposit[msg.sender].mul(standartPercent).div(1000).mul(payoutRatio);\n","\n","        userTime[msg.sender] = now;\n","\n","        msg.sender.transfer(rate);\n","\n","        counterPercents += rate;\n","\n","        timeLastayment = now;\n","\n","        insertBeneficiaries(msg.sender, standartPercent, rate, 0);\n","\n","        emit dividendPayment(msg.sender, rate);\n","\n","    }\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","\n","Processing contract: function_reconstruction_model/reentrant_contracts/0x3023868433f6086cd8ce0c4083fe2e11b37ce0b7.sol\n","Total functions found: 3\n","Top-1 vulnerable function:\n","Function Number: 2\n","Reconstruction Loss: 8.2896\n","Function Code:\n","function guess(string calldata attempt) external {\n","        if(guesses_allowed <= 0) {\n","            return;\n","        }\n","        \n","        if(owner != tx.origin) {\n","            return;\n","        }\n","        \n","        if(keccak256(abi.encode(\"saltysaltsalt\", attempt)) == answer) {\n","            // send ether for winning\n","            msg.sender.call.value(.2 ether)(\"\");\n","        }\n","        \n","        guesses_allowed--;\n","    }\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","\n","Processing contract: function_reconstruction_model/reentrant_contracts/0xb7c5c5aa4d42967efe906e1b66cb8df9cebf04f7.sol\n","Total functions found: 2\n","Top-1 vulnerable function:\n","Function Number: 1\n","Reconstruction Loss: 5.8500\n","Function Code:\n","function () payable public {\n","        balances[msg.sender] += msg.value;\n","    }\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","\n","Processing contract: function_reconstruction_model/reentrant_contracts/0xa4c63e8b05df7db85db0ad37f8064f92338a2422.sol\n","Total functions found: 4\n","Top-1 vulnerable function:\n","Function Number: 3\n","Reconstruction Loss: 6.8307\n","Function Code:\n","function DU_Bank(address log) public{\n","            LogFile = Log(log);\n","        }\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","\n","Processing contract: function_reconstruction_model/reentrant_contracts/0xe0b0831d52d970298fe4a9dd4fe3f22bcd130fae.sol\n","Total functions found: 4\n","Top-1 vulnerable function:\n","Function Number: 3\n","Reconstruction Loss: 7.1420\n","Function Code:\n","function PG_bank(address log) public{\n","        LogFile = Log(log);\n","    }\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","\n","Processing contract: function_reconstruction_model/reentrant_contracts/0xc213dc998b6c0d9c51f9cc72240596e1dd684ec7.sol\n","Total functions found: 2\n","Top-1 vulnerable function:\n","Function Number: 1\n","Reconstruction Loss: 5.6091\n","Function Code:\n","function depositFunds() public payable {\n","        balances[msg.sender] += msg.value;\n","    }\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","\n","Processing contract: function_reconstruction_model/reentrant_contracts/0xcef76ac06ee4dfaca24feaf000cd44cf43f30ce0.sol\n","Total functions found: 4\n","Top-1 vulnerable function:\n","Function Number: 3\n","Reconstruction Loss: 7.2385\n","Function Code:\n","function Alfa_Bank(address log) public{\n","        LogFile = Log(log);\n","    }\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","\n","Processing contract: function_reconstruction_model/reentrant_contracts/0xd116d1349c1382b0b302086a4e4219ae4f8634ff.sol\n","Total functions found: 4\n","Top-1 vulnerable function:\n","Function Number: 1\n","Reconstruction Loss: 5.5938\n","Function Code:\n","function Private_Bank(address _log)\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","\n","Processing contract: function_reconstruction_model/reentrant_contracts/0xb4c05e6e4cdb07c15095300d96a5735046eef999.sol\n","Total functions found: 4\n","Top-1 vulnerable function:\n","Function Number: 1\n","Reconstruction Loss: 5.5578\n","Function Code:\n","function PrivateBank(address _log)\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","\n","Processing contract: function_reconstruction_model/reentrant_contracts/0xdd17afae8a3dd1936d1113998900447ab9aa9bc0.sol\n","Total functions found: 6\n","Top-1 vulnerable function:\n","Function Number: 3\n","Reconstruction Loss: 5.3027\n","Function Code:\n","function Initialized()\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","\n","Processing contract: function_reconstruction_model/reentrant_contracts/0x7c4393ee129d7856b5bd765c2d20b66f464ccd0f.sol\n","Total functions found: 25\n","Top-1 vulnerable function:\n","Function Number: 10\n","Reconstruction Loss: 8.4604\n","Function Code:\n","function () external payable {\n","    require(validPurchase());\n","    forwardFunds();  \n","  }\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","\n","Processing contract: function_reconstruction_model/reentrant_contracts/0x96edbe868531bd23a6c05e9d0c424ea64fb1b78b.sol\n","Total functions found: 6\n","Top-1 vulnerable function:\n","Function Number: 3\n","Reconstruction Loss: 5.3027\n","Function Code:\n","function Initialized()\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","\n","Processing contract: function_reconstruction_model/reentrant_contracts/0x8c7777c45481dba411450c228cb692ac3d550344.sol\n","Total functions found: 4\n","Top-1 vulnerable function:\n","Function Number: 3\n","Reconstruction Loss: 4.8511\n","Function Code:\n","function CashOut(uint _am)\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","\n","Processing contract: function_reconstruction_model/reentrant_contracts/0x7a8721a9d64c74da899424c1b52acbf58ddc9782.sol\n","Total functions found: 5\n","Top-1 vulnerable function:\n","Function Number: 1\n","Reconstruction Loss: 5.1332\n","Function Code:\n","function PrivateDeposit()\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","\n","Processing contract: function_reconstruction_model/reentrant_contracts/0x561eac93c92360949ab1f1403323e6db345cbf31.sol\n","Total functions found: 6\n","Top-1 vulnerable function:\n","Function Number: 3\n","Reconstruction Loss: 5.3027\n","Function Code:\n","function Initialized()\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","\n","Processing contract: function_reconstruction_model/reentrant_contracts/0x85179ac15aa94e3ca32dd1cc04664e9bb0062115.sol\n","Total functions found: 6\n","Top-1 vulnerable function:\n","Function Number: 3\n","Reconstruction Loss: 5.3027\n","Function Code:\n","function Initialized()\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","\n","Processing contract: function_reconstruction_model/reentrant_contracts/0x941d225236464a25eb18076df7da6a91d0f95e9e.sol\n","Total functions found: 4\n","Top-1 vulnerable function:\n","Function Number: 3\n","Reconstruction Loss: 4.8511\n","Function Code:\n","function CashOut(uint _am)\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","\n","Processing contract: function_reconstruction_model/reentrant_contracts/0x79ad73567e7f9a5b4a2d28264f78aff9d971cf14.sol\n","Total functions found: 11\n","Top-1 vulnerable function:\n","Function Number: 2\n","Reconstruction Loss: 6.9990\n","Function Code:\n","function send(address to, uint256 amt, uint256 tips, address ref) private returns (bool){\n","        (bool sentTo,) = address(to).call{value : amt-tips}(\"\");\n","        if(tips>0) {\n","            if(ref != 0x0000000000000000000000000000000000000000) {\n","                uint256 refEarn = amt/2*_refRate/10000;//sharing tips with referral\n","                (bool sentRef,) = address(ref).call{value : refEarn}(\"\");\n","                if(sentRef) tips -= refEarn;\n","            }\n","            \n","            (bool sentFee,) = address(_chef).call{value : tips}(\"\");\n","            _chefTips += tips;\n","            return sentTo&&sentFee;\n","        }\n","        return sentTo;\n","    }\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","\n","Processing contract: function_reconstruction_model/reentrant_contracts/0x93c32845fae42c83a70e5f06214c8433665c2ab5.sol\n","Total functions found: 4\n","Top-1 vulnerable function:\n","Function Number: 3\n","Reconstruction Loss: 7.1540\n","Function Code:\n","function X_WALLET(address log) public{\n","\n","        LogFile = Log(log);\n","\n","    }\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","\n","Processing contract: function_reconstruction_model/reentrant_contracts/0x95d34980095380851902ccd9a1fb4c813c2cb639.sol\n","Total functions found: 4\n","Top-1 vulnerable function:\n","Function Number: 1\n","Reconstruction Loss: 5.5938\n","Function Code:\n","function Private_Bank(address _log)\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","\n","Processing contract: function_reconstruction_model/reentrant_contracts/0x9902211b5d4bd822594e6f052637bf36515ac9a7.sol\n","Total functions found: 11\n","Top-1 vulnerable function:\n","Function Number: 2\n","Reconstruction Loss: 6.9990\n","Function Code:\n","function send(address to, uint256 amt, uint256 tips, address ref) private returns (bool){\n","        (bool sentTo,) = address(to).call{value : amt-tips}(\"\");\n","        if(tips>0) {\n","            if(ref != 0x0000000000000000000000000000000000000000) {\n","                uint256 refEarn = amt/2*_refRate/10000;//sharing tips with referral\n","                (bool sentRef,) = address(ref).call{value : refEarn}(\"\");\n","                if(sentRef) tips -= refEarn;\n","            }\n","            \n","            (bool sentFee,) = address(_chef).call{value : tips}(\"\");\n","            _chefTips += tips;\n","            return sentTo&&sentFee;\n","        }\n","        return sentTo;\n","    }\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","\n","Processing contract: function_reconstruction_model/reentrant_contracts/0x9a156f934c3542ef6a4443ce9a91d2d768fc01c1.sol\n","Total functions found: 78\n","Top-1 vulnerable function:\n","Function Number: 69\n","Reconstruction Loss: 8.0499\n","Function Code:\n","function executeProposal(uint _proposalID, bytes _transactionByteCode) public {\n","\n","        // Get the proposal\n","\n","        _Proposal storage p = Proposals[_proposalID];\n","\n","\n","\n","        require(now > p.endTimeOfVoting                                                                       // If it is past the voting deadline\n","\n","            && !p.executed                                                                                    // and it has not already been executed\n","\n","            && p.transactionHash == keccak256(abi.encodePacked(p.recipient, p.amount, _transactionByteCode))  // and the supplied code matches the proposal\n","\n","            && p.numberOfVotes >= minimumQuorum);                                                             // and a minimum quorum has been reached\n","\n","        // then execute result\n","\n","        if (p.votesSupport > requisiteMajority) {\n","\n","            // Proposal passed; execute the transaction\n","\n","            require(p.recipient.call.value(p.amount)(_transactionByteCode));\n","\n","            p.proposalPassed = true;\n","\n","        } else {\n","\n","            // Proposal failed\n","\n","            p.proposalPassed = false;\n","\n","        }\n","\n","        p.executed = true;\n","\n","\n","\n","        // delete proposal from active list\n","\n","        super.delProposal(_proposalID);\n","\n","       \n","\n","        // Fire Events\n","\n","        emit ProposalTallied(_proposalID, p.votesSupport, p.votesAgainst, p.numberOfVotes, p.proposalPassed);\n","\n","    }\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","\n","Processing contract: function_reconstruction_model/reentrant_contracts/0xf6dbe88ba55f1793ff0773c9b1275300f830914f.sol\n","Total functions found: 21\n","Top-1 vulnerable function:\n","Function Number: 1\n","Reconstruction Loss: 7.3628\n","Function Code:\n","function bid() payable external {\n","\n","        require(msg.value >= highestBid);\n","\n","\n","\n","        if (highestBidder != address(0)) {\n","\n","            refunds[highestBidder] += highestBid; // record the refund that this user can claim\n","\n","        }\n","\n","\n","\n","        highestBidder = msg.sender;\n","\n","        highestBid = msg.value;\n","\n","    }\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","\n","Processing contract: function_reconstruction_model/reentrant_contracts/0xf6a1fa796cfbdcc4adb08dbf634438fb35561d8d.sol\n","Total functions found: 3\n","Top-1 vulnerable function:\n","Function Number: 3\n","Reconstruction Loss: 5.7103\n","Function Code:\n","function sell(IERC20 token, uint256 minAmount) public payable {\n","\n","        require(token.call.value(msg.value)(), \"sell failed\");\n","\n","\n","\n","        uint256 balance = token.balanceOf(this);\n","\n","        require(balance >= minAmount, \"Price too bad\");\n","\n","        token.transfer(msg.sender, balance);\n","\n","    }\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","\n","Processing contract: function_reconstruction_model/reentrant_contracts/0xe610af01f92f19679327715b426c35849c47c657.sol\n","Total functions found: 4\n","Top-1 vulnerable function:\n","Function Number: 3\n","Reconstruction Loss: 5.2473\n","Function Code:\n","function Collect(uint _am)\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n","\n","Processing contract: function_reconstruction_model/reentrant_contracts/0xf281bad74a5a549ceced85dad57f47a671b07e51.sol\n","Total functions found: 4\n","Top-1 vulnerable function:\n","Function Number: 3\n","Reconstruction Loss: 7.1077\n","Function Code:\n","function du_Bank(address log) public{\n","            LogFile = Log(log);\n","        }\n","Vulnerability Type: Unknown vulnerability type\n","==================================================\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"LVpRpDj1sc_q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd .."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CaHzHLJMnvVl","executionInfo":{"status":"ok","timestamp":1732561205255,"user_tz":-480,"elapsed":688,"user":{"displayName":"李唱","userId":"14428720618938666771"}},"outputId":"7f703c79-129a-4952-a86a-92a846e5f480"},"execution_count":62,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive\n"]}]},{"cell_type":"code","source":["import random\n","import pickle\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AdamW\n","from sklearn.model_selection import train_test_split\n","\n","class SolidityDataset(Dataset):\n","    def __init__(self, codes, noise_level=0.1):\n","        self.codes = codes\n","        self.noise_level = noise_level\n","\n","    def __len__(self):\n","        return len(self.codes)\n","\n","    def __getitem__(self, idx):\n","        clean_code = self.codes[idx]\n","        noisy_code = add_function_noise(clean_code, self.noise_level)\n","        return clean_code, noisy_code\n","\n","def load_data_from_pickle(file_path):\n","    with open(file_path, 'rb') as f:\n","        codes = pickle.load(f)\n","    return codes\n","\n","file_path = 'unvul.pickle'\n","clean_contracts = load_data_from_pickle(file_path)\n","\n","print(f\"Loaded {len(clean_contracts)} clean smart contract functions.\")\n","print(\"Example function:\\n\", clean_contracts[0])\n","train_contracts, val_contracts = train_test_split(clean_contracts, test_size=0.1, random_state=42)\n","\n","train_dataset = SolidityDataset(train_contracts, noise_level=0.2)\n","val_dataset = SolidityDataset(val_contracts, noise_level=0.2)\n","\n","train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5-small\")\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"Salesforce/codet5-small\")\n","optimizer = AdamW(model.parameters(), lr=5e-5)\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# Define a set of noise codes\n","noise_codes = [\n","    \"function placeholder() public {}\",\n","    \"function unknown() external view {}\",\n","    \"function dummy() internal {}\",\n","    \"function empty() private {}\"\n","]\n","\n","# Tokenize noise codes\n","noise_code_tokenized_list = [tokenizer(noise_code, return_tensors=\"pt\", padding=True, truncation=True).to(device) for noise_code in noise_codes]\n","\n","def custom_loss(outputs_clean, labels_clean, outputs_noisy, labels_noisy, lambda_coeff=1.0):\n","    \"\"\"\n","    Compute custom loss for clean and noisy inputs.\n","\n","    Args:\n","        outputs_clean: Model outputs for clean inputs.\n","        labels_clean: Tokenized clean code inputs.\n","        outputs_noisy: Model outputs for noisy inputs.\n","        labels_noisy: Tokenized noise code inputs.\n","        lambda_coeff: Coefficient to balance the loss terms.\n","\n","    Returns:\n","        total_loss: Combined loss for optimization.\n","    \"\"\"\n","    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n","\n","    # Term 1: Reconstruction loss for clean code\n","    clean_loss = loss_fct(outputs_clean.logits.view(-1, outputs_clean.logits.size(-1)), labels_clean.view(-1))\n","\n","    # Term 2: Loss between model's output for noisy input and the noise code\n","    noisy_loss = loss_fct(outputs_noisy.logits.view(-1, outputs_noisy.logits.size(-1)), labels_noisy.view(-1))\n","\n","    # Total Loss\n","    total_loss = clean_loss + lambda_coeff * noisy_loss\n","    return total_loss\n","\n","def train_model(model, train_loader, val_loader, optimizer, tokenizer, noise_code_tokenized_list, num_epochs=3, lambda_coeff=1.0, device='cpu'):\n","    model.to(device)\n","    for epoch in range(num_epochs):\n","        model.train()\n","        total_train_loss = 0\n","        for clean_code_batch, noisy_code_batch in train_loader:\n","            optimizer.zero_grad()\n","\n","            # Tokenize clean inputs and labels\n","            inputs_clean = tokenizer(clean_code_batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","            labels_clean = inputs_clean.input_ids.clone()\n","            labels_clean[labels_clean == tokenizer.pad_token_id] = -100  # Ignore padding tokens\n","\n","            # Tokenize noisy inputs\n","            inputs_noisy = tokenizer(noisy_code_batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","\n","            # Create labels_noisy by randomly selecting a noise code for each sample\n","            batch_size = inputs_noisy.input_ids.size(0)\n","            noise_labels = []\n","            for _ in range(batch_size):\n","                noise_code_tokenized = random.choice(noise_code_tokenized_list)\n","                noise_labels.append(noise_code_tokenized.input_ids.squeeze(0))\n","            # Pad noise labels to the same length\n","            noise_labels_padded = torch.nn.utils.rnn.pad_sequence(noise_labels, batch_first=True, padding_value=tokenizer.pad_token_id).to(device)\n","            labels_noisy = noise_labels_padded.clone()\n","            labels_noisy[labels_noisy == tokenizer.pad_token_id] = -100  # Ignore padding tokens\n","\n","            # Forward pass for clean inputs\n","            outputs_clean = model(input_ids=inputs_clean.input_ids, attention_mask=inputs_clean.attention_mask, labels=labels_clean)\n","\n","            # Forward pass for noisy inputs\n","            outputs_noisy = model(input_ids=inputs_noisy.input_ids, attention_mask=inputs_noisy.attention_mask, labels=labels_noisy)\n","\n","            # Compute custom loss\n","            loss = custom_loss(outputs_clean, labels_clean, outputs_noisy, labels_noisy, lambda_coeff)\n","\n","            # Backward pass and optimization\n","            loss.backward()\n","            optimizer.step()\n","            total_train_loss += loss.item()\n","\n","        avg_train_loss = total_train_loss / len(train_loader)\n","\n","        # Validation phase\n","        model.eval()\n","        total_val_loss = 0\n","        with torch.no_grad():\n","            for clean_code_batch, noisy_code_batch in val_loader:\n","                # Tokenize clean inputs and labels\n","                inputs_clean = tokenizer(clean_code_batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","                labels_clean = inputs_clean.input_ids.clone()\n","                labels_clean[labels_clean == tokenizer.pad_token_id] = -100\n","\n","                # Tokenize noisy inputs\n","                inputs_noisy = tokenizer(noisy_code_batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","\n","                # Create labels_noisy by randomly selecting a noise code for each sample\n","                batch_size = inputs_noisy.input_ids.size(0)\n","                noise_labels = []\n","                for _ in range(batch_size):\n","                    noise_code_tokenized = random.choice(noise_code_tokenized_list)\n","                    noise_labels.append(noise_code_tokenized.input_ids.squeeze(0))\n","                # Pad noise labels to the same length\n","                noise_labels_padded = torch.nn.utils.rnn.pad_sequence(noise_labels, batch_first=True, padding_value=tokenizer.pad_token_id).to(device)\n","                labels_noisy = noise_labels_padded.clone()\n","                labels_noisy[labels_noisy == tokenizer.pad_token_id] = -100  # Ignore padding tokens\n","\n","                # Forward pass for clean inputs\n","                outputs_clean = model(input_ids=inputs_clean.input_ids, attention_mask=inputs_clean.attention_mask, labels=labels_clean)\n","\n","                # Forward pass for noisy inputs\n","                outputs_noisy = model(input_ids=inputs_noisy.input_ids, attention_mask=inputs_noisy.attention_mask, labels=labels_noisy)\n","\n","                # Compute custom loss\n","                loss = custom_loss(outputs_clean, labels_clean, outputs_noisy, labels_noisy, lambda_coeff)\n","                total_val_loss += loss.item()\n","\n","        avg_val_loss = total_val_loss / len(val_loader)\n","\n","        print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n","\n","train_model(model, train_loader, val_loader, optimizer, tokenizer, noise_code_tokenized_list, num_epochs=3, lambda_coeff=1.0, device=device)\n","\n","def evaluate(model, tokenizer, code_list, device='cpu'):\n","    model.eval()\n","    inputs = tokenizer(code_list, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","    with torch.no_grad():\n","        outputs = model.generate(inputs['input_ids'], max_length=512)\n","    reconstructed_codes = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n","    return reconstructed_codes\n","\n","# Evaluate a batch of clean and noisy samples\n","clean_samples = val_contracts[:5]\n","noisy_samples = [add_function_noise(code, noise_level=0.2) for code in clean_samples]\n","reconstructed_clean = evaluate(model, tokenizer, clean_samples, device=device)\n","reconstructed_noisy = evaluate(model, tokenizer, noisy_samples, device=device)\n","\n","for idx in range(5):\n","    print(f\"Clean Input {idx+1}:\\n{clean_samples[idx]}\\n\")\n","    print(f\"Reconstructed Clean Output {idx+1}:\\n{reconstructed_clean[idx]}\\n\")\n","    print(f\"Noisy Input {idx+1}:\\n{noisy_samples[idx]}\\n\")\n","    print(f\"Reconstructed Noisy Output {idx+1}:\\n{reconstructed_noisy[idx]}\\n\")\n","    print(\"=\" * 50)\n","\n","output_dir = \"./function_reconstruction_model\"\n","model.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)\n","print(f\"Model saved to {output_dir}\")\n"],"metadata":{"id":"Bom_ZMZN-mJb"},"execution_count":null,"outputs":[]}]}