{"cells":[{"cell_type":"markdown","metadata":{},"source":["该notebook用来微调decoder-only的生成式大模型。通过修改loss函数将next token prediction损失调整成交叉熵损失将生成式任务任务转化成二进制分类任务\n","分别在均衡和不均衡上的数据集上都进行了测试"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["#!/usr/bin/env python\n","import os\n","import pandas as pd\n","import torch\n","import deepspeed\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from transformers import AutoTokenizer\n","from torch.utils.data import DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n","from sklearn.metrics import f1_score\n","\n","def tokenize_and_split(vul_csv_path=\"VulSC.csv\",\n","                         nonvul_csv_path=\"nonVulSC.csv\",\n","                         max_seq_length=512,\n","                         train_size=0.7, val_size=0.15, test_size=0.15):\n","    \"\"\"\n","    Loads the CSV files, parses texts and labels, tokenizes, and splits the data\n","    into train, validation, and test sets using stratified sampling.\n","    Returns: (train_encodings, train_labels, val_encodings, val_labels, test_encodings, test_labels, tokenizer)\n","    \"\"\"\n","    def load_single_csv(csv_path):\n","        try:\n","            df = pd.read_csv(csv_path, header=0)\n","            print(f\"成功加载 {len(df)} 条数据: {os.path.basename(csv_path)}\")\n","            if \"source_code\" in df.columns:\n","                df = df.rename(columns={\"source_code\": \"text\"})\n","            return df\n","        except Exception as e:\n","            raise ValueError(f\"加载 {csv_path} 失败: {str(e)}\")\n","\n","    df_vul = load_single_csv(vul_csv_path)\n","    df_nonvul = load_single_csv(nonvul_csv_path)\n","\n","    def parse_text_label(text):\n","        if not isinstance(text, str):\n","            raise ValueError(f\"非文本输入: {text}\")\n","        if \"//\" in text:\n","            parts = text.rsplit(\"//\", 1)\n","            code = parts[0].strip()\n","            label = parts[1].strip()\n","        else:\n","            last_space_idx = text.rfind(' ')\n","            if last_space_idx == -1:\n","                raise ValueError(f\"无效格式: {text}\")\n","            code = text[:last_space_idx].strip()\n","            label = text[last_space_idx+1:].strip()\n","        if \"<Vul>\" in label:\n","            label = \"<Vul>\"\n","        elif \"<nonVul>\" in label:\n","            label = \"<nonVul>\"\n","        else:\n","            raise ValueError(f\"未知标签: {label} in '{text}'\")\n","        return code, label\n","\n","    texts = []\n","    labels = []\n","    for df, expected_label in zip([df_nonvul, df_vul], [\"<nonVul>\", \"<Vul>\"]):\n","        for text in df[\"text\"]:\n","            try:\n","                code, label = parse_text_label(text)\n","                if label != expected_label:\n","                    raise ValueError(f\"文件标签不一致: {label} vs {expected_label}\")\n","                texts.append(code)\n","                labels.append(1 if label == \"<Vul>\" else 0)\n","            except Exception as e:\n","                print(f\"[!] 数据错误: {str(e)}\")\n","                continue\n","\n","    # Initialize tokenizer and add special tokens.\n","    tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/ISSRELLM4SCReplicate/ModelGPTJ4SC/model\")\n","    tokenizer.pad_token = tokenizer.eos_token\n","    special_tokens = [\"<nonVul>\", \"<Vul>\"]\n","    num_added = tokenizer.add_special_tokens({\"additional_special_tokens\": special_tokens})\n","    print(f\"添加特殊token: {special_tokens} (已存在{len(special_tokens)-num_added}个)\")\n","\n","    # Tokenize all texts.\n","    encodings = tokenizer(\n","        texts,\n","        max_length=max_seq_length,\n","        padding=\"max_length\",\n","        truncation=True,\n","        return_tensors=\"pt\",\n","        add_special_tokens=True\n","    )\n","\n","    print(f\"类别分布: nonVul={labels.count(0)}, Vul={labels.count(1)}\")\n","\n","    # Create indices and perform stratified splits.\n","    labels_np = np.array(labels)\n","    indices = np.arange(len(labels_np))\n","    train_idx, temp_idx = train_test_split(indices, test_size=(1 - train_size), stratify=labels_np, random_state=42)\n","    if len(temp_idx) > 0:\n","        val_prop = val_size / (val_size + test_size)\n","        val_idx, test_idx = train_test_split(temp_idx, test_size=(1 - val_prop), stratify=labels_np[temp_idx], random_state=42)\n","    else:\n","        val_idx, test_idx = np.array([]), np.array([])\n","\n","    def slice_encodings(enc, idx):\n","        return {k: v[idx] for k, v in enc.items()}\n","\n","    train_encodings = slice_encodings(encodings, train_idx)\n","    val_encodings = slice_encodings(encodings, val_idx)\n","    test_encodings = slice_encodings(encodings, test_idx)\n","\n","    train_labels = [labels[i] for i in train_idx]\n","    val_labels = [labels[i] for i in val_idx]\n","    test_labels = [labels[i] for i in test_idx]\n","\n","    return train_encodings, train_labels, val_encodings, val_labels, test_encodings, test_labels, tokenizer\n","\n","\n","def evaluate(model_engine, dataset, criterion, batch_size=4):\n","    \"\"\"\n","    在验证集上计算平均损失、准确率、精确率、召回率、F1 和 ROC-AUC。\n","    其中，将 sample 级标签扩展为函数级标签，与模型输出的函数级 logits 对应。\n","    \"\"\"\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n","    model_engine.eval()\n","    total_loss = 0.0\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            input_ids = batch[\"input_ids\"].to(model_engine.local_rank)\n","            labels_batch = batch[\"labels\"].to(model_engine.local_rank)\n","            outputs = model_engine(input_ids)\n","            # 计算损失\n","            loss = criterion(outputs, labels_batch, input_ids, use_contrast_loss=False)\n","            total_loss += loss.item() * labels_batch.size(0)\n","            # 将 sample 级标签扩展为函数级标签\n","            expanded_labels = []\n","            for i in range(labels_batch.size(0)):\n","                n_funcs = outputs[\"num_funcs\"][i]\n","                # 注意：labels_batch[i] 是标量张量，使用 repeat(n_funcs) 得到一个 [n_funcs] 的张量\n","                expanded_labels.append(labels_batch[i].repeat(n_funcs))\n","            expanded_labels = torch.cat(expanded_labels, dim=0)\n","\n","            # 预测结果\n","            preds = torch.argmax(outputs[\"func_logits\"], dim=1)\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(expanded_labels.cpu().numpy())\n","\n","    avg_loss = total_loss / len(dataset)\n","\n","    # 利用 sklearn 计算各指标\n","    accuracy = accuracy_score(all_labels, all_preds)\n","    precision = precision_score(all_labels, all_preds, zero_division=0)\n","    recall = recall_score(all_labels, all_preds, zero_division=0)\n","    f1 = f1_score(all_labels, all_preds, zero_division=0)\n","    try:\n","        roc_auc = roc_auc_score(all_labels, all_preds)\n","    except Exception:\n","        roc_auc = 0.0\n","\n","    return avg_loss, accuracy, precision, recall, f1, roc_auc"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["vul_csv_path=\"VulSC.csv\"\n","nonvul_csv_path=\"nonVulSC.csv\"\n","def load_single_csv(csv_path):\n","        try:\n","            df = pd.read_csv(csv_path, header=0)\n","            print(f\"成功加载 {len(df)} 条数据: {os.path.basename(csv_path)}\")\n","            if \"source_code\" in df.columns:\n","                df = df.rename(columns={\"source_code\": \"text\"})\n","            return df\n","        except Exception as e:\n","            raise ValueError(f\"加载 {csv_path} 失败: {str(e)}\")\n","\n","df_vul = load_single_csv(vul_csv_path)\n","df_nonvul = load_single_csv(nonvul_csv_path)\n","\n","def parse_text_label(text):\n","    if not isinstance(text, str):\n","        raise ValueError(f\"非文本输入: {text}\")\n","    if \"//\" in text:\n","        parts = text.rsplit(\"//\", 1)\n","        code = parts[0].strip()\n","        label = parts[1].strip()\n","    else:\n","        last_space_idx = text.rfind(' ')\n","        if last_space_idx == -1:\n","            raise ValueError(f\"无效格式: {text}\")\n","        code = text[:last_space_idx].strip()\n","        label = text[last_space_idx+1:].strip()\n","    if \"<Vul>\" in label:\n","        label = \"<Vul>\"\n","    elif \"<nonVul>\" in label:\n","        label = \"<nonVul>\"\n","    else:\n","        raise ValueError(f\"未知标签: {label} in '{text}'\")\n","    return code, label\n","\n","texts = []\n","labels = []\n","for df, expected_label in zip([df_nonvul, df_vul], [\"<nonVul>\", \"<Vul>\"]):\n","    for text in df[\"text\"]:\n","        try:\n","            code, label = parse_text_label(text)\n","            if label != expected_label:\n","                raise ValueError(f\"文件标签不一致: {label} vs {expected_label}\")\n","            texts.append(code)\n","            labels.append(1 if label == \"<Vul>\" else 0)\n","        except Exception as e:\n","            print(f\"[!] 数据错误: {str(e)}\")\n","            continue"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["train_encodings, train_labels, val_encodings, val_labels, test_encodings, test_labels, tokenizer = tokenize_and_split(\n","        vul_csv_path=\"VulSC.csv\",\n","        nonvul_csv_path=\"nonVulSC.csv\",\n","        max_seq_length=512,\n","        train_size=0.7, val_size=0.15, test_size=0.15\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["train_dataset = ContractDataset(train_encodings, train_labels)\n","val_dataset = ContractDataset(val_encodings, val_labels)\n","test_dataset = ContractDataset(test_encodings, test_labels)\n","print(train_dataset[:10])"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["def train():\n","    # 请替换下列路径为自己的数据集文件路径\n","    train_encodings, train_labels, val_encodings, val_labels, test_encodings, test_labels, tokenizer = tokenize_and_split(\n","        vul_csv_path=\"VulSC.csv\",\n","        nonvul_csv_path=\"nonVulSC.csv\",\n","        max_seq_length=512,\n","        train_size=0.7, val_size=0.15, test_size=0.15\n","    )\n","\n","    train_dataset = ContractDataset(train_encodings, train_labels)\n","    val_dataset = ContractDataset(val_encodings, val_labels)\n","    test_dataset = ContractDataset(test_encodings, test_labels)\n","\n","    # 使用更新后的 QuintupletSampler\n","    class_counts = [train_labels.count(0), train_labels.count(1)]\n","    train_sampler = QuintupletSampler(train_labels, class_counts, num_anchors=1)\n","    train_loader = DataLoader(train_dataset, batch_sampler=train_sampler)\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    func_sep_token_id = tokenizer.convert_tokens_to_ids(\"<func_sep>\")\n","    model = VulDetector(func_sep_token_id=func_sep_token_id).to(device)\n","    criterion = HybridLoss(margin1=0.2, margin2=0.2, margin3=0.2, lm_weight=0.2)\n","    writer = SummaryWriter()\n","\n","    ds_config = \"configs/ds_config.json\"\n","    model_engine, optimizer, _, _ = deepspeed.initialize(\n","        model=model,\n","        config=ds_config,\n","        model_parameters=model.parameters()\n","    )\n","\n","    num_epochs = 10\n","    global_step = 0\n","    best_val_accuracy = 0.0\n","    best_epoch = -1\n","\n","    # 记录各 epoch 指标与损失项\n","    epochs_arr = []\n","    train_losses = []\n","    val_losses = []\n","    train_accs = []\n","    val_accs = []\n","    train_precs = []\n","    val_precs = []\n","    train_recalls = []\n","    val_recalls = []\n","    train_f1s = []\n","    val_f1s = []\n","    train_roc_aucs = []\n","    val_roc_aucs = []\n","    epoch_ce_losses = []\n","    epoch_contrast_losses = []\n","    epoch_lm_losses = []\n","    epoch_total_losses = []\n","\n","    for epoch in range(num_epochs):\n","        model_engine.train()\n","        epoch_loss = 0.0\n","        epoch_correct = 0\n","        epoch_total = 0\n","        sum_ce, sum_contrast, sum_lm, sum_total = 0.0, 0.0, 0.0, 0.0\n","        num_batches = 0\n","\n","        for batch in train_loader:\n","            input_ids = batch[\"input_ids\"].to(model_engine.local_rank)\n","            labels_batch = batch[\"labels\"].to(model_engine.local_rank)\n","\n","            outputs = model_engine(input_ids)\n","            # 调用 loss 时返回各部分损失\n","            loss, components = criterion(outputs, labels_batch, input_ids, return_components=True)\n","            model_engine.backward(loss)\n","            model_engine.step()\n","\n","            epoch_loss += loss.item()\n","            sum_ce += components[\"ce_loss\"].item()\n","            sum_contrast += components[\"contrast_loss\"] if isinstance(components[\"contrast_loss\"], float) else components[\"contrast_loss\"].item()\n","            sum_lm += components[\"lm_loss\"].item()\n","            sum_total += components[\"total_loss\"].item()\n","\n","            # 扩展标签计算函数级准确率\n","            expanded_labels = []\n","            for i in range(labels_batch.size(0)):\n","                n_funcs = outputs[\"num_funcs\"][i]\n","                expanded_labels.append(labels_batch[i].repeat(n_funcs))\n","            expanded_labels = torch.cat(expanded_labels, dim=0).to(model_engine.local_rank)\n","            preds = torch.argmax(outputs[\"func_logits\"], dim=1)\n","            epoch_correct += (preds == expanded_labels).sum().item()\n","            epoch_total += preds.size(0)\n","\n","            global_step += 1\n","            writer.add_scalar(\"Batch Loss\", loss.item(), global_step)\n","            num_batches += 1\n","\n","        avg_train_loss = epoch_loss / len(train_loader)\n","        train_accuracy = epoch_correct / epoch_total if epoch_total > 0 else 0\n","\n","        avg_ce = sum_ce / num_batches\n","        avg_contrast = sum_contrast / num_batches\n","        avg_lm = sum_lm / num_batches\n","        avg_total = sum_total / num_batches\n","\n","        # 验证阶段\n","        val_loss, val_accuracy, val_precision, val_recall, val_f1, val_roc = evaluate(model_engine, val_dataset, criterion)\n","        print(f\"Epoch {epoch}: Train Loss = {avg_train_loss:.4f}, Train Acc = {train_accuracy:.4f}\")\n","        print(f\"          Val Loss = {val_loss:.4f}, Val Acc = {val_accuracy:.4f}, Val Prec = {val_precision:.4f}, Val Recall = {val_recall:.4f}, Val F1 = {val_f1:.4f}, Val ROC-AUC = {val_roc:.4f}\")\n","\n","        if val_accuracy > best_val_accuracy:\n","            best_val_accuracy = val_accuracy\n","            best_epoch = epoch\n","            best_checkpoint_dir = \"checkpoints/best\"\n","            os.makedirs(best_checkpoint_dir, exist_ok=True)\n","            model_engine.save_checkpoint(best_checkpoint_dir)\n","            print(f\"Best checkpoint saved at epoch {epoch}\")\n","\n","        epochs_arr.append(epoch)\n","        train_losses.append(avg_train_loss)\n","        val_losses.append(val_loss)\n","        train_accs.append(train_accuracy)\n","        val_accs.append(val_accuracy)\n","        train_precs.append(train_accuracy)  # 示例，实际请计算\n","        train_recalls.append(train_accuracy)\n","        train_f1s.append(train_accuracy)\n","        train_roc_aucs.append(train_accuracy)\n","        val_precs.append(val_precision)\n","        val_recalls.append(val_recall)\n","        val_f1s.append(val_f1)\n","        val_roc_aucs.append(val_roc)\n","\n","        epoch_ce_losses.append(avg_ce)\n","        epoch_contrast_losses.append(avg_contrast)\n","        epoch_lm_losses.append(avg_lm)\n","        epoch_total_losses.append(avg_total)\n","\n","        writer.add_scalar(\"Epoch Train Loss\", avg_train_loss, epoch)\n","        writer.add_scalar(\"Epoch Val Loss\", val_loss, epoch)\n","        writer.add_scalar(\"Epoch Val Accuracy\", val_accuracy, epoch)\n","\n","    # 调用可视化模块生成图表\n","    visualization.plot_training_validation_loss(epochs_arr, train_losses, val_losses, best_val_loss=min(val_losses), best_epoch=best_epoch)\n","    visualization.plot_metrics_curves(epochs_arr, train_accs, val_accs,\n","                                      train_precs, val_precs,\n","                                      train_recalls, val_recalls,\n","                                      train_f1s, val_f1s,\n","                                      train_roc_aucs, val_roc_aucs)\n","    visualization.plot_loss_components(epochs_arr, epoch_ce_losses, epoch_contrast_losses, epoch_lm_losses, epoch_total_losses)\n","\n","    # 此处可根据需要调用 ROC/PR 曲线、t-SNE、混淆矩阵等函数生成对应图表\n","    # 例如，利用测试集预测结果生成混淆矩阵\n","    test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n","    preds_list = []\n","    true_list = []\n","    with torch.no_grad():\n","        for batch in test_loader:\n","            input_ids = batch[\"input_ids\"].to(model_engine.local_rank)\n","            labels_batch = batch[\"labels\"].to(model_engine.local_rank)\n","            outputs = model_engine(input_ids)\n","            expanded_labels = []\n","            for i in range(labels_batch.size(0)):\n","                n_funcs = outputs[\"num_funcs\"][i]\n","                expanded_labels.append(labels_batch[i].repeat(n_funcs))\n","            expanded_labels = torch.cat(expanded_labels, dim=0).to(model_engine.local_rank)\n","            preds = torch.argmax(outputs[\"func_logits\"], dim=1)\n","            preds_list.extend(preds.cpu().numpy())\n","            true_list.extend(expanded_labels.cpu().numpy())\n","    visualization.plot_confusion_matrix(true_list, preds_list, classes=[\"nonVul\", \"Vul\"], normalize=True)\n","\n","    print(\"Training complete. Final checkpoint saved.\")\n","\n","if __name__ == \"__main__\":\n","    train()"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\n","import pandas as pd\n","import pyarrow.parquet as pq\n","import re\n","import json\n","\n","##################################\n","# Step 1: Merge the three Parquet files\n","##################################\n","\n","def merge_parquet_files(train_file, val_file, test_file, output_file):\n","    \"\"\"\n","    Merges three Parquet files (train, val, test) into one dataset.parquet.\n","    \"\"\"\n","    df_train = pd.read_parquet(train_file)\n","    df_val = pd.read_parquet(val_file)\n","    df_test = pd.read_parquet(test_file)\n","\n","    df_merged = pd.concat([df_train, df_val, df_test], ignore_index=True)\n","    df_merged.to_parquet(output_file, index=False)\n","    print(f\"[+] Merged dataset saved to: {output_file}\")\n","    return df_merged\n","\n","##################################\n","# Step 2: Parse vulnerabilities\n","##################################\n","\n","def parse_vulnerabilities(overlapping_str):\n","    \"\"\"\n","    overlapping_str is a JSON-like string describing vulnerabilities and line numbers.\n","    e.g. {\n","      \"IOU\": [\n","        [\n","          {\"defect\": \"...\", \"lines\": [178], \"tool\": \"soldetector\", ...},\n","          {\"error\": \"Integer Overflow.\", \"line\": 178, \"tool\": \"oyente\"}\n","        ],\n","        [\n","          ...\n","        ]\n","      ],\n","      \"NC\": [...]\n","    }\n","\n","    We collect *all* line numbers flagged as vulnerable (1-based).\n","    Returns a set of integers representing vulnerable line numbers.\n","    \"\"\"\n","    vul_lines = set()\n","    if not isinstance(overlapping_str, str) or not overlapping_str.strip():\n","        return vul_lines\n","\n","    try:\n","        data = json.loads(overlapping_str)\n","    except:\n","        # If JSON is invalid, return empty\n","        return vul_lines\n","\n","    # The structure can vary, but typically each vulnerability type has sub-lists\n","    # Each sub-list has multiple items from different tools, which may store\n","    # line data in \"line\" or \"lines\" fields.\n","    for vuln_type, sublists in data.items():\n","        for sublist in sublists:\n","            line_candidates = []\n","            for item in sublist:\n","                # item might have \"lines\": [X, Y], or \"line\": X\n","                if \"lines\" in item and isinstance(item[\"lines\"], list):\n","                    line_candidates.extend(item[\"lines\"])\n","                if \"line\" in item and isinstance(item[\"line\"], int):\n","                    line_candidates.append(item[\"line\"])\n","            for ln in line_candidates:\n","                vul_lines.add(ln)\n","\n","    return vul_lines\n","\n","##################################\n","# Step 3: Extract function ranges\n","##################################\n","\n","FUNCTION_RE = re.compile(r\"^\\s*(function|constructor)\\b\")\n","\n","def extract_function_ranges(source_code):\n","    \"\"\"\n","    Returns a list of (start_line, end_line, function_signature).\n","    A naive approach that scans for lines starting with 'function' or 'constructor'\n","    and counts curly braces to find the end of the function.\n","\n","    Lines are 1-based here.\n","    \"\"\"\n","    lines = source_code.split('\\n')\n","    n = len(lines)\n","\n","    func_ranges = []\n","    in_function = False\n","    brace_depth = 0\n","    func_start_line = None\n","    func_sig = None\n","\n","    for i, line in enumerate(lines):\n","        if not in_function:\n","            # Look for a function or constructor definition\n","            if FUNCTION_RE.search(line):\n","                in_function = True\n","                func_start_line = i + 1  # 1-based\n","                func_sig = line.strip()\n","                brace_depth = line.count('{') - line.count('}')\n","        else:\n","            brace_depth += (line.count('{') - line.count('}'))\n","\n","            if brace_depth <= 0:\n","                func_end_line = i + 1\n","                func_ranges.append((func_start_line, func_end_line, func_sig))\n","                in_function = False\n","                func_start_line = None\n","                func_sig = None\n","\n","    # If the file ends but we never closed the function\n","    if in_function and func_start_line is not None:\n","        func_ranges.append((func_start_line, n, func_sig))\n","\n","    return func_ranges\n","\n","##################################\n","# Step 4: Label functions with <Vul> or <nonVul>\n","##################################\n","\n","def label_functions(source_code, vul_lines):\n","    \"\"\"\n","    - For each function in source_code (based on naive brace count),\n","      check if any line in [start_line, end_line] is in vul_lines.\n","    - If yes, append \"// <Vul>\" on the last line of that function.\n","      Otherwise, \"// <nonVul>\".\n","    - Return:\n","        1) The entire contract with appended labels at function-level\n","        2) A list of function snippets, each with a label field\n","    \"\"\"\n","    lines = source_code.split('\\n')\n","    func_ranges = extract_function_ranges(source_code)\n","\n","    function_snippets = []\n","\n","    for (start_line, end_line, signature) in func_ranges:\n","        # Check if this function has a vulnerable line\n","        is_vul = any(start_line <= ln <= end_line for ln in vul_lines)\n","\n","        label_str = \"// <Vul>\" if is_vul else \"// <nonVul>\"\n","        last_idx = end_line - 1  # 0-based index\n","        if 0 <= last_idx < len(lines):\n","            lines[last_idx] += f\" {label_str}\"\n","\n","        # Extract just this function's snippet\n","        snippet_lines = lines[start_line-1 : end_line]\n","        snippet_text = \"\\n\".join(snippet_lines)\n","\n","        function_snippets.append({\n","            \"snippet\": snippet_text,\n","            \"label\": \"<Vul>\" if is_vul else \"<nonVul>\"\n","        })\n","\n","    labeled_code = \"\\n\".join(lines)\n","    return labeled_code, function_snippets\n","\n","##################################\n","# Step 5: Main driver\n","##################################\n","\n","def main():\n","    # 1) Merge parquet files into dataset.parquet\n","    train_file = \"train.parquet\"\n","    val_file = \"validation.parquet\"\n","    test_file = \"test.parquet\"\n","    merged_output_file = \"dataset.parquet\"\n","\n","    print(\"[+] Merging Parquet files...\")\n","    df_merged = merge_parquet_files(train_file, val_file, test_file, merged_output_file)\n","\n","    # 2) For each row, parse the vulnerabilities, label at function-level\n","    #    Collect function snippets into either VulSC.csv or nonVulSC.csv\n","    vul_functions = []\n","    non_vul_functions = []\n","\n","    print(\"[+] Labeling each contract's functions...\")\n","\n","    for idx, row in df_merged.iterrows():\n","        source_code = row[\"source_code\"]\n","        overlapping_str = row[\"overlapping\"]\n","\n","        # parse all vulnerable lines\n","        vul_lines = parse_vulnerabilities(overlapping_str)\n","\n","        # label function-level\n","        labeled_code, func_snippets = label_functions(source_code, vul_lines)\n","\n","        # Collect snippets\n","        for item in func_snippets:\n","            snippet_text = item[\"snippet\"]\n","            label = item[\"label\"]\n","            if label == \"<Vul>\":\n","                vul_functions.append(snippet_text)\n","            else:\n","                non_vul_functions.append(snippet_text)\n","\n","    # 3) Save to VulSC.csv and nonVulSC.csv\n","    df_vul = pd.DataFrame({\"source_code\": vul_functions})\n","    df_nonvul = pd.DataFrame({\"source_code\": non_vul_functions})\n","\n","    df_vul.to_csv(\"VulSC.csv\", index=False)\n","    df_nonvul.to_csv(\"nonVulSC.csv\", index=False)\n","\n","    print(\"[+] Saved VulSC.csv (vulnerable functions) and nonVulSC.csv (non-vulnerable functions).\")\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\n","\"\"\"\n","Script Overview:\n","1. Load `VulSC.csv` and `nonVulSC.csv` containing vulnerable and non-vulnerable functions.\n","2. Initialize the GPT-J-6B tokenizer with special tokens `<Vul>` and `<nonVul>`.\n","3. Count tokens in each function.\n","4. Bin the token counts into specified ranges.\n","5. Display the distribution of token counts.\n","6. Optionally, save the enhanced datasets with token counts and bins.\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","from transformers import GPT2TokenizerFast\n","from tqdm import tqdm\n","\n","def load_datasets(vul_csv_path, nonvul_csv_path):\n","    \"\"\"\n","    Load vulnerable and non-vulnerable function datasets.\n","\n","    Args:\n","        vul_csv_path (str): Path to `VulSC.csv`.\n","        nonvul_csv_path (str): Path to `nonVulSC.csv`.\n","\n","    Returns:\n","        tuple: (df_vul, df_nonvul) DataFrames for vulnerable and non-vulnerable functions.\n","    \"\"\"\n","    print(\"[+] Loading datasets...\")\n","    df_vul = pd.read_csv(vul_csv_path)\n","    df_nonvul = pd.read_csv(nonvul_csv_path)\n","    print(f\"[+] Loaded {len(df_vul)} vulnerable functions and {len(df_nonvul)} non-vulnerable functions.\")\n","    return df_vul, df_nonvul\n","\n","def initialize_tokenizer(special_tokens):\n","    \"\"\"\n","    Initialize the GPT-J-6B tokenizer and add special tokens.\n","\n","    Args:\n","        special_tokens (list of str): Special tokens to add.\n","\n","    Returns:\n","        GPT2TokenizerFast: Configured tokenizer.\n","    \"\"\"\n","    print(\"[+] Initializing GPT-J-6B tokenizer...\")\n","    tokenizer = GPT2TokenizerFast.from_pretrained(\"EleutherAI/gpt-j-6B\")\n","\n","    # Add special tokens\n","    print(\"[+] Adding special tokens to the tokenizer...\")\n","    tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})\n","    return tokenizer\n","\n","def count_tokens(df, tokenizer):\n","    \"\"\"\n","    Count the number of tokens in each function using the tokenizer.\n","\n","    Args:\n","        df (pd.DataFrame): DataFrame containing 'source_code'.\n","        tokenizer (GPT2TokenizerFast): Configured tokenizer.\n","\n","    Returns:\n","        pd.DataFrame: DataFrame with an added 'num_tokens' column.\n","    \"\"\"\n","    print(\"[+] Counting tokens in functions...\")\n","\n","    def tokenize_and_count(code):\n","        tokens = tokenizer.encode(code, add_special_tokens=False)\n","        return len(tokens)\n","\n","    # Apply token counting with progress bar\n","    tqdm.pandas(desc=\"Tokenizing Functions\")\n","    df[\"num_tokens\"] = df[\"source_code\"].progress_apply(tokenize_and_count)\n","\n","    return df\n","\n","def bin_tokens(df, bins, labels):\n","    \"\"\"\n","    Bin the token counts into specified ranges.\n","\n","    Args:\n","        df (pd.DataFrame): DataFrame with 'num_tokens' column.\n","        bins (list of int): Bin edges.\n","        labels (list of str): Labels for the bins.\n","\n","    Returns:\n","        pd.DataFrame: DataFrame with an added 'token_bin' column.\n","    \"\"\"\n","    print(\"[+] Binning token counts...\")\n","    df[\"token_bin\"] = pd.cut(df[\"num_tokens\"], bins=bins, labels=labels, right=False)\n","    return df\n","\n","def display_distributions(df_vul, df_nonvul, bins, labels):\n","    \"\"\"\n","    Display the distribution of token counts.\n","\n","    Args:\n","        df_vul (pd.DataFrame): Vulnerable functions DataFrame.\n","        df_nonvul (pd.DataFrame): Non-vulnerable functions DataFrame.\n","        bins (list of int): Bin edges.\n","        labels (list of str): Labels for the bins.\n","    \"\"\"\n","    num_vul_funcs = len(df_vul)\n","    num_nonvul_funcs = len(df_nonvul)\n","    total_funcs = num_vul_funcs + num_nonvul_funcs\n","\n","    print(\"\\n===== Function Counts =====\")\n","    print(f\"Vulnerable Functions: {num_vul_funcs}\")\n","    print(f\"Non-Vulnerable Functions: {num_nonvul_funcs}\")\n","    print(f\"Total Functions: {total_funcs}\")\n","\n","    print(\"\\n===== Token Count Distribution =====\")\n","    print(\"\\n[+] Vulnerable Functions:\")\n","    vul_dist = df_vul[\"token_bin\"].value_counts(sort=False).rename_axis('Token Bin').reset_index(name='Counts')\n","    print(vul_dist)\n","\n","    print(\"\\n[+] Non-Vulnerable Functions:\")\n","    nonvul_dist = df_nonvul[\"token_bin\"].value_counts(sort=False).rename_axis('Token Bin').reset_index(name='Counts')\n","    print(nonvul_dist)\n","\n","    print(\"\\n[+] All Functions Combined:\")\n","    combined_df = pd.concat([df_vul, df_nonvul], ignore_index=True)\n","    combined_dist = combined_df[\"token_bin\"].value_counts(sort=False).rename_axis('Token Bin').reset_index(name='Counts')\n","    print(combined_dist)\n","\n","def save_enhanced_datasets(df_vul, df_nonvul, vul_out_csv, nonvul_out_csv):\n","    \"\"\"\n","    Save the DataFrames with token counts and bins to CSV files.\n","\n","    Args:\n","        df_vul (pd.DataFrame): Vulnerable functions DataFrame.\n","        df_nonvul (pd.DataFrame): Non-vulnerable functions DataFrame.\n","        vul_out_csv (str): Output path for vulnerable functions.\n","        nonvul_out_csv (str): Output path for non-vulnerable functions.\n","    \"\"\"\n","    print(\"[+] Saving enhanced datasets with token counts and bins...\")\n","    df_vul.to_csv(vul_out_csv, index=False)\n","    df_nonvul.to_csv(nonvul_out_csv, index=False)\n","    print(f\"[+] Saved {vul_out_csv} and {nonvul_out_csv}.\")\n","\n","def main():\n","    # File paths\n","    vul_csv_path = \"VulSC.csv\"\n","    nonvul_csv_path = \"nonVulSC.csv\"\n","    vul_out_csv = \"VulSC_with_tokens.csv\"\n","    nonvul_out_csv = \"nonVulSC_with_tokens.csv\"\n","\n","    # Token bins and labels\n","    bins = [0, 50, 100, 200, 300, 500, 1000, 2000, np.inf]\n","    labels = [\"0-50\", \"50-100\", \"100-200\", \"200-300\", \"300-500\", \"500-1000\", \"1000-2000\", \">2000\"]\n","\n","    # Special tokens to add\n","    special_tokens = [\"<Vul>\", \"<nonVul>\"]\n","\n","    # Step 1: Load datasets\n","    df_vul, df_nonvul = load_datasets(vul_csv_path, nonvul_csv_path)\n","\n","    # Step 2: Initialize tokenizer\n","    tokenizer = initialize_tokenizer(special_tokens)\n","\n","    # Step 3: Count tokens\n","    df_vul = count_tokens(df_vul, tokenizer)\n","    df_nonvul = count_tokens(df_nonvul, tokenizer)\n","\n","    # Step 4: Bin tokens\n","    df_vul = bin_tokens(df_vul, bins, labels)\n","    df_nonvul = bin_tokens(df_nonvul, bins, labels)\n","\n","    # Step 5: Display distributions\n","    display_distributions(df_vul, df_nonvul, bins, labels)\n","\n","    # Step 6: Save enhanced datasets\n","    save_enhanced_datasets(df_vul, df_nonvul, vul_out_csv, nonvul_out_csv)\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["#build the balanced dataset\n","#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\n","\"\"\"\n","Script Overview:\n","1. Load `VulSC.csv` and `nonVulSC.csv`.\n","2. Add a `label` column to each dataset (`1` for vulnerable, `0` for non-vulnerable).\n","3. Randomly select 714 non-vulnerable functions to balance the dataset.\n","4. Combine the selected non-vulnerable functions with all vulnerable functions.\n","5. Shuffle the combined dataset.\n","6. Save the balanced dataset as `BalancedDataset.csv`.\n","\"\"\"\n","\n","import pandas as pd\n","from tqdm import tqdm\n","\n","def load_datasets(vul_csv_path, nonvul_csv_path):\n","    \"\"\"\n","    Load vulnerable and non-vulnerable function datasets.\n","\n","    Args:\n","        vul_csv_path (str): Path to `VulSC.csv`.\n","        nonvul_csv_path (str): Path to `nonVulSC.csv`.\n","\n","    Returns:\n","        tuple: (df_vul, df_nonvul) DataFrames for vulnerable and non-vulnerable functions.\n","    \"\"\"\n","    print(\"[+] Loading datasets...\")\n","\n","    try:\n","        df_vul = pd.read_csv(vul_csv_path)\n","        print(f\"[+] Loaded {len(df_vul)} vulnerable functions from {vul_csv_path}.\")\n","    except FileNotFoundError:\n","        print(f\"[-] File not found: {vul_csv_path}\")\n","        exit(1)\n","    except Exception as e:\n","        print(f\"[-] Error loading {vul_csv_path}: {e}\")\n","        exit(1)\n","\n","    try:\n","        df_nonvul = pd.read_csv(nonvul_csv_path)\n","        print(f\"[+] Loaded {len(df_nonvul)} non-vulnerable functions from {nonvul_csv_path}.\")\n","    except FileNotFoundError:\n","        print(f\"[-] File not found: {nonvul_csv_path}\")\n","        exit(1)\n","    except Exception as e:\n","        print(f\"[-] Error loading {nonvul_csv_path}: {e}\")\n","        exit(1)\n","\n","    return df_vul, df_nonvul\n","\n","def add_labels(df_vul, df_nonvul):\n","    \"\"\"\n","    Add a `label` column to each DataFrame.\n","\n","    Args:\n","        df_vul (pd.DataFrame): Vulnerable functions DataFrame.\n","        df_nonvul (pd.DataFrame): Non-vulnerable functions DataFrame.\n","\n","    Returns:\n","        tuple: Updated DataFrames with `label` columns.\n","    \"\"\"\n","    print(\"[+] Adding label columns...\")\n","    df_vul = df_vul.copy()\n","    df_nonvul = df_nonvul.copy()\n","\n","    df_vul['label'] = 1\n","    df_nonvul['label'] = 0\n","\n","    return df_vul, df_nonvul\n","\n","def sample_nonvulnerable(df_nonvul, sample_size, random_state=42):\n","    \"\"\"\n","    Randomly sample non-vulnerable functions.\n","\n","    Args:\n","        df_nonvul (pd.DataFrame): Non-vulnerable functions DataFrame.\n","        sample_size (int): Number of samples to select.\n","        random_state (int): Seed for reproducibility.\n","\n","    Returns:\n","        pd.DataFrame: Sampled non-vulnerable functions.\n","    \"\"\"\n","    print(f\"[+] Sampling {sample_size} non-vulnerable functions...\")\n","\n","    if len(df_nonvul) < sample_size:\n","        print(f\"[-] Not enough non-vulnerable functions to sample. Available: {len(df_nonvul)}, Required: {sample_size}\")\n","        exit(1)\n","\n","    df_nonvul_sampled = df_nonvul.sample(n=sample_size, random_state=random_state).reset_index(drop=True)\n","    print(f\"[+] Sampled {len(df_nonvul_sampled)} non-vulnerable functions.\")\n","\n","    return df_nonvul_sampled\n","\n","def create_balanced_dataset(df_vul, df_nonvul_sampled):\n","    \"\"\"\n","    Combine vulnerable and sampled non-vulnerable functions into a balanced dataset.\n","\n","    Args:\n","        df_vul (pd.DataFrame): Vulnerable functions DataFrame.\n","        df_nonvul_sampled (pd.DataFrame): Sampled non-vulnerable functions DataFrame.\n","\n","    Returns:\n","        pd.DataFrame: Combined and shuffled balanced dataset.\n","    \"\"\"\n","    print(\"[+] Combining vulnerable and non-vulnerable functions...\")\n","    balanced_df = pd.concat([df_vul, df_nonvul_sampled], ignore_index=True)\n","    print(f\"[+] Combined dataset size: {len(balanced_df)} functions.\")\n","\n","    print(\"[+] Shuffling the combined dataset...\")\n","    balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n","    print(\"[+] Shuffled the combined dataset.\")\n","\n","    return balanced_df\n","\n","def save_balanced_dataset(balanced_df, output_csv_path):\n","    \"\"\"\n","    Save the balanced dataset to a CSV file.\n","\n","    Args:\n","        balanced_df (pd.DataFrame): Balanced dataset DataFrame.\n","        output_csv_path (str): Path to save the balanced dataset CSV.\n","    \"\"\"\n","    print(f\"[+] Saving balanced dataset to {output_csv_path}...\")\n","    try:\n","        balanced_df.to_csv(output_csv_path, index=False)\n","        print(f\"[+] Balanced dataset saved successfully to {output_csv_path}.\")\n","    except Exception as e:\n","        print(f\"[-] Error saving balanced dataset: {e}\")\n","        exit(1)\n","\n","def main():\n","    # File paths\n","    vul_csv_path = \"VulSC.csv\"\n","    nonvul_csv_path = \"nonVulSC.csv\"\n","    output_balanced_csv = \"BalancedDataset.csv\"\n","    sample_size = 714  # Number of non-vulnerable functions to sample\n","\n","    # Step 1: Load datasets\n","    df_vul, df_nonvul = load_datasets(vul_csv_path, nonvul_csv_path)\n","\n","    # Step 2: Add label columns\n","    df_vul, df_nonvul = add_labels(df_vul, df_nonvul)\n","\n","    # Step 3: Sample non-vulnerable functions\n","    df_nonvul_sampled = sample_nonvulnerable(df_nonvul, sample_size, random_state=42)\n","\n","    # Step 4: Create balanced dataset\n","    balanced_df = create_balanced_dataset(df_vul, df_nonvul_sampled)\n","\n","    # Optional: Verify the balance\n","    print(\"\\n===== Balanced Dataset Summary =====\")\n","    print(balanced_df['label'].value_counts())\n","\n","    # Step 5: Save the balanced dataset\n","    save_balanced_dataset(balanced_df, output_balanced_csv)\n","\n","    print(\"\\n[+] Balanced dataset creation completed successfully.\")\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["#initialize the tokenizer with special tokens\n","#GPTJ-6B uses a tokenizer compatible with GPT-2. we'll add <vul> and <nonvul> as special tokens\n","\n","from transformers import GPT2TokenizerFast\n","\n","# Initialize the tokenizer\n","tokenizer = GPT2TokenizerFast.from_pretrained(\"EleutherAI/gpt-j-6B\")\n","\n","# Add special tokens\n","special_tokens_dict = {'additional_special_tokens': ['<Vul>', '<nonVul>', '<EOS>']}\n","num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n","print(f\"Added {num_added_toks} special tokens.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["#define custom datasets for next-token-prediction loss and binary classification loss\n","# next token prediction: each training example is formatted as function_code+label+<EOS>\n","#binary classification : each training example is formated as function_code+<EOS>, with labels indicating vulnerability\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# Load the balanced dataset\n","balanced_df = pd.read_csv(\"BalancedDataset.csv\")\n","\n","# Shuffle the dataset\n","balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n","\n","# Split into train, validation, and test sets\n","train_df, temp_df = train_test_split(balanced_df, test_size=0.2, random_state=42, stratify=balanced_df['label'])\n","val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])\n","\n","# # Save splits if needed\n","# train_df.to_csv(\"train_balanced.csv\", index=False)\n","# val_df.to_csv(\"validation_balanced.csv\", index=False)\n","# test_df.to_csv(\"test_balanced.csv\", index=False)\n","\n","#create NTP dataset\n","def create_ntp_format(df):\n","    \"\"\"\n","    Formats the dataset for Next Token Prediction.\n","    Each example: function_code + label + <EOS>\n","\n","    Args:\n","        df (pd.DataFrame): DataFrame with 'source_code' and 'label' columns.\n","\n","    Returns:\n","        List[str]: List of formatted strings.\n","    \"\"\"\n","    ntp_data = []\n","    for _, row in df.iterrows():\n","        function_code = row['source_code']\n","        label = \"<Vul>\" if row['label'] == 1 else \"<nonVul>\"\n","        formatted = f\"{function_code} {label} <EOS>\"\n","        ntp_data.append(formatted)\n","    return ntp_data\n","\n","#create BC dataset\n","def create_bc_format(df):\n","    \"\"\"\n","    Formats the dataset for Binary Classification.\n","    Each example: function_code + <EOS>\n","\n","    Args:\n","        df (pd.DataFrame): DataFrame with 'source_code' and 'label' columns.\n","\n","    Returns:\n","        List[dict]: List of dictionaries with 'text' and 'label' keys.\n","    \"\"\"\n","    bc_data = []\n","    for _, row in df.iterrows():\n","        function_code = row['source_code']\n","        label = row['label']\n","        formatted = f\"{function_code} <EOS>\"\n","        bc_data.append({'text': formatted, 'label': label})\n","    return bc_data\n","\n","#set up the GPTJ-6B model with LoRA\n","#use PEFT library to apply LoRA to finetune GPTJ-6B for efficient fine-tuning\n","from transformers import GPTJForCausalLM, AutoConfig\n","from peft import LoraConfig, get_peft_model\n","\n","# Load model configuration\n","config = AutoConfig.from_pretrained(\"ModelGPTJ4SC/model\")\n","config.num_labels = 1  # For binary classification\n","\n","# Load the GPT-J-6B model\n","model = GPTJForCausalLM.from_pretrained(\n","    \"ModelGPTJ4SC/model\",\n","    load_in_8bit=True,  # Enable int8 quantization\n","    device_map=\"auto\"\n",")\n","\n","# Resize token embeddings to accommodate new special tokens\n","model.resize_token_embeddings(len(tokenizer))\n","\n","# Define LoRA configuration\n","lora_config = LoraConfig(\n","    r=8,\n","    lora_alpha=32,\n","    lora_dropout=0.1,\n","    target_modules = [\"q_proj\", \"v_proj\", \"fc_out\"],\n","    task_type=\"CAUSAL_LM\"\n",")\n","\n","# Apply LoRA to the model\n","model = get_peft_model(model, lora_config)"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["#define traing arguments and trainer\n","#define custom datasets for NTP and BC, respectively\n","from datasets import Dataset\n","from transformers import TrainingArguments\n","import torch\n","from torch import nn\n","from transformers import Trainer\n","\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","# Create NTP dataset\n","ntp_train_data = create_ntp_format(train_df)\n","ntp_val_data = create_ntp_format(val_df)\n","ntp_test_data = create_ntp_format(test_df)\n","\n","ntp_train_dataset = Dataset.from_dict({\"text\": ntp_train_data})\n","ntp_val_dataset = Dataset.from_dict({\"text\": ntp_val_data})\n","ntp_test_dataset = Dataset.from_dict({\"text\": ntp_test_data})\n","\n","# Create BC dataset\n","bc_train_data = create_bc_format(train_df)\n","bc_val_data = create_bc_format(val_df)\n","bc_test_data = create_bc_format(test_df)\n","\n","bc_train_dataset = Dataset.from_list(bc_train_data)\n","bc_val_dataset = Dataset.from_list(bc_val_data)\n","bc_test_dataset = Dataset.from_list(bc_test_data)\n","\n","#tokenization functions\n","#NTP tokenization\n","def tokenize_ntp_function(examples):\n","    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=1024)\n","\n","#BC tokenization\n","def tokenize_bc_function(examples):\n","    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=1024)\n","\n","#apply tokenization\n","# Tokenize NTP datasets\n","ntp_train_tokenized = ntp_train_dataset.map(tokenize_ntp_function, batched=True)\n","ntp_val_tokenized = ntp_val_dataset.map(tokenize_ntp_function, batched=True)\n","ntp_test_tokenized = ntp_test_dataset.map(tokenize_ntp_function, batched=True)\n","\n","# Tokenize BC datasets\n","bc_train_tokenized = bc_train_dataset.map(tokenize_bc_function, batched=True)\n","bc_val_tokenized = bc_val_dataset.map(tokenize_bc_function, batched=True)\n","bc_test_tokenized = bc_test_dataset.map(tokenize_bc_function, batched=True)\n","\n","# Set format for PyTorch\n","ntp_train_tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n","ntp_val_tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n","ntp_test_tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n","\n","bc_train_tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n","bc_val_tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n","bc_test_tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n","\n","#define training arguments\n","#define separate training arguments for NTP and BC\n","\n","# Common training arguments\n","common_training_args = {\n","    #\"output_dir\": \"./gptj_finetuned\",\n","    \"overwrite_output_dir\": True,\n","    \"num_train_epochs\": 5,\n","    \"per_device_train_batch_size\": 1,  # Adjust based on GPU memory\n","    \"per_device_eval_batch_size\": 1,\n","    \"gradient_accumulation_steps\": 16,  # To simulate larger batch size\n","    \"evaluation_strategy\": \"epoch\",\n","    \"save_strategy\": \"epoch\",\n","    #\"logging_dir\": \"./logs\",\n","    \"logging_steps\": 100,\n","    \"fp16\": True,  # Enable mixed precision\n","    #\"gradient_checkpointing\": True,  # Save memory\n","    \"report_to\": \"tensorboard\",  # 'none' Disables reporting to WandB or other platforms\n","    #\"remove_unused_columns\": False,\n","    # \"load_best_model_at_end\": True,  # Enable loading the best model\n","    # \"metric_for_best_model\": \"accuracy\",  # Specify the metric for best model\n","    # \"greater_is_better\": True,  # Whether the metric should be maximized\n","}\n","\n","# NTP training arguments\n","ntp_training_args = TrainingArguments(\n","    **common_training_args,\n","    output_dir=\"./gptj_finetuned_ntp\",\n","    logging_dir=\"./logs_ntp\",\n",")\n","\n","# BC training arguments\n","bc_training_args = TrainingArguments(\n","    **common_training_args,\n","    output_dir=\"./gptj_finetuned_bc\",\n","    logging_dir=\"./logs_bc\",\n",")\n","\n","#define data collators\n","#for NTP, next token prediction, same as the orignal task, so we can load by libraries\n","from transformers import DataCollatorForLanguageModeling\n","\n","# NTP Data Collator\n","ntp_data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer,\n","    mlm=False  # Causal LM\n",")\n","\n","# def bc_data_collator(features):\n","#         batch = {\n","#             'input_ids': torch.stack([f['input_ids'] for f in features]),\n","#             'attention_mask': torch.stack([f['attention_mask'] for f in features]),\n","#             'labels': torch.tensor([f['label'] for f in features])\n","#         }\n","#         return batch\n","\n","#for BC, we need implement a custom data collator to handle labels\n","\n","class BinaryClassificationTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n","        \"\"\"\n","        Custom loss computation for Binary Classification.\n","\n","        Args:\n","            model: The model to compute the loss with.\n","            inputs: A batch of inputs.\n","            return_outputs: Whether to return model outputs.\n","\n","        Returns:\n","            The loss, and optionally the model outputs.\n","        \"\"\"\n","        labels = inputs.pop(\"labels\")\n","        outputs = model(**inputs)\n","        logits = outputs.logits  # [batch_size, seq_len, vocab_size]\n","\n","        # Find the index of <EOS>\n","        eos_token_id = tokenizer.convert_tokens_to_ids('<EOS>')\n","        # Mask to select only the logits corresponding to <EOS>\n","        eos_mask = (inputs['input_ids'] == eos_token_id)\n","        # For each example in the batch, get the logits at the <EOS> position\n","        eos_logits = []\n","        for i in range(logits.size(0)):\n","            eos_positions = torch.where(eos_mask[i])[0]\n","            if len(eos_positions) == 0:\n","                # If <EOS> not found, take the last token\n","                eos_pos = logits.size(1) - 1\n","            else:\n","                # Take the last occurrence of <EOS>\n","                eos_pos = eos_positions[-1]\n","            eos_logits.append(logits[i, eos_pos, :])\n","        eos_logits = torch.stack(eos_logits)  # [batch_size, vocab_size]\n","\n","        # For binary classification, map logits to a single probability\n","        # We'll map to the probability of predicting '<Vul>' as positive class\n","        # Assuming '<Vul>' and '<nonVul>' are part of the vocabulary\n","        vul_token_id = tokenizer.convert_tokens_to_ids('<Vul>')\n","        nonvul_token_id = tokenizer.convert_tokens_to_ids('<nonVul>')\n","\n","        # Compute probability for '<Vul>'\n","        vul_probs = torch.softmax(eos_logits, dim=-1)[:, vul_token_id]\n","\n","        # Labels should be 1 or 0\n","        labels = labels.float()\n","\n","        # Define Binary Cross-Entropy loss\n","        loss_fn = nn.BCELoss()\n","        loss = loss_fn(vul_probs, labels)\n","\n","        return (loss, outputs) if return_outputs else loss\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["from sklearn.metrics import roc_auc_score, accuracy_score, precision_recall_fscore_support\n","from torch import nn\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from transformers import EarlyStoppingCallback\n","import math\n","\n","def compute_metrics_ntp(eval_pred):\n","    \"\"\"\n","    Compute metrics for NTP evaluation.\n","    For NTP, perplexity is a standard metric.\n","    \"\"\"\n","    eval_loss = eval_pred.get(\"eval_loss\", None)\n","    if eval_loss is not None:\n","        perplexity = math.exp(eval_loss)\n","    else:\n","        perplexity = 'N/A'\n","    print(f\"[DEBUG] NTP - eval_loss: {eval_loss}, perplexity: {perplexity}\")  # Debugging\n","    return {\"perplexity\": perplexity}\n","\n","\n","from sklearn.metrics import roc_auc_score, accuracy_score, precision_recall_fscore_support\n","\n","def compute_metrics_bc(eval_pred):\n","    \"\"\"\n","    Compute metrics for BC evaluation.\n","    Includes ROC AUC, Accuracy, Precision, Recall, and F1-Score.\n","    \"\"\"\n","    logits, labels = eval_pred\n","    # Convert logits to probabilities for the positive class (index 1)\n","    probs = torch.softmax(torch.tensor(logits), dim=-1)[:, 1].numpy()\n","    labels = labels.numpy()\n","\n","    print(f\"[DEBUG] BC - Labels: {labels}\")\n","    print(f\"[DEBUG] BC - Probabilities: {probs}\")\n","\n","    # Compute metrics\n","    try:\n","        roc_auc = roc_auc_score(labels, probs)\n","    except ValueError:\n","        roc_auc = 'N/A'  # Handle cases where ROC AUC cannot be computed\n","\n","    predicted_labels = (probs >= 0.5).astype(int)\n","    accuracy = accuracy_score(labels, predicted_labels)\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, predicted_labels, average='binary')\n","\n","    print(f\"[DEBUG] BC - ROC AUC: {roc_auc}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-Score: {f1}\")\n","\n","    return {\n","        'roc_auc': roc_auc,\n","        'accuracy': accuracy,\n","        'precision': precision,\n","        'recall': recall,\n","        'f1_score': f1,  # Changed key to 'f1_score' for consistency\n","    }\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["from transformers import Trainer\n","\n","# NTP Trainer\n","ntp_trainer = Trainer(\n","    model=model,  # GPT-J-6B with LoRA and quantization\n","    args=ntp_training_args,\n","    train_dataset=ntp_train_tokenized,\n","    eval_dataset=ntp_val_tokenized,\n","    data_collator=ntp_data_collator\n","    #compute_metrics=compute_metrics_ntp\n",")\n","\n","# BC Trainer with custom loss\n","bc_trainer = BinaryClassificationTrainer(\n","    model=model,  # GPT-J-6B with LoRA and quantization\n","    args=bc_training_args,\n","    train_dataset=bc_train_tokenized,\n","    eval_dataset=bc_val_tokenized,\n","    data_collator=lambda data: {\n","        'input_ids': torch.stack([f['input_ids'] for f in data]),\n","        'attention_mask': torch.stack([f['attention_mask'] for f in data]),\n","        'labels': torch.tensor([f['label'] for f in data])\n","    }\n","    #compute_metrics=compute_metrics_bc\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["#finetune the model using both NTP and BC loss functions\n","#finetune with next token prediction loss function\n","print(\"=== Starting Fine-Tuning with Next Token Prediction (NTP) ===\")\n","ntp_trainer.train()\n","ntp_trainer.save_model(\"./gptj_finetuned_ntp\")\n","print(\"=== Fine-Tuning with NTP Completed ===\")"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNK48hu+QwmTyuXx0YHtkI3","gpuType":"A100","mount_file_id":"1D3-F3ig4BmplDWsFBLAlXHQcpHYyIKTh","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
