{"cells":[{"cell_type":"markdown","id":"c07f4f80","metadata":{"papermill":{"duration":0.017054,"end_time":"2024-09-06T09:15:41.986042","exception":false,"start_time":"2024-09-06T09:15:41.968988","status":"completed"},"tags":[]},"source":["<hr>"]},{"cell_type":"markdown","id":"f208fa37","metadata":{},"source":["本notebook实现从0到1构建自己的大模型"]},{"cell_type":"markdown","id":"5d66b2a0","metadata":{"papermill":{"duration":0.016493,"end_time":"2024-09-06T09:15:42.019274","exception":false,"start_time":"2024-09-06T09:15:42.002781","status":"completed"},"tags":[]},"source":["#Setup Environment\n","安装和导入必要的包"]},{"cell_type":"code","execution_count":null,"id":"e8a07de6","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-09-06T09:15:42.055811Z","iopub.status.busy":"2024-09-06T09:15:42.055157Z","iopub.status.idle":"2024-09-06T09:15:56.624119Z","shell.execute_reply":"2024-09-06T09:15:56.622995Z"},"papermill":{"duration":14.590302,"end_time":"2024-09-06T09:15:56.626283","exception":false,"start_time":"2024-09-06T09:15:42.035981","status":"completed"},"tags":[]},"outputs":[],"source":["\n","!pip install --upgrade --quiet tiktoken"]},{"cell_type":"code","execution_count":2,"id":"a16e8ebb","metadata":{"execution":{"iopub.execute_input":"2024-09-06T09:15:56.661832Z","iopub.status.busy":"2024-09-06T09:15:56.661455Z","iopub.status.idle":"2024-09-06T09:15:59.972301Z","shell.execute_reply":"2024-09-06T09:15:59.971479Z"},"papermill":{"duration":3.331245,"end_time":"2024-09-06T09:15:59.974725","exception":false,"start_time":"2024-09-06T09:15:56.64348","status":"completed"},"tags":[]},"outputs":[],"source":["# 导入包\n","import numpy as np\n","import torch \n","from torch.utils.data import Dataset, DataLoader\n","from torch import nn \n","import tiktoken as ttk\n","\n","from tqdm import tqdm\n","import os\n","import urllib.request\n","from dataclasses import dataclass, field,asdict, replace\n","\n","from typing import Dict"]},{"cell_type":"code","execution_count":3,"id":"bbefc9ae","metadata":{"execution":{"iopub.execute_input":"2024-09-06T09:16:00.011209Z","iopub.status.busy":"2024-09-06T09:16:00.010214Z","iopub.status.idle":"2024-09-06T09:16:00.461197Z","shell.execute_reply":"2024-09-06T09:16:00.46016Z"},"papermill":{"duration":0.471468,"end_time":"2024-09-06T09:16:00.463635","exception":false,"start_time":"2024-09-06T09:15:59.992167","status":"completed"},"tags":[]},"outputs":[],"source":["# 从仓库获得数据（raw text,需要进一步处理才能得到模型可处理的数据形式）\n","if not os.path.exists(\"the-verdict.txt\"):\n","    url = (\"https://raw.githubusercontent.com/rasbt/\"\n","           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n","           \"the-verdict.txt\")\n","    file_path = \"the-verdict.txt\"\n","    urllib.request.urlretrieve(url, file_path)"]},{"cell_type":"code","execution_count":4,"id":"845a3860","metadata":{"execution":{"iopub.execute_input":"2024-09-06T09:16:00.499617Z","iopub.status.busy":"2024-09-06T09:16:00.49924Z","iopub.status.idle":"2024-09-06T09:16:00.564572Z","shell.execute_reply":"2024-09-06T09:16:00.563599Z"},"papermill":{"duration":0.085571,"end_time":"2024-09-06T09:16:00.566598","exception":false,"start_time":"2024-09-06T09:16:00.481027","status":"completed"},"tags":[]},"outputs":[],"source":["@dataclass\n","class GPTConfig:\n","    vocab_size: int = 50257\n","    context_length: int = 512\n","    emb_dim: int = 768\n","    n_heads: int = 12\n","    n_layers: int = 12\n","    drop_rate: float = 0.1\n","    qkv_bias: bool = False\n","\n","    def to_dict(self) -> dict:\n","        return asdict(self)\n","\n","    def __repr__(self) -> str:\n","        config_dict = self.to_dict()\n","        formatted_items = [f'\"{key}\": {repr(value)}' for key, value in config_dict.items()]\n","        return \"GPT_CONFIG_124M = {\\n    \" + \",\\n    \".join(formatted_items) + \"\\n}\"\n","\n","@dataclass\n","class DataConfig:\n","    dataPath: str =r'/kaggle/working/the-verdict.txt'\n","    max_length: int = GPTConfig.context_length\n","    batch_size: int = 64\n","    train_ratio : float = 0.90\n","    stride: int = GPTConfig.context_length\n","#     def __post_init__(self):\n","#         self.stride = self.max_length // 2\n","DataConfig =DataConfig()\n","GPTConfig=GPTConfig()\n","device =  torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":5,"id":"5b76fb35","metadata":{"execution":{"iopub.execute_input":"2024-09-06T09:16:00.602054Z","iopub.status.busy":"2024-09-06T09:16:00.60171Z","iopub.status.idle":"2024-09-06T09:16:00.608622Z","shell.execute_reply":"2024-09-06T09:16:00.607738Z"},"papermill":{"duration":0.027053,"end_time":"2024-09-06T09:16:00.61083","exception":false,"start_time":"2024-09-06T09:16:00.583777","status":"completed"},"tags":[]},"outputs":[],"source":["def read_txt(path: str) -> str:\n","    try:\n","        with open(path, \"r\", encoding=\"utf-8\") as f:\n","            raw_text = f.read()\n","        return raw_text\n","    except FileNotFoundError:\n","        print(f\"Error: File not found at {path}\")\n","        return \"\"\n","    except Exception as e:\n","        print(f\"An error occurred: {e}\")\n","        return \"\"\n","\n","def text_to_token_ids(text, tokenizer): #text--》token id映射\n","    encoded = tokenizer.encode(text)\n","    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension\n","    return encoded_tensor\n","\n","\n","def token_ids_to_text(token_ids, tokenizer): #token id-》text解码\n","    flat = token_ids.squeeze(0)  # remove batch dimension\n","    return tokenizer.decode(flat.tolist())"]},{"cell_type":"markdown","id":"f8b85d45","metadata":{},"source":["大模型处理流程：数据--》tokenizer--》模型--post-processing--》输出\n","第一步：数据处理：分词（token）--》token id---》embedding--》model processing"]},{"cell_type":"code","execution_count":6,"id":"ead0480f","metadata":{"execution":{"iopub.execute_input":"2024-09-06T09:16:00.714411Z","iopub.status.busy":"2024-09-06T09:16:00.713681Z","iopub.status.idle":"2024-09-06T09:16:00.724322Z","shell.execute_reply":"2024-09-06T09:16:00.723479Z"},"papermill":{"duration":0.030598,"end_time":"2024-09-06T09:16:00.72625","exception":false,"start_time":"2024-09-06T09:16:00.695652","status":"completed"},"tags":[]},"outputs":[],"source":["\n","class LLMDataset(Dataset):\n","    \"\"\"\n","   模拟gpt这种decoder-only架构的模型，生成式任务。所以自定义数据集类来处理文本数据，将他们转化为输入和输出序列用来语言建模\n","\n","    Args:\n","        txt (str): The input text to be tokenized and processed.\n","        tokenizer (Tokenizer): The tokenizer to be used for encoding the text.\n","        max_length (int): The maximum length of each input sequence.\n","        stride (int): The number of tokens to skip between sequences.\n","    \"\"\"\n","\n","    def __init__(self, txt, tokenizer, max_length: int, stride: int):\n","        self.tokenizer = tokenizer\n","        token_ids = tokenizer.encode(txt)\n","        self.input_ids = []\n","        self.target_ids = []\n","        \n","        for i in tqdm(range(0, len(token_ids) - max_length, stride)):\n","            input_chunk = token_ids[i:i + max_length]\n","            target_chunk = token_ids[i + 1:i + max_length + 1]\n","            self.input_ids.append(torch.tensor(input_chunk))\n","            self.target_ids.append(torch.tensor(target_chunk))\n","\n","    def __len__(self):\n","        \"\"\"\n","        返回数据集样本数目\n","        \"\"\"\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx):\n","        return self.input_ids[idx], self.target_ids[idx]\n","\n","\n","def LLM_DataLoader(txt, tokenizer, batch_size: int, max_length: int, stride: int,\n","                   shuffle: bool = True, drop_last: bool = True):\n","    \"\"\"\n","    加载LLMDataset\n","\n","    Args:\n","        txt (str): The input text to be tokenized and processed.\n","        tokenizer (Tokenizer): The tokenizer to be used for encoding the text.\n","        batch_size (int): The number of samples per batch to load.\n","        max_length (int): The maximum length of each input sequence.\n","        stride (int): The number of tokens to skip between sequences.\n","        shuffle (bool, optional): Whether to shuffle the data at every epoch. Defaults to True.\n","        drop_last (bool, optional): Whether to drop the last incomplete batch. Defaults to True.\n","\n","    \"\"\"\n","    llmdataset = LLMDataset(txt, tokenizer, max_length, stride)\n","    llmdataloader = DataLoader(llmdataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n","    return llmdataloader\n"]},{"cell_type":"code","execution_count":null,"id":"508507fa","metadata":{"execution":{"iopub.execute_input":"2024-09-06T09:16:00.761894Z","iopub.status.busy":"2024-09-06T09:16:00.761106Z","iopub.status.idle":"2024-09-06T09:16:05.736059Z","shell.execute_reply":"2024-09-06T09:16:05.734999Z"},"papermill":{"duration":4.995273,"end_time":"2024-09-06T09:16:05.738383","exception":false,"start_time":"2024-09-06T09:16:00.74311","status":"completed"},"tags":[]},"outputs":[],"source":["# 加载数据集，tokenizer化测试\n","raw_data = read_txt(DataConfig.dataPath)\n","tokenizer = ttk.get_encoding(\"gpt2\") \n","\n","total_token = len(tokenizer.encode(raw_data))\n","print(f\"-> Number of Characters : {len(raw_data)}\\n-> Number of Tokens : {total_token}\")"]},{"cell_type":"code","execution_count":null,"id":"7f826fca","metadata":{"execution":{"iopub.execute_input":"2024-09-06T09:16:05.774777Z","iopub.status.busy":"2024-09-06T09:16:05.7744Z","iopub.status.idle":"2024-09-06T09:16:05.781057Z","shell.execute_reply":"2024-09-06T09:16:05.780299Z"},"papermill":{"duration":0.027206,"end_time":"2024-09-06T09:16:05.783225","exception":false,"start_time":"2024-09-06T09:16:05.756019","status":"completed"},"tags":[]},"outputs":[],"source":["# 将数据集split成训练集，测试集\n","train_ratio = DataConfig.train_ratio\n","split_idxs = int(train_ratio * len(raw_data))\n","train_data = raw_data[:split_idxs]\n","val_data = raw_data[split_idxs:]\n","print(f'-> Length of training data : {len(train_data)}\\n-> Length of val_data : {len(val_data)}')\n","\n","\n","# Sanity check\n","if total_token * (train_ratio) < GPTConfig.context_length:\n","    print(\"Not enough tokens for the training loader. \"\n","          \"Try to lower the `GPTConfig.context_length or \"\n","          \"increase the `training_ratio`\")\n","\n","if total_token * (1-train_ratio) < GPTConfig.context_length:\n","    print(\"Not enough tokens for the validation loader. \"\n","          \"Try to lower the `GPTConfig.context_length` or \"\n","          \"decrease the `training_ratio`\")"]},{"cell_type":"code","execution_count":null,"id":"00b70473","metadata":{"execution":{"iopub.execute_input":"2024-09-06T09:16:05.818826Z","iopub.status.busy":"2024-09-06T09:16:05.818467Z","iopub.status.idle":"2024-09-06T09:16:05.901543Z","shell.execute_reply":"2024-09-06T09:16:05.900488Z"},"papermill":{"duration":0.103755,"end_time":"2024-09-06T09:16:05.904088","exception":false,"start_time":"2024-09-06T09:16:05.800333","status":"completed"},"tags":[]},"outputs":[],"source":["# 处理成LLM可接受的输入\n","train_dataloader = LLM_DataLoader(\n","  txt= train_data,\n","  tokenizer = tokenizer,\n","  max_length = DataConfig.max_length,\n","  batch_size =  DataConfig.batch_size,\n","  stride =  DataConfig.stride,\n","  shuffle=False,\n","  drop_last = False\n",")\n","\n","#测试\n","print(\"View example:\")\n","dataiter = iter(train_dataloader)\n","firstbatch =next(dataiter)\n","print(f'inputs: \\n{firstbatch[0]} \\ntarget: \\n{firstbatch[1]}')\n","firstbatch[0].shape"]},{"cell_type":"code","execution_count":null,"id":"fd845f23","metadata":{"execution":{"iopub.execute_input":"2024-09-06T09:16:05.94073Z","iopub.status.busy":"2024-09-06T09:16:05.940151Z","iopub.status.idle":"2024-09-06T09:16:05.952223Z","shell.execute_reply":"2024-09-06T09:16:05.951291Z"},"papermill":{"duration":0.032614,"end_time":"2024-09-06T09:16:05.954289","exception":false,"start_time":"2024-09-06T09:16:05.921675","status":"completed"},"tags":[]},"outputs":[],"source":["val_dataloader = LLM_DataLoader(\n","  txt= val_data,\n","  tokenizer = tokenizer,\n","  max_length = DataConfig.max_length,\n","  batch_size =  DataConfig.batch_size,\n","  stride =  DataConfig.stride,\n","  shuffle=False,\n","  drop_last = False\n",")\n","#测试\n","dataiter = iter(val_dataloader)\n","firstbatch =next(dataiter)\n","firstbatch[0].shape\n","\n","#可用来训练"]},{"cell_type":"markdown","id":"ffbe200d","metadata":{},"source":["#实现多头注意力机制\n","#关于为什么引入多头注意力机制，该机制作用等，可见大模型学习笔记"]},{"cell_type":"code","execution_count":11,"id":"cac7178f","metadata":{"execution":{"iopub.execute_input":"2024-09-06T09:16:06.139293Z","iopub.status.busy":"2024-09-06T09:16:06.138915Z","iopub.status.idle":"2024-09-06T09:16:06.152502Z","shell.execute_reply":"2024-09-06T09:16:06.151588Z"},"papermill":{"duration":0.036775,"end_time":"2024-09-06T09:16:06.154495","exception":false,"start_time":"2024-09-06T09:16:06.11772","status":"completed"},"tags":[]},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","    \"\"\"\n","    多头注意力模块\n","    \n","    Args:\n","        d_in (int): Input dimension.\n","        d_out (int): Output dimension.\n","        context_length (int): The length of the input sequence.\n","        dropout (float): Dropout probability.\n","        num_heads (int): Number of attention heads.\n","        qkv_bias (bool, optional): Whether to include bias in query, key, and value projections. Default is False.\n","    \n","    Attributes:\n","        d_out (int): Output dimension.\n","        num_heads (int): Number of attention heads.\n","        head_dim (int): Dimension of each attention head.\n","        w_queries (nn.Linear): Linear projection for queries.\n","        w_keys (nn.Linear): Linear projection for keys.\n","        w_values (nn.Linear): Linear projection for values.\n","        out_proj (nn.Linear): Linear projection for output.\n","        dropout (nn.Dropout): Dropout layer.\n","        mask (torch.Tensor): Lower triangular mask to ensure causality.\n","    \"\"\"\n","    def __init__(self, d_in: int, d_out: int, context_length: int,\n","                 dropout: float, num_heads: int, qkv_bias: bool = False):\n","        super(MultiHeadAttention, self).__init__()\n","        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n","        self.d_out = d_out\n","        self.num_heads = num_heads\n","        self.head_dim = d_out // num_heads\n","\n","        self.w_queries = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.w_keys = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.w_values = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.out_proj = nn.Linear(d_out, d_out)\n","        self.dropout = nn.Dropout(dropout)\n","        \n","        self.register_buffer(\n","            'mask',\n","            torch.tril(torch.ones(context_length, context_length)).unsqueeze(0).unsqueeze(0)\n","        )\n","\n","    def forward(self, x):\n","        batches, num_tokens, dim_in = x.shape\n","\n","        # Linear projections\n","        queries = self.w_queries(x)\n","        keys = self.w_keys(x)\n","        values = self.w_values(x)\n","\n","        # Reshape and transpose for multi-head attention\n","        queries = queries.view(batches, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n","        keys = keys.view(batches, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n","        values = values.view(batches, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n","\n","        # Attention score calculation\n","        attn_scores = (queries @ keys.transpose(2, 3)) / (self.head_dim ** 0.5)\n","\n","        # Apply mask: Broadcasting across batches and heads\n","        attn_scores = attn_scores.masked_fill(self.mask[:, :, :num_tokens, :num_tokens] == 0, float('-inf'))\n","\n","        # Softmax to get attention weights\n","        attn_weights = torch.softmax(attn_scores, dim=-1)\n","        attn_weights = self.dropout(attn_weights)\n","\n","        # Context vector computation\n","        context_vec = (attn_weights @ values).transpose(1, 2)\n","        context_vec = context_vec.contiguous().view(batches, num_tokens, self.d_out)\n","\n","        # Final linear projection\n","        context_vec = self.out_proj(context_vec)\n","        \n","        return context_vec\n","\n"]},{"cell_type":"markdown","id":"0926c91c","metadata":{},"source":["#构建大语言模型LLM\n","#LLM decoder-only的模型架构有N层transformer架构组成。每层包含自注意力机制和前向传播层，层归一化层，残差连接（为了解决梯度消失问题）。"]},{"cell_type":"code","execution_count":12,"id":"17ab1ce3","metadata":{"execution":{"iopub.execute_input":"2024-09-06T09:16:06.337857Z","iopub.status.busy":"2024-09-06T09:16:06.337464Z","iopub.status.idle":"2024-09-06T09:16:06.344623Z","shell.execute_reply":"2024-09-06T09:16:06.343719Z"},"papermill":{"duration":0.0285,"end_time":"2024-09-06T09:16:06.346611","exception":false,"start_time":"2024-09-06T09:16:06.318111","status":"completed"},"tags":[]},"outputs":[],"source":["class LayerNorm(nn.Module):\n","    \"\"\"\n","    层归一化模块\n","    \n","    Args:\n","        emb_dim (int): The dimension of the input embeddings.\n","    \n","    Attributes:\n","        eps (float): A small value to avoid division by zero.\n","        scale (nn.Parameter): Learnable scale parameter.\n","        shift (nn.Parameter): Learnable shift parameter.\n","    \"\"\"\n","    def __init__(self, emb_dim):\n","        super(LayerNorm, self).__init__()\n","        self.eps = 1e-5\n","        self.scale = nn.Parameter(torch.ones(emb_dim))\n","        self.shift = nn.Parameter(torch.zeros(emb_dim))\n","\n","    def forward(self, x):\n","        mean = x.mean(dim=-1, keepdim=True)\n","        var = x.var(dim=-1, keepdim=True)\n","        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n","        return self.scale * norm_x + self.shift\n"]},{"cell_type":"code","execution_count":13,"id":"430c003c","metadata":{"execution":{"iopub.execute_input":"2024-09-06T09:16:06.421245Z","iopub.status.busy":"2024-09-06T09:16:06.420862Z","iopub.status.idle":"2024-09-06T09:16:06.426624Z","shell.execute_reply":"2024-09-06T09:16:06.425765Z"},"papermill":{"duration":0.02719,"end_time":"2024-09-06T09:16:06.428692","exception":false,"start_time":"2024-09-06T09:16:06.401502","status":"completed"},"tags":[]},"outputs":[],"source":["class GELU(nn.Module):\n","    \"\"\"\n","    GELU激活函数\n","    GELU(x) = 0.5 * x * (1 + tanh(sqrt(2/π) * (x + 0.044715 * x^3)))\n","    \"\"\"\n","    def __init__(self):\n","        super(GELU, self).__init__()\n","\n","    def forward(self, x):\n","        return 0.5 * x * (1 + torch.tanh(\n","            torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))\n","        ))"]},{"cell_type":"code","execution_count":14,"id":"420d73b7","metadata":{"execution":{"iopub.execute_input":"2024-09-06T09:16:06.502333Z","iopub.status.busy":"2024-09-06T09:16:06.50149Z","iopub.status.idle":"2024-09-06T09:16:06.507681Z","shell.execute_reply":"2024-09-06T09:16:06.506902Z"},"papermill":{"duration":0.02701,"end_time":"2024-09-06T09:16:06.509575","exception":false,"start_time":"2024-09-06T09:16:06.482565","status":"completed"},"tags":[]},"outputs":[],"source":["class FeedForwardGELU(nn.Module):\n","    \"\"\"\n","    前向传播层\n","    \n","    Args:\n","        cfg (dict): Configuration dictionary with key 'emb_dim' representing the embedding dimension.\n","    \n","    该网络包括：\n","    1. 一个线性层，维度从embedding dimension--》4* embedding dimension;\n","    2. GELU()激活层\n","    3. 一个线性层，维度映射回 embedding dimension;\n","    \"\"\"\n","    def __init__(self, cfg):\n","        super(FeedForwardGELU, self).__init__()\n","        emb_dim = cfg.emb_dim\n","        \n","        self.layers = nn.Sequential(\n","            nn.Linear(emb_dim, 4 * emb_dim),\n","            GELU(),\n","            nn.Linear(4 * emb_dim, emb_dim),\n","        )\n","\n","    def forward(self, x):\n","        return self.layers(x)\n"]},{"cell_type":"markdown","id":"1096e51b","metadata":{},"source":["构建transformer块的每个模块都已经具备，下面构建transformer架构"]},{"cell_type":"code","execution_count":15,"id":"75f876f1","metadata":{"execution":{"iopub.execute_input":"2024-09-06T09:16:06.617824Z","iopub.status.busy":"2024-09-06T09:16:06.61722Z","iopub.status.idle":"2024-09-06T09:16:06.624839Z","shell.execute_reply":"2024-09-06T09:16:06.624058Z"},"papermill":{"duration":0.028078,"end_time":"2024-09-06T09:16:06.626656","exception":false,"start_time":"2024-09-06T09:16:06.598578","status":"completed"},"tags":[]},"outputs":[],"source":["class TransformerBlock(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.att = MultiHeadAttention(\n","            d_in=cfg.emb_dim,\n","            d_out=cfg.emb_dim,\n","            context_length=cfg.context_length,\n","            num_heads=cfg.n_heads,\n","            dropout=cfg.drop_rate,\n","            qkv_bias=cfg.qkv_bias)\n","        self.ff = FeedForwardGELU(cfg)\n","        self.norm1 = LayerNorm(cfg.emb_dim)\n","        self.norm2 = LayerNorm(cfg.emb_dim)\n","        self.dropout = nn.Dropout(cfg.drop_rate)\n","\n","    def forward(self, x):\n","\n","        resid_conn = x\n","        x = self.norm1(x) \n","        x = self.att(x)\n","        x = self.dropout(x)\n","        x = x + resid_conn\n","\n","        resid_conn = x\n","        x = self.norm2(x)\n","        x = self.ff(x)\n","        x = self.dropout(x)\n","        x = x + resid_conn\n","        return x"]},{"cell_type":"code","execution_count":16,"id":"db048a2b","metadata":{"execution":{"iopub.execute_input":"2024-09-06T09:16:06.735336Z","iopub.status.busy":"2024-09-06T09:16:06.734531Z","iopub.status.idle":"2024-09-06T09:16:06.742916Z","shell.execute_reply":"2024-09-06T09:16:06.742027Z"},"papermill":{"duration":0.029246,"end_time":"2024-09-06T09:16:06.744883","exception":false,"start_time":"2024-09-06T09:16:06.715637","status":"completed"},"tags":[]},"outputs":[],"source":["# 构建最终的LLM架构\n","class GPTModel(nn.Module):\n","    def __init__(self,cfg):\n","        super().__init__()\n","        self.tok_emb = nn.Embedding(cfg.vocab_size,cfg.emb_dim) #token编码\n","        self.pos_emb = nn.Embedding(cfg.context_length,cfg.emb_dim) #位置编码\n","        self.transformer_blocks = nn.Sequential( *[TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n","        self.final_norm = LayerNorm(cfg.emb_dim)\n","        self.out_ff = nn.Linear(cfg.emb_dim,cfg.vocab_size,bias = False)\n","\n","    def forward(self, idx):\n","        batch_size, seq_len = idx.shape\n","        tok_embeds = self.tok_emb(idx)\n","        pos_embeds = self.pos_emb(torch.arange(seq_len, device=idx.device))\n","\n","        x = tok_embeds + pos_embeds\n","        x = self.dropout_emb(x)\n","        x = self.transformer_blocks(x)\n","        x = self.final_norm(x)\n","        logits = self.out_ff(x)\n","        return logits\n"]},{"cell_type":"markdown","id":"6e324af5","metadata":{},"source":["training阶段\n","在训练之前，要搞清楚：1. 如何用LLM来生成文本；2. 计算训练和验证集损失"]},{"cell_type":"code","execution_count":17,"id":"c051fb16","metadata":{"execution":{"iopub.execute_input":"2024-09-06T09:16:06.853452Z","iopub.status.busy":"2024-09-06T09:16:06.853148Z","iopub.status.idle":"2024-09-06T09:16:06.8671Z","shell.execute_reply":"2024-09-06T09:16:06.866235Z"},"papermill":{"duration":0.034911,"end_time":"2024-09-06T09:16:06.86899","exception":false,"start_time":"2024-09-06T09:16:06.834079","status":"completed"},"tags":[]},"outputs":[],"source":["#1. 生成文本\n","def generate_text_simple(model, idx, max_new_tokens, context_size):\n","    # idx is (B, T) array of indices in the current context\n","    for _ in range(max_new_tokens):\n","\n","        idx_cond = idx[:, -context_size:]\n","\n","        # 获得预测\n","        with torch.no_grad():\n","            logits = model(idx_cond)\n","\n","        # (batch, n_token, vocab_size) --》 (batch, vocab_size)\n","        logits = logits[:, -1, :]\n","\n","        # 获得具有最高概率值的index\n","        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n","\n","        # 将具有最高概率值的token添加到现有序列\n","        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n","\n","    return idx\n","\n","\n","\n","def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n","\n","    for _ in range(max_new_tokens):\n","        idx_cond = idx[:, -context_size:]\n","        with torch.no_grad():\n","            logits = model(idx_cond)\n","        logits = logits[:, -1, :]\n","\n","        # New: Filter logits with top_k sampling\n","        if top_k is not None:\n","            # Keep only top_k values\n","            top_logits, _ = torch.topk(logits, top_k)\n","            min_val = top_logits[:, -1]\n","            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)\n","\n","        # New: Apply temperature scaling\n","        if temperature > 0.0:\n","            logits = logits / temperature\n","\n","            # Apply softmax to get probabilities\n","            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n","\n","            # Sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n","\n","        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n","        else:\n","            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n","\n","        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n","            break\n","\n","        # Same as before: append sampled index to the running sequence\n","        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n","\n","    return idx\n","\n","\n","def generate_and_print_sample(model, tokenizer, device, start_context,temperature, top_k, eos_id):\n","    model.eval()\n","    context_size = model.pos_emb.weight.shape[0]\n","    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n","    with torch.no_grad():\n","#         token_ids = generate_text_simple(\n","#             model=model, idx=encoded,\n","#             max_new_tokens=50, context_size=context_size\n","#         )\n","         token_ids = generate(\n","            model=model, idx=encoded,\n","            max_new_tokens=50, context_size=context_size,\n","             temperature=temperature, top_k=top_k, eos_id=eos_id\n","        )\n","    decoded_text = token_ids_to_text(token_ids, tokenizer)\n","    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n","    model.train()\n","    \n"]},{"cell_type":"code","execution_count":18,"id":"0f60ad08","metadata":{"execution":{"iopub.execute_input":"2024-09-06T09:16:06.941202Z","iopub.status.busy":"2024-09-06T09:16:06.940908Z","iopub.status.idle":"2024-09-06T09:16:06.948321Z","shell.execute_reply":"2024-09-06T09:16:06.947461Z"},"papermill":{"duration":0.028038,"end_time":"2024-09-06T09:16:06.950275","exception":false,"start_time":"2024-09-06T09:16:06.922237","status":"completed"},"tags":[]},"outputs":[],"source":["#2.计算损失\n","def calc_loss_batch(input_batch, target_batch, model, device):\n","    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n","    logits = model(input_batch)\n","    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n","    return loss\n","\n","\n","def calc_loss_loader(data_loader, model, device, num_batches=None):\n","    total_loss = 0.\n","    if len(data_loader) == 0:\n","        return float(\"nan\")\n","    elif num_batches is None:\n","        num_batches = len(data_loader)\n","    else:\n","        # Reduce the number of batches to match the total number of batches in the data loader\n","        # if num_batches exceeds the number of batches in the data loader\n","        num_batches = min(num_batches, len(data_loader))\n","    for i, (input_batch, target_batch) in enumerate(data_loader):\n","        if i < num_batches:\n","            loss = calc_loss_batch(input_batch, target_batch, model, device)\n","            total_loss += loss.item()\n","        else:\n","            break\n","    return total_loss / num_batches"]},{"cell_type":"code","execution_count":19,"id":"48595f65","metadata":{"execution":{"iopub.execute_input":"2024-09-06T09:16:07.023718Z","iopub.status.busy":"2024-09-06T09:16:07.022837Z","iopub.status.idle":"2024-09-06T09:16:07.035398Z","shell.execute_reply":"2024-09-06T09:16:07.034483Z"},"papermill":{"duration":0.033926,"end_time":"2024-09-06T09:16:07.037329","exception":false,"start_time":"2024-09-06T09:16:07.003403","status":"completed"},"tags":[]},"outputs":[],"source":["def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n","                       eval_freq, eval_iter, start_context, tokenizer,\n","                      temperature, top_k, eos_id):\n","    # Initialize lists to track losses and tokens seen\n","    train_losses, val_losses, track_tokens_seen = [], [], []\n","    tokens_seen, global_step = 0, -1\n","\n","    # Main training loop\n","    for epoch in range(num_epochs):\n","        model.train()  # Set model to training mode\n","        optimizer.zero_grad()\n","\n","        for input_batch, target_batch in train_loader:\n","            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n","            loss = calc_loss_batch(input_batch, target_batch, model, device)\n","            loss.backward() # Calculate loss gradients\n","            optimizer.step() # Update model weights using loss gradients\n","            tokens_seen += input_batch.numel()\n","            global_step += 1\n","\n","            # Optional evaluation step\n","            if global_step % eval_freq == 0:\n","                train_loss, val_loss = evaluate_model(\n","                    model, train_loader, val_loader, device, eval_iter)\n","                train_losses.append(train_loss)\n","                val_losses.append(val_loss)\n","                track_tokens_seen.append(tokens_seen)\n","                print(f\"Ep {epoch+1} (Step {global_step:03d}): \"\n","                  f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n","\n","        # Print a sample text after each epoch\n","        print(\"example: \")\n","        generate_and_print_sample(\n","            model, tokenizer, device, start_context,\n","            temperature, top_k, eos_id\n","        )\n","        print('-*-'*10)\n","\n","    return train_losses, val_losses, track_tokens_seen\n","\n","\n","def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n","    model.eval()\n","    with torch.no_grad():\n","        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n","        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n","    model.train()\n","    return train_loss, val_loss\n","\n"]},{"cell_type":"code","execution_count":null,"id":"16365976","metadata":{"execution":{"iopub.execute_input":"2024-09-06T09:16:07.109649Z","iopub.status.busy":"2024-09-06T09:16:07.109341Z","iopub.status.idle":"2024-09-06T09:17:22.818601Z","shell.execute_reply":"2024-09-06T09:17:22.817518Z"},"papermill":{"duration":75.72999,"end_time":"2024-09-06T09:17:22.820601","exception":false,"start_time":"2024-09-06T09:16:07.090611","status":"completed"},"scrolled":true,"tags":[]},"outputs":[],"source":["#训练模型\n","def initialize_weights(m):\n","    if isinstance(m, nn.Linear):\n","        torch.nn.init.xavier_uniform_(m.weight)\n","        if m.bias is not None:\n","            m.bias.data.fill_(0.01)\n","\n","\n","torch.manual_seed(123)\n","model = GPTModel(GPTConfig)\n","model.to(device)\n","model.apply(initialize_weights)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=0.00009, weight_decay=0.1)\n","torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","\n","num_epochs = 50\n","train_losses, val_losses, tokens_seen = train_model_simple(\n","    model, train_dataloader, val_dataloader, optimizer, device,\n","    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n","    start_context=\"Every effort moves\", tokenizer=tokenizer,\n","        top_k=10,temperature=0.4,eos_id=None\n",")"]},{"cell_type":"code","execution_count":null,"id":"ff9692a2","metadata":{"execution":{"iopub.execute_input":"2024-09-06T09:17:24.56348Z","iopub.status.busy":"2024-09-06T09:17:24.562423Z","iopub.status.idle":"2024-09-06T09:17:27.984803Z","shell.execute_reply":"2024-09-06T09:17:27.983796Z"},"papermill":{"duration":3.45237,"end_time":"2024-09-06T09:17:27.986921","exception":false,"start_time":"2024-09-06T09:17:24.534551","status":"completed"},"tags":[]},"outputs":[],"source":["#模型验证\n","model.to(\"cpu\")\n","model.eval()\n","\n","\n","token_ids = generate(\n","    model=model,\n","    idx=text_to_token_ids(\"quite insensible to the irony\", tokenizer),\n","    max_new_tokens=25,\n","    context_size=GPTConfig.context_length,\n","    top_k=5,temperature=0.7,eos_id=None\n","\n",")\n","#测试\n","print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"]},{"cell_type":"markdown","id":"b35d4925","metadata":{"papermill":{"duration":0.026118,"end_time":"2024-09-06T09:17:28.093478","exception":false,"start_time":"2024-09-06T09:17:28.06736","status":"completed"},"tags":[]},"source":["<hr>"]},{"cell_type":"code","execution_count":null,"id":"c30a31a1","metadata":{"execution":{"iopub.execute_input":"2024-09-06T09:17:28.200295Z","iopub.status.busy":"2024-09-06T09:17:28.199935Z","iopub.status.idle":"2024-09-06T09:17:30.994608Z","shell.execute_reply":"2024-09-06T09:17:30.993407Z"},"papermill":{"duration":2.82473,"end_time":"2024-09-06T09:17:30.996954","exception":false,"start_time":"2024-09-06T09:17:28.172224","status":"completed"},"tags":[]},"outputs":[],"source":["#保存效果最好的模型参数以便后面加载\n","print('saving model and optimizer...')\n","torch.save({\n","    \"model_state_dict\": model.state_dict(),\n","    \"optimizer_state_dict\": optimizer.state_dict(),\n","    }, \n","    \"model_and_optimizer.pth\"\n",")\n","print('Done')"]},{"cell_type":"code","execution_count":null,"id":"39231cf2","metadata":{"execution":{"iopub.execute_input":"2024-09-06T09:17:31.124324Z","iopub.status.busy":"2024-09-06T09:17:31.123395Z","iopub.status.idle":"2024-09-06T09:17:34.262755Z","shell.execute_reply":"2024-09-06T09:17:34.261711Z"},"papermill":{"duration":3.170716,"end_time":"2024-09-06T09:17:34.264945","exception":false,"start_time":"2024-09-06T09:17:31.094229","status":"completed"},"tags":[]},"outputs":[],"source":["# 加载模型用来训练或者使用\n","print('loading...')\n","checkpoint = torch.load(\"model_and_optimizer.pth\", weights_only=True)\n","\n","model = GPTModel(GPTConfig)\n","model.to(device)\n","model.load_state_dict(checkpoint[\"model_state_dict\"])\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.1)\n","optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n","model.train();\n","print('Done')"]},{"cell_type":"code","execution_count":null,"id":"a1f863bc","metadata":{"execution":{"iopub.execute_input":"2024-09-06T09:17:34.320676Z","iopub.status.busy":"2024-09-06T09:17:34.320286Z","iopub.status.idle":"2024-09-06T09:17:37.423004Z","shell.execute_reply":"2024-09-06T09:17:37.422024Z"},"papermill":{"duration":3.133285,"end_time":"2024-09-06T09:17:37.425552","exception":false,"start_time":"2024-09-06T09:17:34.292267","status":"completed"},"tags":[]},"outputs":[],"source":["num_epochs = 2\n","train_losses, val_losses, tokens_seen = train_model_simple(\n","    model, train_dataloader, val_dataloader, optimizer, device,\n","    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n","    start_context=\"Every effort moves \", tokenizer=tokenizer,\n","        top_k=10,temperature=2.7,eos_id=None\n",")"]},{"cell_type":"code","execution_count":null,"id":"472713e5","metadata":{"execution":{"iopub.execute_input":"2024-09-06T09:17:37.614342Z","iopub.status.busy":"2024-09-06T09:17:37.61342Z","iopub.status.idle":"2024-09-06T09:17:42.384122Z","shell.execute_reply":"2024-09-06T09:17:42.38308Z"},"papermill":{"duration":4.81137,"end_time":"2024-09-06T09:17:42.386303","exception":false,"start_time":"2024-09-06T09:17:37.574933","status":"completed"},"tags":[]},"outputs":[],"source":["#通过transformers库加载已经训练好的模型架构和参数\n","from transformers import GPT2Model\n","\n","gpt2_small=  \"openai-community/gpt2\"\n","\n","gpt_hf = GPT2Model.from_pretrained(gpt2_small, cache_dir=\"checkpoints\")\n","gpt_hf.eval()\n"]},{"cell_type":"code","execution_count":null,"id":"09b61caf","metadata":{"execution":{"iopub.execute_input":"2024-09-06T09:17:42.444471Z","iopub.status.busy":"2024-09-06T09:17:42.443977Z","iopub.status.idle":"2024-09-06T09:17:42.450679Z","shell.execute_reply":"2024-09-06T09:17:42.449721Z"},"papermill":{"duration":0.037933,"end_time":"2024-09-06T09:17:42.452699","exception":false,"start_time":"2024-09-06T09:17:42.414766","status":"completed"},"tags":[]},"outputs":[],"source":["copyConfig = replace(GPTConfig)\n","copyConfig.qkv_bias =True\n","copyConfig.context_length =  1024\n","copyConfig.drop_rate= 0.0       \n","copyConfig"]},{"cell_type":"code","execution_count":29,"id":"5cd34faf","metadata":{"execution":{"iopub.execute_input":"2024-09-06T09:17:42.514859Z","iopub.status.busy":"2024-09-06T09:17:42.514542Z","iopub.status.idle":"2024-09-06T09:17:42.519393Z","shell.execute_reply":"2024-09-06T09:17:42.518551Z"},"papermill":{"duration":0.036841,"end_time":"2024-09-06T09:17:42.52155","exception":false,"start_time":"2024-09-06T09:17:42.484709","status":"completed"},"tags":[]},"outputs":[],"source":["def assign_check(left, right):\n","    if left.shape != right.shape:\n","        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n","    return torch.nn.Parameter(right.clone().detach())"]},{"cell_type":"code","execution_count":30,"id":"0c603287","metadata":{"execution":{"iopub.execute_input":"2024-09-06T09:17:42.580602Z","iopub.status.busy":"2024-09-06T09:17:42.580302Z","iopub.status.idle":"2024-09-06T09:17:42.596898Z","shell.execute_reply":"2024-09-06T09:17:42.596053Z"},"papermill":{"duration":0.047456,"end_time":"2024-09-06T09:17:42.598893","exception":false,"start_time":"2024-09-06T09:17:42.551437","status":"completed"},"tags":[]},"outputs":[],"source":["def load_weights(gpt, gpt_hf):\n","\n","    d = gpt_hf.state_dict()\n","\n","    gpt.pos_emb.weight = assign_check(gpt.pos_emb.weight, d[\"wpe.weight\"])\n","    gpt.tok_emb.weight = assign_check(gpt.tok_emb.weight, d[\"wte.weight\"])\n","    \n","    for b in range(copyConfig.n_layers):\n","        q_w, k_w, v_w = np.split(d[f\"h.{b}.attn.c_attn.weight\"], 3, axis=-1)\n","        gpt.transformer_blocks[b].att.w_queries.weight = assign_check(gpt.transformer_blocks[b].att.w_queries.weight, q_w.T)\n","        gpt.transformer_blocks[b].att.w_keys.weight = assign_check(gpt.transformer_blocks[b].att.w_keys.weight, k_w.T)\n","        gpt.transformer_blocks[b].att.w_values.weight = assign_check(gpt.transformer_blocks[b].att.w_values.weight, v_w.T)\n","    \n","        q_b, k_b, v_b = np.split(d[f\"h.{b}.attn.c_attn.bias\"], 3, axis=-1)\n","        gpt.transformer_blocks[b].att.w_queries.bias = assign_check(gpt.transformer_blocks[b].att.w_queries.bias, q_b)\n","        gpt.transformer_blocks[b].att.w_keys.bias = assign_check(gpt.transformer_blocks[b].att.w_keys.bias, k_b)\n","        gpt.transformer_blocks[b].att.w_values.bias = assign_check(gpt.transformer_blocks[b].att.w_values.bias, v_b)\n","    \n","    \n","        gpt.transformer_blocks[b].att.out_proj.weight = assign_check(gpt.transformer_blocks[b].att.out_proj.weight, d[f\"h.{b}.attn.c_proj.weight\"].T)\n","        gpt.transformer_blocks[b].att.out_proj.bias = assign_check(gpt.transformer_blocks[b].att.out_proj.bias, d[f\"h.{b}.attn.c_proj.bias\"])\n","    \n","        gpt.transformer_blocks[b].ff.layers[0].weight = assign_check(gpt.transformer_blocks[b].ff.layers[0].weight, d[f\"h.{b}.mlp.c_fc.weight\"].T)\n","        gpt.transformer_blocks[b].ff.layers[0].bias = assign_check(gpt.transformer_blocks[b].ff.layers[0].bias, d[f\"h.{b}.mlp.c_fc.bias\"])\n","        gpt.transformer_blocks[b].ff.layers[2].weight = assign_check(gpt.transformer_blocks[b].ff.layers[2].weight, d[f\"h.{b}.mlp.c_proj.weight\"].T)\n","        gpt.transformer_blocks[b].ff.layers[2].bias = assign_check(gpt.transformer_blocks[b].ff.layers[2].bias, d[f\"h.{b}.mlp.c_proj.bias\"])\n","    \n","        gpt.transformer_blocks[b].norm1.scale = assign_check(gpt.transformer_blocks[b].norm1.scale, d[f\"h.{b}.ln_1.weight\"])\n","        gpt.transformer_blocks[b].norm1.shift = assign_check(gpt.transformer_blocks[b].norm1.shift, d[f\"h.{b}.ln_1.bias\"])\n","        gpt.transformer_blocks[b].norm2.scale = assign_check(gpt.transformer_blocks[b].norm2.scale, d[f\"h.{b}.ln_2.weight\"])\n","        gpt.transformer_blocks[b].norm2.shift = assign_check(gpt.transformer_blocks[b].norm2.shift, d[f\"h.{b}.ln_2.bias\"])\n","    \n","        gpt.final_norm.scale = assign_check(gpt.final_norm.scale, d[f\"ln_f.weight\"])\n","        gpt.final_norm.shift = assign_check(gpt.final_norm.shift, d[f\"ln_f.bias\"])\n","        gpt.out_ff.weight = assign_check(gpt.out_ff.weight, d[\"wte.weight\"])"]},{"cell_type":"code","execution_count":31,"id":"00fb98d1","metadata":{"execution":{"iopub.execute_input":"2024-09-06T09:17:42.655572Z","iopub.status.busy":"2024-09-06T09:17:42.655069Z","iopub.status.idle":"2024-09-06T09:17:44.794655Z","shell.execute_reply":"2024-09-06T09:17:44.793657Z"},"papermill":{"duration":2.1706,"end_time":"2024-09-06T09:17:44.797041","exception":false,"start_time":"2024-09-06T09:17:42.626441","status":"completed"},"tags":[]},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","gpt = GPTModel(copyConfig)\n","load_weights(gpt, gpt_hf)\n"]},{"cell_type":"code","execution_count":null,"id":"ab8fb51f","metadata":{"execution":{"iopub.execute_input":"2024-09-06T09:17:44.856236Z","iopub.status.busy":"2024-09-06T09:17:44.855464Z","iopub.status.idle":"2024-09-06T09:17:45.446984Z","shell.execute_reply":"2024-09-06T09:17:45.446019Z"},"papermill":{"duration":0.623389,"end_time":"2024-09-06T09:17:45.449469","exception":false,"start_time":"2024-09-06T09:17:44.82608","status":"completed"},"tags":[]},"outputs":[],"source":["# test\n","token_ids = generate(\n","    model=gpt.to(device),\n","    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n","    max_new_tokens=30,\n","    context_size=copyConfig.context_length,\n","    top_k=1,\n","    temperature=1.0\n",")\n","\n","print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30762,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"papermill":{"default_parameters":{},"duration":232.844049,"end_time":"2024-09-06T09:19:31.87945","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-09-06T09:15:39.035401","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"1288cda84b944d92ba9981f49da8b1d5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1359f883f2bd4c6f886b85fdfb500323":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e50dfa1156c4ef4851479b0e784ffad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2925ea64aef54875bdfa1c3f07d6df0e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a8b6c2483ceb40af97a5d381fa933ab8","placeholder":"​","style":"IPY_MODEL_b5476961a6f04e4b8806d67e102587d2","value":"model.safetensors: 100%"}},"2b3bbba10b3b437bb0cdef2cfd54ded7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3959ef94679145779e112115b6b44811":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"399793a50df44de893f21cf959f84eef":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"490301ff8a544de1b13e3030adc8c2b8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ba09336280d6411aa68033233eed3311","IPY_MODEL_bc3b94b7ece14d8eb4a559bb275ab7b4","IPY_MODEL_c9330950c9b24c9bbea544494c5e5cd8"],"layout":"IPY_MODEL_4ba1ac0f15e942f29ce3e9541db1c938"}},"4ba1ac0f15e942f29ce3e9541db1c938":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d8bd60656f94c85adf3fa9a53d519cf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"527fa0f3e1d34f768c2419420cc40cac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_399793a50df44de893f21cf959f84eef","placeholder":"​","style":"IPY_MODEL_dca8669445e540b3aac799fa7da3d911","value":" 548M/548M [00:02&lt;00:00, 290MB/s]"}},"587ce8ba18084b4e807caf4bf7296871":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_be61271995204a58832578c6e2288215","max":548105171,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2b3bbba10b3b437bb0cdef2cfd54ded7","value":548105171}},"a3ca62092dda4f99a9be41e94e2927aa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a8b6c2483ceb40af97a5d381fa933ab8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b5476961a6f04e4b8806d67e102587d2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ba09336280d6411aa68033233eed3311":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e65918a49ee64233a2e9b1caa6427cc8","placeholder":"​","style":"IPY_MODEL_1e50dfa1156c4ef4851479b0e784ffad","value":"config.json: 100%"}},"bc3b94b7ece14d8eb4a559bb275ab7b4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1288cda84b944d92ba9981f49da8b1d5","max":665,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a3ca62092dda4f99a9be41e94e2927aa","value":665}},"be61271995204a58832578c6e2288215":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9330950c9b24c9bbea544494c5e5cd8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1359f883f2bd4c6f886b85fdfb500323","placeholder":"​","style":"IPY_MODEL_3959ef94679145779e112115b6b44811","value":" 665/665 [00:00&lt;00:00, 54.9kB/s]"}},"dca8669445e540b3aac799fa7da3d911":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e358ef14e89248b9ba237118e8ef8838":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2925ea64aef54875bdfa1c3f07d6df0e","IPY_MODEL_587ce8ba18084b4e807caf4bf7296871","IPY_MODEL_527fa0f3e1d34f768c2419420cc40cac"],"layout":"IPY_MODEL_4d8bd60656f94c85adf3fa9a53d519cf"}},"e65918a49ee64233a2e9b1caa6427cc8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":5}
